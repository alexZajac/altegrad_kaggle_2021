{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FinalSubmission",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b44f8565cce40deb24f7064655de383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_35ca1c4ed91248bfb0c5c84bd0455bf4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e81db207fd0141d2b6aea3212fef122e",
              "IPY_MODEL_3b6830524b0b43f6bbe91bf5e5a0c5a7"
            ]
          }
        },
        "35ca1c4ed91248bfb0c5c84bd0455bf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e81db207fd0141d2b6aea3212fef122e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5061aee3e94c4af185ba50ceb0b6fc94",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2503,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2503,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efcbba90f7594880bc92febc60479374"
          }
        },
        "3b6830524b0b43f6bbe91bf5e5a0c5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3a1a8e79eb1495794eb8b089be4aa2c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2503/2503 [04:57&lt;00:00,  8.43it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f74c7f8db90245f4b44e0ab798b66f4c"
          }
        },
        "5061aee3e94c4af185ba50ceb0b6fc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efcbba90f7594880bc92febc60479374": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3a1a8e79eb1495794eb8b089be4aa2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f74c7f8db90245f4b44e0ab798b66f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyEICWgNTXqf"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivYv6anz9Lw6",
        "outputId": "9f8b778e-ec14-4ba9-fe50-b5923ab13e84"
      },
      "source": [
        "!pip install gdown\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/b4/c9/b413a24f759641bc27ef98c144b590023c8038dfb8a3f09e713e9dff12c1/ipython_autotime-0.3.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (54.0.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.0.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.7.0)\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.1\n",
            "time: 1.91 ms (started: 2021-03-04 11:48:12 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_P0fvJt7sob"
      },
      "source": [
        "# Train & Test authors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llizn38y6ysf",
        "outputId": "41e6b551-f3f2-4fff-f095-0157b16bc771"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1Qbi954Bwx-PplM8F_7TrB_blcqcB-bF2\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Qbi954Bwx-PplM8F_7TrB_blcqcB-bF2\n",
            "To: /content/train.csv\n",
            "\r  0% 0.00/305k [00:00<?, ?B/s]\r100% 305k/305k [00:00<00:00, 45.3MB/s]\n",
            "time: 1.42 s (started: 2021-03-04 11:48:12 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvDMYQkW82wK",
        "outputId": "335e5c27-80b0-4ead-a65e-200c98af1cf7"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1hWycEy8rQ8e_krGyUhqGQiMBxZtD9SWy\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hWycEy8rQ8e_krGyUhqGQiMBxZtD9SWy\n",
            "To: /content/test.csv\n",
            "\r0.00B [00:00, ?B/s]\r2.47MB [00:00, 79.1MB/s]\n",
            "time: 1.82 s (started: 2021-03-04 11:48:13 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmu_cJ88739n"
      },
      "source": [
        "# Graphs features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WetSA-ME51Ys",
        "outputId": "9a2093da-81f3-4dc9-eebc-7570f6a79a16"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1j7h-SUy2gMJBds6eQalw4CFZC3O7Pr05\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j7h-SUy2gMJBds6eQalw4CFZC3O7Pr05\n",
            "To: /content/collaboration_network.edgelist\n",
            "38.8MB [00:00, 75.3MB/s]\n",
            "time: 2.62 s (started: 2021-03-04 11:48:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pgovEZLtSck",
        "outputId": "b5db6aba-36ef-4ebf-ce8b-7940c458942e"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1ewZowFKZNSXwaH9yt5Ad4-ieOHpyV1W1\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ewZowFKZNSXwaH9yt5Ad4-ieOHpyV1W1\n",
            "To: /content/X_train_graph.csv\n",
            "\r0.00B [00:00, ?B/s]\r4.67MB [00:00, 32.9MB/s]\n",
            "time: 2.22 s (started: 2021-03-04 11:48:18 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE-1_jQ4tey-",
        "outputId": "1151f30e-e11a-4bb7-da5c-cc266fbd6716"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1obp93WcXFh22mZMhuLguKtIEPNaJXlyD\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1obp93WcXFh22mZMhuLguKtIEPNaJXlyD\n",
            "To: /content/X_test_graph.csv\n",
            "42.1MB [00:00, 74.5MB/s]\n",
            "time: 2.82 s (started: 2021-03-04 11:48:20 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29coo_7b8MI_"
      },
      "source": [
        "# Abstract embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyueitTxtHMM",
        "outputId": "5b105889-0f9d-4f76-ac8d-8152b3f10aeb"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1yQuJlP_Igc5W2zZUEgupwvj9SNSfUTRV\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yQuJlP_Igc5W2zZUEgupwvj9SNSfUTRV\n",
            "To: /content/author_to_embeddings_scibert_uncased.npy\n",
            "1.44GB [00:14, 99.5MB/s]\n",
            "time: 39.3 s (started: 2021-03-04 11:48:23 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp75ve0pahs6",
        "outputId": "b94ca381-ea23-4521-b903-db1d9c5696e1"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1AEn8P631y-pq6szgAbTm538oM6waf-5E\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AEn8P631y-pq6szgAbTm538oM6waf-5E\n",
            "To: /content/author_embedding.csv\n",
            "683MB [00:23, 28.7MB/s]\n",
            "time: 40.6 s (started: 2021-03-04 11:49:02 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3J5DM9_xYM8"
      },
      "source": [
        "# Node embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMaM0k50wlIr",
        "outputId": "fca091f8-e109-475a-cda9-cefc110fbf93"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1-7PiaxpmYiPKsRdMIlkKqfpJjR_zy5HE\""
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-7PiaxpmYiPKsRdMIlkKqfpJjR_zy5HE\n",
            "To: /content/Deepwalk_256_30_100.csv\n",
            "1.19GB [00:10, 119MB/s]\n",
            "time: 23.8 s (started: 2021-03-04 12:19:12 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2Zj8_Yb20CG"
      },
      "source": [
        "# Merging and reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKP-HwH_73DM",
        "outputId": "e1de4389-2ace-40f0-8c60-8347e4396393"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# read training data\n",
        "df_train = pd.read_csv('train.csv', dtype={'authorID': np.int64, 'h_index': np.float32})\n",
        "n_train = df_train.shape[0]\n",
        "\n",
        "# read test data\n",
        "df_test = pd.read_csv('test.csv', dtype={'authorID': np.int64})\n",
        "n_test = df_test.shape[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.16 s (started: 2021-03-04 11:49:44 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfUS9SVO5kW4",
        "outputId": "95149c5f-14ec-4d99-c71c-ab5f9bcf7560"
      },
      "source": [
        "G = nx.read_edgelist('collaboration_network.edgelist',\r\n",
        "                     delimiter=' ', nodetype=int)\r\n",
        "n_nodes = G.number_of_nodes()\r\n",
        "n_edges = G.number_of_edges()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.33 s (started: 2021-03-04 11:49:45 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAWUMY3MyzEc",
        "outputId": "59ee71d3-43cf-4ecc-acea-7dda7d415676"
      },
      "source": [
        "# Read train and test graph features\r\n",
        "X_train_graph = pd.read_csv('X_train_graph.csv', index_col=0)\r\n",
        "X_test_graph = pd.read_csv('X_test_graph.csv', index_col=0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 534 ms (started: 2021-03-04 11:49:53 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7oWJKzcwL2U",
        "outputId": "efc6b1c0-fa04-47d3-81c8-9e397ee41c61"
      },
      "source": [
        "# read embeddings of abstracts\r\n",
        "embeddings_abstracts = np.load('author_to_embeddings_scibert_uncased.npy', allow_pickle=True).item()\r\n",
        "embeddings_abstracts = pd.DataFrame.from_dict(embeddings_abstracts, orient='index')\r\n",
        "embeddings_abstracts.reset_index(inplace=True)\r\n",
        "embeddings_abstracts.rename(columns={\"index\": \"authorID\"}, inplace=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2min 27s (started: 2021-03-04 12:22:55 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOAd-oHm2u1n"
      },
      "source": [
        "# Replacing authors with zeroed embeddings by the mean of their direct neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aodkmuHpHsE",
        "outputId": "64055ef0-68af-4601-87ef-75b10bad92fd"
      },
      "source": [
        "zeros_author_id = embeddings_abstracts[(embeddings_abstracts.T == 0).any()][\"authorID\"]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.08 s (started: 2021-03-04 12:25:22 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j0oqlnO_MaB",
        "outputId": "61f58be9-cb4b-45a5-d128-677e3520f2dc"
      },
      "source": [
        "embeddings_abstracts_2 = pd.read_csv(\"author_embedding.csv\", header=None)\r\n",
        "embeddings_abstracts_2.rename(columns={0: 'authorID'}, inplace=True)\r\n",
        "embeddings_abstracts = embeddings_abstracts_2.merge(embeddings_abstracts, on=\"authorID\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 21.2 s (started: 2021-03-04 12:25:53 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWBB5uhKHYIM",
        "outputId": "3f4309db-0dc1-42c1-b00e-35eeab6f0d7c"
      },
      "source": [
        "embeddings_abstracts.set_index(\"authorID\", inplace=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 2.69 ms (started: 2021-03-04 12:26:14 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "5b44f8565cce40deb24f7064655de383",
            "35ca1c4ed91248bfb0c5c84bd0455bf4",
            "e81db207fd0141d2b6aea3212fef122e",
            "3b6830524b0b43f6bbe91bf5e5a0c5a7",
            "5061aee3e94c4af185ba50ceb0b6fc94",
            "efcbba90f7594880bc92febc60479374",
            "b3a1a8e79eb1495794eb8b089be4aa2c",
            "f74c7f8db90245f4b44e0ab798b66f4c"
          ]
        },
        "id": "Q3yomPJEqCcG",
        "outputId": "9e6d546f-4d5e-4acc-a180-7301a3f8c3a6"
      },
      "source": [
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "authors_to_drop = []\r\n",
        "for authorID in tqdm(zeros_author_id):\r\n",
        "    neighbors = list(G.neighbors(authorID))\r\n",
        "    if len(neighbors):\r\n",
        "        neighbors_embeddings = embeddings_abstracts.query(\"authorID == @neighbors\").mean(axis=0).values\r\n",
        "        if np.sum(neighbors_embeddings) == 0:\r\n",
        "            authors_to_drop.append(authorID)\r\n",
        "        else:\r\n",
        "            embeddings_abstracts.loc[authorID] = neighbors_embeddings\r\n",
        "    else:\r\n",
        "        authors_to_drop.append(authorID)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b44f8565cce40deb24f7064655de383",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2503.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "time: 3min 49s (started: 2021-03-04 12:26:14 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHHH43YMIOor",
        "outputId": "055c2bc4-cb65-4ae0-f22f-3436a69e71d7"
      },
      "source": [
        "embeddings_abstracts.reset_index(inplace=True)\r\n",
        "embeddings_abstracts.rename(columns={\"index\": \"authorID\"}, inplace=True)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 5.33 ms (started: 2021-03-04 12:30:03 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "QSDGBQatxDt0",
        "outputId": "07278d47-a451-4032-9245-8a8d56b16b4f"
      },
      "source": [
        "embeddings_nodes = pd.read_csv(\"Deepwalk_256_30_100.csv\")\r\n",
        "embeddings_nodes"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authorID</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>216</th>\n",
              "      <th>217</th>\n",
              "      <th>218</th>\n",
              "      <th>219</th>\n",
              "      <th>220</th>\n",
              "      <th>221</th>\n",
              "      <th>222</th>\n",
              "      <th>223</th>\n",
              "      <th>224</th>\n",
              "      <th>225</th>\n",
              "      <th>226</th>\n",
              "      <th>227</th>\n",
              "      <th>228</th>\n",
              "      <th>229</th>\n",
              "      <th>230</th>\n",
              "      <th>231</th>\n",
              "      <th>232</th>\n",
              "      <th>233</th>\n",
              "      <th>234</th>\n",
              "      <th>235</th>\n",
              "      <th>236</th>\n",
              "      <th>237</th>\n",
              "      <th>238</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2811232256</td>\n",
              "      <td>0.078322</td>\n",
              "      <td>0.425899</td>\n",
              "      <td>-0.282558</td>\n",
              "      <td>-0.212039</td>\n",
              "      <td>0.437473</td>\n",
              "      <td>-0.533683</td>\n",
              "      <td>0.118062</td>\n",
              "      <td>0.109346</td>\n",
              "      <td>0.153380</td>\n",
              "      <td>-0.091917</td>\n",
              "      <td>0.131568</td>\n",
              "      <td>0.142600</td>\n",
              "      <td>-0.323563</td>\n",
              "      <td>0.105417</td>\n",
              "      <td>-0.260582</td>\n",
              "      <td>1.025874</td>\n",
              "      <td>-0.238729</td>\n",
              "      <td>0.843510</td>\n",
              "      <td>-0.599690</td>\n",
              "      <td>0.055727</td>\n",
              "      <td>-0.121802</td>\n",
              "      <td>0.168530</td>\n",
              "      <td>-0.043600</td>\n",
              "      <td>-0.254090</td>\n",
              "      <td>-0.302414</td>\n",
              "      <td>-0.218395</td>\n",
              "      <td>0.488611</td>\n",
              "      <td>-0.224013</td>\n",
              "      <td>-0.009983</td>\n",
              "      <td>0.145883</td>\n",
              "      <td>0.580643</td>\n",
              "      <td>-0.146240</td>\n",
              "      <td>-0.018496</td>\n",
              "      <td>0.362598</td>\n",
              "      <td>0.126936</td>\n",
              "      <td>0.503825</td>\n",
              "      <td>-0.300405</td>\n",
              "      <td>-0.295040</td>\n",
              "      <td>-0.183400</td>\n",
              "      <td>...</td>\n",
              "      <td>0.271267</td>\n",
              "      <td>0.234581</td>\n",
              "      <td>0.449759</td>\n",
              "      <td>-0.747318</td>\n",
              "      <td>0.013454</td>\n",
              "      <td>-0.139800</td>\n",
              "      <td>-0.535146</td>\n",
              "      <td>-0.034288</td>\n",
              "      <td>0.319036</td>\n",
              "      <td>0.225892</td>\n",
              "      <td>0.408359</td>\n",
              "      <td>-0.096428</td>\n",
              "      <td>0.003071</td>\n",
              "      <td>0.359191</td>\n",
              "      <td>-0.428916</td>\n",
              "      <td>-0.244639</td>\n",
              "      <td>-0.118960</td>\n",
              "      <td>-0.469189</td>\n",
              "      <td>0.282080</td>\n",
              "      <td>0.149989</td>\n",
              "      <td>-0.006739</td>\n",
              "      <td>0.218743</td>\n",
              "      <td>0.221592</td>\n",
              "      <td>0.189575</td>\n",
              "      <td>-0.789706</td>\n",
              "      <td>0.046691</td>\n",
              "      <td>-0.182378</td>\n",
              "      <td>0.245982</td>\n",
              "      <td>0.140387</td>\n",
              "      <td>-0.832719</td>\n",
              "      <td>-0.119419</td>\n",
              "      <td>0.691741</td>\n",
              "      <td>-0.149556</td>\n",
              "      <td>0.216920</td>\n",
              "      <td>0.084310</td>\n",
              "      <td>0.386564</td>\n",
              "      <td>0.538023</td>\n",
              "      <td>-0.127464</td>\n",
              "      <td>-0.111425</td>\n",
              "      <td>0.410204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2231369732</td>\n",
              "      <td>-0.471883</td>\n",
              "      <td>0.559586</td>\n",
              "      <td>-0.163301</td>\n",
              "      <td>0.237019</td>\n",
              "      <td>-0.165376</td>\n",
              "      <td>-0.120098</td>\n",
              "      <td>0.047937</td>\n",
              "      <td>-0.236933</td>\n",
              "      <td>0.092725</td>\n",
              "      <td>-0.049726</td>\n",
              "      <td>0.707705</td>\n",
              "      <td>-0.230389</td>\n",
              "      <td>-0.389177</td>\n",
              "      <td>0.221694</td>\n",
              "      <td>0.168639</td>\n",
              "      <td>0.110412</td>\n",
              "      <td>0.215994</td>\n",
              "      <td>0.288412</td>\n",
              "      <td>-0.533525</td>\n",
              "      <td>0.209637</td>\n",
              "      <td>0.238793</td>\n",
              "      <td>-0.582479</td>\n",
              "      <td>0.521500</td>\n",
              "      <td>0.065697</td>\n",
              "      <td>0.460028</td>\n",
              "      <td>-0.328503</td>\n",
              "      <td>0.380392</td>\n",
              "      <td>-0.619274</td>\n",
              "      <td>0.435834</td>\n",
              "      <td>-0.750890</td>\n",
              "      <td>-0.768089</td>\n",
              "      <td>-0.286724</td>\n",
              "      <td>-0.552109</td>\n",
              "      <td>-0.152070</td>\n",
              "      <td>0.647019</td>\n",
              "      <td>0.207891</td>\n",
              "      <td>0.094218</td>\n",
              "      <td>-0.064490</td>\n",
              "      <td>-0.132373</td>\n",
              "      <td>...</td>\n",
              "      <td>0.223085</td>\n",
              "      <td>-0.037926</td>\n",
              "      <td>-0.312431</td>\n",
              "      <td>-0.109890</td>\n",
              "      <td>-0.086524</td>\n",
              "      <td>-0.201077</td>\n",
              "      <td>0.828090</td>\n",
              "      <td>0.122569</td>\n",
              "      <td>-0.130583</td>\n",
              "      <td>-0.691747</td>\n",
              "      <td>-0.178923</td>\n",
              "      <td>0.599113</td>\n",
              "      <td>-0.640131</td>\n",
              "      <td>-0.218122</td>\n",
              "      <td>-0.241580</td>\n",
              "      <td>0.626449</td>\n",
              "      <td>-0.141804</td>\n",
              "      <td>-0.457540</td>\n",
              "      <td>-0.581892</td>\n",
              "      <td>-0.051166</td>\n",
              "      <td>-0.218517</td>\n",
              "      <td>0.397215</td>\n",
              "      <td>-0.048215</td>\n",
              "      <td>0.068008</td>\n",
              "      <td>-0.065612</td>\n",
              "      <td>-0.063717</td>\n",
              "      <td>-0.558906</td>\n",
              "      <td>0.126132</td>\n",
              "      <td>0.201289</td>\n",
              "      <td>-0.327667</td>\n",
              "      <td>-0.443188</td>\n",
              "      <td>-0.691294</td>\n",
              "      <td>0.254635</td>\n",
              "      <td>-0.378854</td>\n",
              "      <td>0.130171</td>\n",
              "      <td>0.161605</td>\n",
              "      <td>-0.155014</td>\n",
              "      <td>0.274213</td>\n",
              "      <td>-0.480836</td>\n",
              "      <td>0.240680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2138570756</td>\n",
              "      <td>-0.348740</td>\n",
              "      <td>-0.636298</td>\n",
              "      <td>-0.471484</td>\n",
              "      <td>1.053085</td>\n",
              "      <td>-0.709146</td>\n",
              "      <td>-0.507657</td>\n",
              "      <td>0.264496</td>\n",
              "      <td>0.674356</td>\n",
              "      <td>-0.452875</td>\n",
              "      <td>-0.063166</td>\n",
              "      <td>0.186672</td>\n",
              "      <td>0.390289</td>\n",
              "      <td>-0.463047</td>\n",
              "      <td>0.109510</td>\n",
              "      <td>0.870878</td>\n",
              "      <td>0.091992</td>\n",
              "      <td>1.088165</td>\n",
              "      <td>0.592943</td>\n",
              "      <td>0.031754</td>\n",
              "      <td>-0.981353</td>\n",
              "      <td>0.554831</td>\n",
              "      <td>1.111873</td>\n",
              "      <td>0.137177</td>\n",
              "      <td>0.858939</td>\n",
              "      <td>0.018842</td>\n",
              "      <td>-0.288868</td>\n",
              "      <td>-0.836527</td>\n",
              "      <td>0.501834</td>\n",
              "      <td>-0.422141</td>\n",
              "      <td>-1.000779</td>\n",
              "      <td>-0.144457</td>\n",
              "      <td>-0.847862</td>\n",
              "      <td>0.634302</td>\n",
              "      <td>-0.366793</td>\n",
              "      <td>0.255091</td>\n",
              "      <td>-0.067136</td>\n",
              "      <td>-0.113507</td>\n",
              "      <td>0.249805</td>\n",
              "      <td>0.134027</td>\n",
              "      <td>...</td>\n",
              "      <td>0.372837</td>\n",
              "      <td>0.113388</td>\n",
              "      <td>0.096595</td>\n",
              "      <td>0.418761</td>\n",
              "      <td>0.448495</td>\n",
              "      <td>-0.944356</td>\n",
              "      <td>-0.264901</td>\n",
              "      <td>0.493232</td>\n",
              "      <td>1.385660</td>\n",
              "      <td>0.684338</td>\n",
              "      <td>-0.336780</td>\n",
              "      <td>0.639505</td>\n",
              "      <td>-0.991373</td>\n",
              "      <td>-0.107552</td>\n",
              "      <td>-0.421237</td>\n",
              "      <td>0.096745</td>\n",
              "      <td>-0.594588</td>\n",
              "      <td>-0.854068</td>\n",
              "      <td>0.995786</td>\n",
              "      <td>0.799913</td>\n",
              "      <td>-0.549937</td>\n",
              "      <td>0.322106</td>\n",
              "      <td>-0.191949</td>\n",
              "      <td>0.215680</td>\n",
              "      <td>0.769107</td>\n",
              "      <td>0.367288</td>\n",
              "      <td>-0.749021</td>\n",
              "      <td>-0.765478</td>\n",
              "      <td>-0.792780</td>\n",
              "      <td>-0.298891</td>\n",
              "      <td>-1.126784</td>\n",
              "      <td>-0.639824</td>\n",
              "      <td>0.049366</td>\n",
              "      <td>0.452313</td>\n",
              "      <td>0.643203</td>\n",
              "      <td>0.746429</td>\n",
              "      <td>1.086678</td>\n",
              "      <td>0.236849</td>\n",
              "      <td>0.340452</td>\n",
              "      <td>-0.757036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41943048</td>\n",
              "      <td>1.270913</td>\n",
              "      <td>-0.315748</td>\n",
              "      <td>0.310227</td>\n",
              "      <td>0.026623</td>\n",
              "      <td>0.540356</td>\n",
              "      <td>-0.338011</td>\n",
              "      <td>1.016998</td>\n",
              "      <td>-0.387781</td>\n",
              "      <td>0.143474</td>\n",
              "      <td>0.202624</td>\n",
              "      <td>0.715351</td>\n",
              "      <td>0.482565</td>\n",
              "      <td>0.445205</td>\n",
              "      <td>0.491785</td>\n",
              "      <td>-0.095650</td>\n",
              "      <td>1.701916</td>\n",
              "      <td>1.017774</td>\n",
              "      <td>-0.534539</td>\n",
              "      <td>-0.703506</td>\n",
              "      <td>0.593074</td>\n",
              "      <td>-0.154503</td>\n",
              "      <td>0.351308</td>\n",
              "      <td>-0.245392</td>\n",
              "      <td>-0.343607</td>\n",
              "      <td>-0.413617</td>\n",
              "      <td>-0.701321</td>\n",
              "      <td>0.869906</td>\n",
              "      <td>-0.866260</td>\n",
              "      <td>0.141226</td>\n",
              "      <td>0.413044</td>\n",
              "      <td>-0.405591</td>\n",
              "      <td>0.228429</td>\n",
              "      <td>-0.538031</td>\n",
              "      <td>0.028628</td>\n",
              "      <td>-0.065368</td>\n",
              "      <td>0.267358</td>\n",
              "      <td>-0.335964</td>\n",
              "      <td>0.804797</td>\n",
              "      <td>-0.465212</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.440811</td>\n",
              "      <td>-0.735010</td>\n",
              "      <td>-0.577510</td>\n",
              "      <td>-0.943832</td>\n",
              "      <td>-0.320501</td>\n",
              "      <td>0.915926</td>\n",
              "      <td>0.278528</td>\n",
              "      <td>0.335984</td>\n",
              "      <td>0.417562</td>\n",
              "      <td>0.226836</td>\n",
              "      <td>0.668925</td>\n",
              "      <td>-0.603946</td>\n",
              "      <td>0.431352</td>\n",
              "      <td>-0.772272</td>\n",
              "      <td>0.440270</td>\n",
              "      <td>-0.017126</td>\n",
              "      <td>1.527564</td>\n",
              "      <td>-0.749838</td>\n",
              "      <td>-0.310076</td>\n",
              "      <td>0.705478</td>\n",
              "      <td>-0.604189</td>\n",
              "      <td>0.564075</td>\n",
              "      <td>0.439398</td>\n",
              "      <td>0.940764</td>\n",
              "      <td>0.129746</td>\n",
              "      <td>-0.299077</td>\n",
              "      <td>0.372555</td>\n",
              "      <td>0.524091</td>\n",
              "      <td>0.050374</td>\n",
              "      <td>-0.245328</td>\n",
              "      <td>0.645308</td>\n",
              "      <td>-0.518049</td>\n",
              "      <td>-0.118645</td>\n",
              "      <td>-1.096815</td>\n",
              "      <td>0.365355</td>\n",
              "      <td>0.902132</td>\n",
              "      <td>-0.157883</td>\n",
              "      <td>-0.275473</td>\n",
              "      <td>-0.112284</td>\n",
              "      <td>0.111466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2014838794</td>\n",
              "      <td>-0.099550</td>\n",
              "      <td>0.083398</td>\n",
              "      <td>-0.036287</td>\n",
              "      <td>0.462367</td>\n",
              "      <td>0.331648</td>\n",
              "      <td>-0.075080</td>\n",
              "      <td>0.256208</td>\n",
              "      <td>-0.046188</td>\n",
              "      <td>-0.445900</td>\n",
              "      <td>0.096864</td>\n",
              "      <td>0.297487</td>\n",
              "      <td>-0.135837</td>\n",
              "      <td>0.327910</td>\n",
              "      <td>0.049031</td>\n",
              "      <td>-0.211283</td>\n",
              "      <td>0.212004</td>\n",
              "      <td>-0.154033</td>\n",
              "      <td>0.414642</td>\n",
              "      <td>0.425927</td>\n",
              "      <td>-0.353696</td>\n",
              "      <td>0.052132</td>\n",
              "      <td>-0.157554</td>\n",
              "      <td>-0.081566</td>\n",
              "      <td>-0.106935</td>\n",
              "      <td>-0.026086</td>\n",
              "      <td>-0.211331</td>\n",
              "      <td>-0.331683</td>\n",
              "      <td>-0.198246</td>\n",
              "      <td>-0.348588</td>\n",
              "      <td>0.440369</td>\n",
              "      <td>-0.429626</td>\n",
              "      <td>-0.324424</td>\n",
              "      <td>-0.169719</td>\n",
              "      <td>0.098839</td>\n",
              "      <td>0.220807</td>\n",
              "      <td>0.351054</td>\n",
              "      <td>-0.061675</td>\n",
              "      <td>-0.119106</td>\n",
              "      <td>0.119979</td>\n",
              "      <td>...</td>\n",
              "      <td>0.181966</td>\n",
              "      <td>0.129280</td>\n",
              "      <td>0.683205</td>\n",
              "      <td>-0.082316</td>\n",
              "      <td>0.193911</td>\n",
              "      <td>-0.019069</td>\n",
              "      <td>-0.110586</td>\n",
              "      <td>0.185420</td>\n",
              "      <td>0.044359</td>\n",
              "      <td>0.238117</td>\n",
              "      <td>0.096932</td>\n",
              "      <td>0.262903</td>\n",
              "      <td>-0.417044</td>\n",
              "      <td>0.109975</td>\n",
              "      <td>-0.234698</td>\n",
              "      <td>-0.551077</td>\n",
              "      <td>-0.369400</td>\n",
              "      <td>0.333238</td>\n",
              "      <td>0.155876</td>\n",
              "      <td>0.230078</td>\n",
              "      <td>0.147910</td>\n",
              "      <td>0.101246</td>\n",
              "      <td>-0.000578</td>\n",
              "      <td>-0.104032</td>\n",
              "      <td>-0.030537</td>\n",
              "      <td>-0.250839</td>\n",
              "      <td>-0.226444</td>\n",
              "      <td>-0.208137</td>\n",
              "      <td>0.637890</td>\n",
              "      <td>0.402661</td>\n",
              "      <td>0.232916</td>\n",
              "      <td>-0.122200</td>\n",
              "      <td>-0.049734</td>\n",
              "      <td>-0.341676</td>\n",
              "      <td>-0.034348</td>\n",
              "      <td>-0.003209</td>\n",
              "      <td>0.079313</td>\n",
              "      <td>0.556014</td>\n",
              "      <td>-0.144683</td>\n",
              "      <td>0.136037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231234</th>\n",
              "      <td>2343043062</td>\n",
              "      <td>-0.300804</td>\n",
              "      <td>-0.404748</td>\n",
              "      <td>-0.154969</td>\n",
              "      <td>0.644117</td>\n",
              "      <td>0.538103</td>\n",
              "      <td>0.100497</td>\n",
              "      <td>0.252018</td>\n",
              "      <td>-0.526020</td>\n",
              "      <td>0.387659</td>\n",
              "      <td>-0.166707</td>\n",
              "      <td>0.719887</td>\n",
              "      <td>-0.221859</td>\n",
              "      <td>0.701711</td>\n",
              "      <td>0.340344</td>\n",
              "      <td>0.080662</td>\n",
              "      <td>0.123299</td>\n",
              "      <td>0.299167</td>\n",
              "      <td>0.238420</td>\n",
              "      <td>-0.849155</td>\n",
              "      <td>0.005476</td>\n",
              "      <td>-1.009497</td>\n",
              "      <td>-0.366798</td>\n",
              "      <td>0.092266</td>\n",
              "      <td>0.279540</td>\n",
              "      <td>0.679201</td>\n",
              "      <td>0.107634</td>\n",
              "      <td>-0.010949</td>\n",
              "      <td>0.420014</td>\n",
              "      <td>1.074489</td>\n",
              "      <td>-0.089427</td>\n",
              "      <td>-0.226461</td>\n",
              "      <td>0.720864</td>\n",
              "      <td>0.810319</td>\n",
              "      <td>-0.304045</td>\n",
              "      <td>-0.117362</td>\n",
              "      <td>0.236684</td>\n",
              "      <td>-0.359596</td>\n",
              "      <td>-0.543027</td>\n",
              "      <td>-0.159319</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.408133</td>\n",
              "      <td>-0.653197</td>\n",
              "      <td>1.551010</td>\n",
              "      <td>-0.097699</td>\n",
              "      <td>0.149880</td>\n",
              "      <td>-0.032251</td>\n",
              "      <td>0.882579</td>\n",
              "      <td>0.571870</td>\n",
              "      <td>0.736243</td>\n",
              "      <td>-0.037667</td>\n",
              "      <td>0.211727</td>\n",
              "      <td>-0.705546</td>\n",
              "      <td>0.015499</td>\n",
              "      <td>-0.659414</td>\n",
              "      <td>0.175624</td>\n",
              "      <td>-0.523628</td>\n",
              "      <td>0.272440</td>\n",
              "      <td>-0.454787</td>\n",
              "      <td>-0.337871</td>\n",
              "      <td>0.225292</td>\n",
              "      <td>-0.493610</td>\n",
              "      <td>0.118983</td>\n",
              "      <td>-0.212427</td>\n",
              "      <td>-0.433567</td>\n",
              "      <td>-0.115970</td>\n",
              "      <td>-0.149601</td>\n",
              "      <td>0.307105</td>\n",
              "      <td>0.008205</td>\n",
              "      <td>-0.146113</td>\n",
              "      <td>-0.244394</td>\n",
              "      <td>-0.404295</td>\n",
              "      <td>0.014890</td>\n",
              "      <td>0.643550</td>\n",
              "      <td>0.205918</td>\n",
              "      <td>-0.233266</td>\n",
              "      <td>0.231023</td>\n",
              "      <td>0.235246</td>\n",
              "      <td>-0.373278</td>\n",
              "      <td>0.050300</td>\n",
              "      <td>0.134854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231235</th>\n",
              "      <td>2184183797</td>\n",
              "      <td>0.215280</td>\n",
              "      <td>0.488684</td>\n",
              "      <td>-0.465959</td>\n",
              "      <td>-0.639169</td>\n",
              "      <td>0.142733</td>\n",
              "      <td>-0.556257</td>\n",
              "      <td>-0.324164</td>\n",
              "      <td>0.024629</td>\n",
              "      <td>0.048298</td>\n",
              "      <td>-0.011312</td>\n",
              "      <td>0.699444</td>\n",
              "      <td>0.074882</td>\n",
              "      <td>0.758682</td>\n",
              "      <td>0.888488</td>\n",
              "      <td>0.772629</td>\n",
              "      <td>0.776980</td>\n",
              "      <td>0.055378</td>\n",
              "      <td>-0.203762</td>\n",
              "      <td>-1.205883</td>\n",
              "      <td>0.976079</td>\n",
              "      <td>0.772860</td>\n",
              "      <td>-0.697435</td>\n",
              "      <td>-0.598755</td>\n",
              "      <td>-0.202012</td>\n",
              "      <td>0.752455</td>\n",
              "      <td>-0.402755</td>\n",
              "      <td>0.181421</td>\n",
              "      <td>0.724873</td>\n",
              "      <td>-0.139080</td>\n",
              "      <td>0.072853</td>\n",
              "      <td>0.429010</td>\n",
              "      <td>0.337635</td>\n",
              "      <td>-0.362737</td>\n",
              "      <td>0.623929</td>\n",
              "      <td>0.102195</td>\n",
              "      <td>-0.014824</td>\n",
              "      <td>-0.035087</td>\n",
              "      <td>-1.025689</td>\n",
              "      <td>1.080275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269747</td>\n",
              "      <td>0.201039</td>\n",
              "      <td>0.676047</td>\n",
              "      <td>-0.182680</td>\n",
              "      <td>0.348730</td>\n",
              "      <td>0.487845</td>\n",
              "      <td>-0.457833</td>\n",
              "      <td>0.117272</td>\n",
              "      <td>-0.117204</td>\n",
              "      <td>0.362769</td>\n",
              "      <td>0.357715</td>\n",
              "      <td>0.449958</td>\n",
              "      <td>-1.339291</td>\n",
              "      <td>0.102003</td>\n",
              "      <td>-0.172820</td>\n",
              "      <td>-0.496586</td>\n",
              "      <td>0.057882</td>\n",
              "      <td>0.148015</td>\n",
              "      <td>-0.388629</td>\n",
              "      <td>-0.567518</td>\n",
              "      <td>0.717722</td>\n",
              "      <td>0.178860</td>\n",
              "      <td>0.227660</td>\n",
              "      <td>-0.036984</td>\n",
              "      <td>-0.431410</td>\n",
              "      <td>0.347519</td>\n",
              "      <td>-0.566312</td>\n",
              "      <td>-0.276590</td>\n",
              "      <td>0.171467</td>\n",
              "      <td>0.213625</td>\n",
              "      <td>-0.409809</td>\n",
              "      <td>-0.058072</td>\n",
              "      <td>-1.152791</td>\n",
              "      <td>0.734739</td>\n",
              "      <td>-0.387874</td>\n",
              "      <td>0.130348</td>\n",
              "      <td>0.009563</td>\n",
              "      <td>0.242793</td>\n",
              "      <td>-0.380453</td>\n",
              "      <td>-0.534975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231236</th>\n",
              "      <td>2763522041</td>\n",
              "      <td>-0.555273</td>\n",
              "      <td>-0.377701</td>\n",
              "      <td>-0.476636</td>\n",
              "      <td>0.370934</td>\n",
              "      <td>-0.912233</td>\n",
              "      <td>-0.382113</td>\n",
              "      <td>-0.192715</td>\n",
              "      <td>-0.217987</td>\n",
              "      <td>-0.223319</td>\n",
              "      <td>0.564571</td>\n",
              "      <td>0.098607</td>\n",
              "      <td>-0.435840</td>\n",
              "      <td>0.381581</td>\n",
              "      <td>0.213167</td>\n",
              "      <td>-0.043864</td>\n",
              "      <td>0.789726</td>\n",
              "      <td>0.118797</td>\n",
              "      <td>0.010710</td>\n",
              "      <td>-0.016437</td>\n",
              "      <td>-0.764596</td>\n",
              "      <td>0.394369</td>\n",
              "      <td>-0.045595</td>\n",
              "      <td>0.174212</td>\n",
              "      <td>-0.626907</td>\n",
              "      <td>0.130017</td>\n",
              "      <td>-0.562354</td>\n",
              "      <td>0.064994</td>\n",
              "      <td>-0.751530</td>\n",
              "      <td>-0.274063</td>\n",
              "      <td>-0.898561</td>\n",
              "      <td>-0.410279</td>\n",
              "      <td>-0.656243</td>\n",
              "      <td>0.198123</td>\n",
              "      <td>0.686738</td>\n",
              "      <td>-0.350377</td>\n",
              "      <td>0.828006</td>\n",
              "      <td>0.446368</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>-0.295250</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059625</td>\n",
              "      <td>-0.069563</td>\n",
              "      <td>0.677934</td>\n",
              "      <td>-0.157942</td>\n",
              "      <td>0.015627</td>\n",
              "      <td>-0.142168</td>\n",
              "      <td>0.372774</td>\n",
              "      <td>0.356886</td>\n",
              "      <td>0.341095</td>\n",
              "      <td>-0.398264</td>\n",
              "      <td>-0.240679</td>\n",
              "      <td>-0.677603</td>\n",
              "      <td>0.267418</td>\n",
              "      <td>-0.758932</td>\n",
              "      <td>-0.510102</td>\n",
              "      <td>0.351365</td>\n",
              "      <td>-0.206561</td>\n",
              "      <td>-0.564358</td>\n",
              "      <td>-0.047687</td>\n",
              "      <td>0.639995</td>\n",
              "      <td>0.383085</td>\n",
              "      <td>-0.149294</td>\n",
              "      <td>-0.138021</td>\n",
              "      <td>-0.165049</td>\n",
              "      <td>-0.931451</td>\n",
              "      <td>0.197603</td>\n",
              "      <td>0.365336</td>\n",
              "      <td>-0.114640</td>\n",
              "      <td>-0.353416</td>\n",
              "      <td>0.209989</td>\n",
              "      <td>-0.585165</td>\n",
              "      <td>-0.274999</td>\n",
              "      <td>0.503482</td>\n",
              "      <td>0.269771</td>\n",
              "      <td>-0.365511</td>\n",
              "      <td>0.759070</td>\n",
              "      <td>-0.307194</td>\n",
              "      <td>0.205406</td>\n",
              "      <td>0.433426</td>\n",
              "      <td>0.030907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231237</th>\n",
              "      <td>2231894011</td>\n",
              "      <td>0.200399</td>\n",
              "      <td>-0.130658</td>\n",
              "      <td>-0.058943</td>\n",
              "      <td>0.404701</td>\n",
              "      <td>-0.649133</td>\n",
              "      <td>-0.559879</td>\n",
              "      <td>0.532233</td>\n",
              "      <td>-1.703480</td>\n",
              "      <td>-0.258006</td>\n",
              "      <td>0.556872</td>\n",
              "      <td>0.903159</td>\n",
              "      <td>-0.678360</td>\n",
              "      <td>-0.812204</td>\n",
              "      <td>-1.061112</td>\n",
              "      <td>0.741415</td>\n",
              "      <td>0.413224</td>\n",
              "      <td>0.112414</td>\n",
              "      <td>0.084462</td>\n",
              "      <td>-0.744321</td>\n",
              "      <td>0.446404</td>\n",
              "      <td>0.340602</td>\n",
              "      <td>-0.401095</td>\n",
              "      <td>-0.065610</td>\n",
              "      <td>0.009575</td>\n",
              "      <td>-0.190284</td>\n",
              "      <td>-0.641656</td>\n",
              "      <td>-0.244612</td>\n",
              "      <td>0.048766</td>\n",
              "      <td>-0.586671</td>\n",
              "      <td>0.123552</td>\n",
              "      <td>-0.569089</td>\n",
              "      <td>-0.572668</td>\n",
              "      <td>0.921661</td>\n",
              "      <td>0.328598</td>\n",
              "      <td>-0.294767</td>\n",
              "      <td>0.499488</td>\n",
              "      <td>-0.361881</td>\n",
              "      <td>-1.168720</td>\n",
              "      <td>0.841622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.279321</td>\n",
              "      <td>-0.305534</td>\n",
              "      <td>-0.353894</td>\n",
              "      <td>0.259638</td>\n",
              "      <td>-0.766640</td>\n",
              "      <td>0.961987</td>\n",
              "      <td>0.952217</td>\n",
              "      <td>1.052562</td>\n",
              "      <td>0.341378</td>\n",
              "      <td>-0.856903</td>\n",
              "      <td>0.994949</td>\n",
              "      <td>0.870674</td>\n",
              "      <td>0.413377</td>\n",
              "      <td>0.071956</td>\n",
              "      <td>0.089989</td>\n",
              "      <td>0.261608</td>\n",
              "      <td>-0.440542</td>\n",
              "      <td>0.906597</td>\n",
              "      <td>-0.049476</td>\n",
              "      <td>-0.765646</td>\n",
              "      <td>-0.566669</td>\n",
              "      <td>0.367405</td>\n",
              "      <td>0.062754</td>\n",
              "      <td>-0.050627</td>\n",
              "      <td>-0.780302</td>\n",
              "      <td>0.658460</td>\n",
              "      <td>0.031260</td>\n",
              "      <td>-0.046778</td>\n",
              "      <td>-1.628822</td>\n",
              "      <td>-0.643813</td>\n",
              "      <td>-0.837343</td>\n",
              "      <td>-1.681839</td>\n",
              "      <td>0.026580</td>\n",
              "      <td>0.855661</td>\n",
              "      <td>0.270307</td>\n",
              "      <td>0.712514</td>\n",
              "      <td>1.306025</td>\n",
              "      <td>0.355903</td>\n",
              "      <td>-0.528477</td>\n",
              "      <td>0.710708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231238</th>\n",
              "      <td>2464677886</td>\n",
              "      <td>-0.477532</td>\n",
              "      <td>-0.280744</td>\n",
              "      <td>-0.380817</td>\n",
              "      <td>0.218292</td>\n",
              "      <td>-0.227769</td>\n",
              "      <td>-0.014909</td>\n",
              "      <td>0.069212</td>\n",
              "      <td>-0.193575</td>\n",
              "      <td>0.283849</td>\n",
              "      <td>0.173135</td>\n",
              "      <td>0.397100</td>\n",
              "      <td>-0.074648</td>\n",
              "      <td>-0.376484</td>\n",
              "      <td>0.269478</td>\n",
              "      <td>-0.443462</td>\n",
              "      <td>-0.039497</td>\n",
              "      <td>-0.024258</td>\n",
              "      <td>0.333629</td>\n",
              "      <td>0.272531</td>\n",
              "      <td>0.015635</td>\n",
              "      <td>-0.172989</td>\n",
              "      <td>0.088663</td>\n",
              "      <td>-0.063534</td>\n",
              "      <td>0.097301</td>\n",
              "      <td>-0.233146</td>\n",
              "      <td>-0.070287</td>\n",
              "      <td>0.155937</td>\n",
              "      <td>-0.297828</td>\n",
              "      <td>0.093201</td>\n",
              "      <td>0.020557</td>\n",
              "      <td>-0.083272</td>\n",
              "      <td>-0.041544</td>\n",
              "      <td>0.125945</td>\n",
              "      <td>-0.042620</td>\n",
              "      <td>-0.030763</td>\n",
              "      <td>0.156485</td>\n",
              "      <td>0.090644</td>\n",
              "      <td>0.106912</td>\n",
              "      <td>0.088282</td>\n",
              "      <td>...</td>\n",
              "      <td>0.343780</td>\n",
              "      <td>-0.012576</td>\n",
              "      <td>-0.026899</td>\n",
              "      <td>0.001955</td>\n",
              "      <td>0.022881</td>\n",
              "      <td>-0.143012</td>\n",
              "      <td>-0.410088</td>\n",
              "      <td>0.374311</td>\n",
              "      <td>0.158656</td>\n",
              "      <td>0.177369</td>\n",
              "      <td>0.041750</td>\n",
              "      <td>0.210083</td>\n",
              "      <td>-0.263943</td>\n",
              "      <td>0.292431</td>\n",
              "      <td>0.173214</td>\n",
              "      <td>-0.124575</td>\n",
              "      <td>0.085696</td>\n",
              "      <td>0.056349</td>\n",
              "      <td>-0.015379</td>\n",
              "      <td>-0.278145</td>\n",
              "      <td>-0.118087</td>\n",
              "      <td>0.122194</td>\n",
              "      <td>-0.151908</td>\n",
              "      <td>-0.112376</td>\n",
              "      <td>-0.299787</td>\n",
              "      <td>0.323317</td>\n",
              "      <td>-0.185543</td>\n",
              "      <td>0.011447</td>\n",
              "      <td>0.341579</td>\n",
              "      <td>-0.320428</td>\n",
              "      <td>0.023443</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.137211</td>\n",
              "      <td>0.021590</td>\n",
              "      <td>-0.255542</td>\n",
              "      <td>0.197564</td>\n",
              "      <td>-0.028556</td>\n",
              "      <td>0.272086</td>\n",
              "      <td>-0.439930</td>\n",
              "      <td>-0.060896</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>231239 rows Ã— 257 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          authorID         0         1  ...       253       254       255\n",
              "0       2811232256  0.078322  0.425899  ... -0.127464 -0.111425  0.410204\n",
              "1       2231369732 -0.471883  0.559586  ...  0.274213 -0.480836  0.240680\n",
              "2       2138570756 -0.348740 -0.636298  ...  0.236849  0.340452 -0.757036\n",
              "3         41943048  1.270913 -0.315748  ... -0.275473 -0.112284  0.111466\n",
              "4       2014838794 -0.099550  0.083398  ...  0.556014 -0.144683  0.136037\n",
              "...            ...       ...       ...  ...       ...       ...       ...\n",
              "231234  2343043062 -0.300804 -0.404748  ... -0.373278  0.050300  0.134854\n",
              "231235  2184183797  0.215280  0.488684  ...  0.242793 -0.380453 -0.534975\n",
              "231236  2763522041 -0.555273 -0.377701  ...  0.205406  0.433426  0.030907\n",
              "231237  2231894011  0.200399 -0.130658  ...  0.355903 -0.528477  0.710708\n",
              "231238  2464677886 -0.477532 -0.280744  ...  0.272086 -0.439930 -0.060896\n",
              "\n",
              "[231239 rows x 257 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "stream",
          "text": [
            "time: 34.5 s (started: 2021-03-04 12:30:03 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg4FluqSyvcK",
        "outputId": "94172fca-442e-44b4-f7ba-70fbadbacb25"
      },
      "source": [
        "# Merge train data\r\n",
        "X_train_ = df_train.merge(X_train_graph, on=\"authorID\")\r\n",
        "X_train_embeddings = embeddings_abstracts.merge(embeddings_nodes, on=\"authorID\")\r\n",
        "X_train_embeddings = X_train_embeddings.query(\"authorID != @authors_to_drop\")\r\n",
        "X_train = X_train_.merge(X_train_embeddings, on=\"authorID\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.66 s (started: 2021-03-04 12:30:38 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWVUpBL92vFn",
        "outputId": "2ad1bc1c-814c-45d2-90ad-e6388b5f7ca5"
      },
      "source": [
        "# Merge test data\r\n",
        "X_test_ = df_test.merge(X_test_graph, on=\"authorID\")\r\n",
        "X_test_embeddings = embeddings_abstracts.merge(embeddings_nodes, on=\"authorID\")\r\n",
        "X_test = X_test_.merge(X_test_embeddings, on=\"authorID\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.81 s (started: 2021-03-04 12:30:47 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEFABJ1dv3Yd"
      },
      "source": [
        "# MLP modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqwTdl1HIdo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11650f11-b1e8-4fd4-db3b-8a8b0eb52f5b"
      },
      "source": [
        "import torch\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "\r\n",
        "seed = 42\r\n",
        "random.seed(seed)\r\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
        "os.environ[\"SEED\"] = str(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "torch.manual_seed(seed)\r\n",
        "torch.cuda.manual_seed(seed)\r\n",
        "torch.backends.cudnn.deterministic = True\r\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10.9 ms (started: 2021-03-04 12:30:53 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5V13hh-m90UV",
        "outputId": "12f581bd-345c-4ef2-f45e-503e7063c892"
      },
      "source": [
        "# Removing h_index and author_id\r\n",
        "y_train = X_train[\"h_index\"]\r\n",
        "X_train.drop(columns=[\"authorID\", \"h_index\", \"paper_per_author_weighted_std\"], inplace=True)\r\n",
        "print(X_train.head())\r\n",
        "\r\n",
        "X_test.drop(columns=[\"authorID\", \"h_index_pred\", \"paper_per_author_weighted_std\"], inplace=True)\r\n",
        "print(X_test.head())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   paper_per_author  degree  core_number  ...       253       254       255\n",
            "0                 4     3.0          3.0  ... -0.075822 -0.324128 -0.069239\n",
            "1                10     5.0          5.0  ...  0.386224 -0.457021 -0.055061\n",
            "2                 1     5.0          5.0  ... -0.044442 -0.362217  0.777577\n",
            "3                 1     3.0          3.0  ... -0.004279 -0.124086  0.478126\n",
            "4                10     4.0          2.0  ...  0.585561  0.396294 -0.473556\n",
            "\n",
            "[5 rows x 1297 columns]\n",
            "   paper_per_author  degree  core_number  ...       253       254       255\n",
            "0                10    16.0          5.0  ... -1.014556  0.080996  0.027389\n",
            "1                10     2.0          2.0  ... -0.182785  0.037005 -0.102235\n",
            "2                10   107.0         13.0  ... -0.020704 -0.425805  0.037034\n",
            "3                10     3.0          3.0  ... -0.242449 -1.076120  0.245068\n",
            "4                 1     2.0          2.0  ... -0.426978 -1.795817  0.696226\n",
            "\n",
            "[5 rows x 1297 columns]\n",
            "time: 2.11 s (started: 2021-03-04 12:30:53 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh2JAGg-8l86",
        "outputId": "ffb27abf-8ccb-4fc2-b803-5eaa9c616c97"
      },
      "source": [
        "# scale with mean and std reduction\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
        "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
        ")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.17 s (started: 2021-03-04 12:30:56 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qRI1rU8pmpU",
        "outputId": "d5d0d922-c000-4ea6-aa0b-9f3b21929cfa"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import DataLoader, TensorDataset\r\n",
        "\r\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "train_dataset = TensorDataset(\r\n",
        "    torch.tensor(X_train_final, dtype=torch.float),\r\n",
        "    torch.tensor(y_train_final.values)\r\n",
        ")\r\n",
        "\r\n",
        "test_dataset = TensorDataset(\r\n",
        "    torch.tensor(X_test_final, dtype=torch.float),\r\n",
        "    torch.tensor(y_test_final.values)\r\n",
        ")\r\n",
        "\r\n",
        "train_loader = DataLoader(\r\n",
        "    train_dataset, batch_size=512, shuffle=True,\r\n",
        ")\r\n",
        "test_loader = DataLoader(\r\n",
        "    test_dataset, batch_size=512, shuffle=False,\r\n",
        ")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 207 ms (started: 2021-03-04 12:31:00 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaDq0W1H5H0a",
        "outputId": "94343e87-6530-4095-fc8e-9aab01d56e5d"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class MaximNet(nn.Module):\r\n",
        "    def __init__(self, hidden_channels=700):\r\n",
        "        super(MaximNet, self).__init__()\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            nn.Linear(X_train_final.shape[-1], 1082),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(1082),\r\n",
        "            nn.Dropout(0.3397733184132094),            \r\n",
        "            nn.Linear(1082, 495),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(495),\r\n",
        "            nn.Dropout(0.34941637481636484),\r\n",
        "            nn.Linear(495, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x).squeeze(1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.15 ms (started: 2021-03-04 12:31:00 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaJWpemjqZmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685f843e-5ccd-443c-f8c8-5a76667c3413"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "class AlexNet(nn.Module):\r\n",
        "    def __init__(self, dropout=0.5, hidden_channels=700):\r\n",
        "        super(MLPModel, self).__init__()\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            nn.Linear(X_train_final.shape[-1], hidden_channels),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(hidden_channels),\r\n",
        "            nn.Dropout(dropout),            \r\n",
        "            nn.Linear(hidden_channels, hidden_channels // 2),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(hidden_channels // 2),\r\n",
        "            nn.Dropout(dropout),\r\n",
        "            nn.Linear(hidden_channels // 2, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x).squeeze(1)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 7.06 ms (started: 2021-03-04 12:31:00 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "364-VitYvKbS",
        "outputId": "055b200e-9ca2-4410-e00d-80c8ac033a45"
      },
      "source": [
        "class SoumNet(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(SoumNet, self).__init__()\r\n",
        "        self.model = nn.Sequential(\r\n",
        "            nn.Linear(X_train_final.shape[-1], 1437),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(1437),\r\n",
        "            nn.Dropout(0.32922811714191247),            \r\n",
        "            nn.Linear(1437, 981),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(981),\r\n",
        "            nn.Dropout(0.6083589126382183),\r\n",
        "            nn.Linear(981, 1903),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(1903),\r\n",
        "            nn.Dropout(0.28454930321217037),\r\n",
        "            nn.Linear(1903, 1298),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.BatchNorm1d(1298),\r\n",
        "            nn.Dropout(0.40523335468427923),\r\n",
        "            nn.Linear(1298, 1),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x).squeeze(1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 10.6 ms (started: 2021-03-04 12:31:00 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t81kpoyR0vpp",
        "outputId": "58db0513-3dd5-4fb9-9761-6e76e7183213"
      },
      "source": [
        "criterion = torch.nn.L1Loss()  # Define loss criterion.\r\n",
        "model = MaximNet().to(DEVICE)\r\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.95, weight_decay=5e-4, nesterov=True) for AlexNet\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007660119153202037)\r\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.00018092648234365849, weight_decay=0.00021648264694047528) for SoumNet\r\n",
        "n_epochs = 2000"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 9.75 s (started: 2021-03-04 12:31:00 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHjyNbFf0NTK",
        "outputId": "c6c2237d-0860-45cd-a76a-2aeb6f9ae8d5"
      },
      "source": [
        "def train_test(model, optimizer, criterion, n_epochs):\r\n",
        "    best_loss = float('inf')\r\n",
        "    best_epoch = 0\r\n",
        "\r\n",
        "    for epoch in range(1, n_epochs+1):\r\n",
        "        total_loss = 0\r\n",
        "        model.train()\r\n",
        "        for (x, y) in train_loader:\r\n",
        "            x = x.to(DEVICE)\r\n",
        "            y = y.to(DEVICE)\r\n",
        "            optimizer.zero_grad()  # Clear gradients.\r\n",
        "            out = model(x)  # Perform a single forward pass.\r\n",
        "            loss = criterion(out, y)  # Compute the loss solely based on the training nodes\r\n",
        "            loss.backward()  # Derive gradients.\r\n",
        "            optimizer.step()\r\n",
        "            total_loss += loss.item()\r\n",
        "        test_loss = 0\r\n",
        "        model.eval()\r\n",
        "        for (x, y) in test_loader:\r\n",
        "            x = x.to(DEVICE)\r\n",
        "            y = y.to(DEVICE)\r\n",
        "            pred = model(x)  # Perform a single forward pass.\r\n",
        "            loss = criterion(pred, y)  # Compute the loss solely based on the training nodes\r\n",
        "            test_loss += loss.item()\r\n",
        "        final_test_loss = test_loss/len(test_loader)\r\n",
        "        print(f'Epoch: {epoch}, Train Loss: {total_loss/len(train_loader):.4f}, Test Loss: {final_test_loss}')\r\n",
        "        if final_test_loss < best_loss:\r\n",
        "            best_loss = final_test_loss\r\n",
        "            best_epoch = epoch\r\n",
        "            # y_pred_save = model(torch.tensor(X_test_scaled, dtype=torch.float).to(DEVICE))\r\n",
        "            # y_pred_save = y_pred_save.cpu().detach().numpy()\r\n",
        "            # df_test['h_index_pred'].update(pd.Series(np.rint(y_pred_save)))\r\n",
        "            # df_test.loc[:, [\"authorID\", \"h_index_pred\"]].to_csv(\r\n",
        "            #     f'predictions_MLP_concat_abstracts.csv', index=False\r\n",
        "            # )\r\n",
        "            torch.save(model, \"final_net.pth\")\r\n",
        "    print(f\"Epoch: {best_epoch} -> Best loss: {best_loss}\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 24.2 ms (started: 2021-03-04 12:31:10 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YGdnJ-xHKVh",
        "outputId": "8985950b-f563-479a-84af-58982b9fbc0c"
      },
      "source": [
        "train_test(model, optimizer, criterion, n_epochs)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 9.4047, Test Loss: 9.101816129684448\n",
            "Epoch: 2, Train Loss: 8.7345, Test Loss: 7.763346481323242\n",
            "Epoch: 3, Train Loss: 7.2201, Test Loss: 6.210181188583374\n",
            "Epoch: 4, Train Loss: 5.4350, Test Loss: 4.5398801326751705\n",
            "Epoch: 5, Train Loss: 4.4026, Test Loss: 3.948401403427124\n",
            "Epoch: 6, Train Loss: 4.0049, Test Loss: 3.767803359031677\n",
            "Epoch: 7, Train Loss: 3.7320, Test Loss: 3.716431450843811\n",
            "Epoch: 8, Train Loss: 3.5363, Test Loss: 3.7068888902664185\n",
            "Epoch: 9, Train Loss: 3.4446, Test Loss: 3.615803265571594\n",
            "Epoch: 10, Train Loss: 3.3102, Test Loss: 3.5986692667007447\n",
            "Epoch: 11, Train Loss: 3.1635, Test Loss: 3.4912639617919923\n",
            "Epoch: 12, Train Loss: 3.0905, Test Loss: 3.626188063621521\n",
            "Epoch: 13, Train Loss: 2.9962, Test Loss: 3.6050584077835084\n",
            "Epoch: 14, Train Loss: 2.9121, Test Loss: 3.589313817024231\n",
            "Epoch: 15, Train Loss: 2.8020, Test Loss: 3.5727709770202636\n",
            "Epoch: 16, Train Loss: 2.7369, Test Loss: 3.601625895500183\n",
            "Epoch: 17, Train Loss: 2.6863, Test Loss: 3.537149500846863\n",
            "Epoch: 18, Train Loss: 2.7313, Test Loss: 3.531098413467407\n",
            "Epoch: 19, Train Loss: 2.5091, Test Loss: 3.6021650314331053\n",
            "Epoch: 20, Train Loss: 2.5387, Test Loss: 3.604032802581787\n",
            "Epoch: 21, Train Loss: 2.4693, Test Loss: 3.4714587926864624\n",
            "Epoch: 22, Train Loss: 2.3810, Test Loss: 3.556320810317993\n",
            "Epoch: 23, Train Loss: 2.3516, Test Loss: 3.604327082633972\n",
            "Epoch: 24, Train Loss: 2.3066, Test Loss: 3.5321428537368775\n",
            "Epoch: 25, Train Loss: 2.2291, Test Loss: 3.4999179601669312\n",
            "Epoch: 26, Train Loss: 2.2132, Test Loss: 3.4907498121261598\n",
            "Epoch: 27, Train Loss: 2.1783, Test Loss: 3.483734703063965\n",
            "Epoch: 28, Train Loss: 2.1468, Test Loss: 3.5640100717544554\n",
            "Epoch: 29, Train Loss: 2.1355, Test Loss: 3.5004633188247682\n",
            "Epoch: 30, Train Loss: 2.1606, Test Loss: 3.5110674142837524\n",
            "Epoch: 31, Train Loss: 2.0619, Test Loss: 3.5446462392807008\n",
            "Epoch: 32, Train Loss: 2.0400, Test Loss: 3.5293883085250854\n",
            "Epoch: 33, Train Loss: 2.0334, Test Loss: 3.4426029682159425\n",
            "Epoch: 34, Train Loss: 2.0397, Test Loss: 3.4937732219696045\n",
            "Epoch: 35, Train Loss: 1.9416, Test Loss: 3.493430995941162\n",
            "Epoch: 36, Train Loss: 1.8851, Test Loss: 3.454785919189453\n",
            "Epoch: 37, Train Loss: 1.8801, Test Loss: 3.639872694015503\n",
            "Epoch: 38, Train Loss: 1.9239, Test Loss: 3.3850342512130736\n",
            "Epoch: 39, Train Loss: 1.8878, Test Loss: 3.599170184135437\n",
            "Epoch: 40, Train Loss: 1.8429, Test Loss: 3.4815117835998537\n",
            "Epoch: 41, Train Loss: 1.7762, Test Loss: 3.4845007419586183\n",
            "Epoch: 42, Train Loss: 1.8302, Test Loss: 3.492595171928406\n",
            "Epoch: 43, Train Loss: 1.7816, Test Loss: 3.4817094564437867\n",
            "Epoch: 44, Train Loss: 1.8264, Test Loss: 3.460135579109192\n",
            "Epoch: 45, Train Loss: 1.7732, Test Loss: 3.5182655572891237\n",
            "Epoch: 46, Train Loss: 1.7023, Test Loss: 3.480899715423584\n",
            "Epoch: 47, Train Loss: 1.7615, Test Loss: 3.5205872535705565\n",
            "Epoch: 48, Train Loss: 1.7454, Test Loss: 3.5348634004592894\n",
            "Epoch: 49, Train Loss: 1.7052, Test Loss: 3.4621460914611815\n",
            "Epoch: 50, Train Loss: 1.7350, Test Loss: 3.465464234352112\n",
            "Epoch: 51, Train Loss: 1.6880, Test Loss: 3.4954298973083495\n",
            "Epoch: 52, Train Loss: 1.7096, Test Loss: 3.445469045639038\n",
            "Epoch: 53, Train Loss: 1.6443, Test Loss: 3.422284483909607\n",
            "Epoch: 54, Train Loss: 1.6515, Test Loss: 3.4829686164855955\n",
            "Epoch: 55, Train Loss: 1.6859, Test Loss: 3.4284573078155516\n",
            "Epoch: 56, Train Loss: 1.5896, Test Loss: 3.4575435638427736\n",
            "Epoch: 57, Train Loss: 1.6074, Test Loss: 3.500548839569092\n",
            "Epoch: 58, Train Loss: 1.6033, Test Loss: 3.3717071056365966\n",
            "Epoch: 59, Train Loss: 1.5655, Test Loss: 3.4233126878738402\n",
            "Epoch: 60, Train Loss: 1.5358, Test Loss: 3.4466214418411254\n",
            "Epoch: 61, Train Loss: 1.5472, Test Loss: 3.4558937788009643\n",
            "Epoch: 62, Train Loss: 1.5629, Test Loss: 3.5023125648498534\n",
            "Epoch: 63, Train Loss: 1.5401, Test Loss: 3.468247413635254\n",
            "Epoch: 64, Train Loss: 1.5559, Test Loss: 3.477836847305298\n",
            "Epoch: 65, Train Loss: 1.5300, Test Loss: 3.4515358209609985\n",
            "Epoch: 66, Train Loss: 1.5308, Test Loss: 3.446432685852051\n",
            "Epoch: 67, Train Loss: 1.4774, Test Loss: 3.5636504650115968\n",
            "Epoch: 68, Train Loss: 1.5148, Test Loss: 3.374034118652344\n",
            "Epoch: 69, Train Loss: 1.5206, Test Loss: 3.4752938508987428\n",
            "Epoch: 70, Train Loss: 1.5370, Test Loss: 3.4243541240692137\n",
            "Epoch: 71, Train Loss: 1.5020, Test Loss: 3.3686389446258547\n",
            "Epoch: 72, Train Loss: 1.4833, Test Loss: 3.415202808380127\n",
            "Epoch: 73, Train Loss: 1.4563, Test Loss: 3.3753546714782714\n",
            "Epoch: 74, Train Loss: 1.4432, Test Loss: 3.4350366592407227\n",
            "Epoch: 75, Train Loss: 1.4971, Test Loss: 3.42185115814209\n",
            "Epoch: 76, Train Loss: 1.4577, Test Loss: 3.3635674476623536\n",
            "Epoch: 77, Train Loss: 1.5514, Test Loss: 3.437136507034302\n",
            "Epoch: 78, Train Loss: 1.4582, Test Loss: 3.3996099710464476\n",
            "Epoch: 79, Train Loss: 1.3966, Test Loss: 3.449227976799011\n",
            "Epoch: 80, Train Loss: 1.4677, Test Loss: 3.406199407577515\n",
            "Epoch: 81, Train Loss: 1.3841, Test Loss: 3.4480639696121216\n",
            "Epoch: 82, Train Loss: 1.3929, Test Loss: 3.4000797271728516\n",
            "Epoch: 83, Train Loss: 1.4012, Test Loss: 3.445266914367676\n",
            "Epoch: 84, Train Loss: 1.3831, Test Loss: 3.338155460357666\n",
            "Epoch: 85, Train Loss: 1.4257, Test Loss: 3.406121349334717\n",
            "Epoch: 86, Train Loss: 1.4176, Test Loss: 3.371636962890625\n",
            "Epoch: 87, Train Loss: 1.3653, Test Loss: 3.4187577962875366\n",
            "Epoch: 88, Train Loss: 1.4240, Test Loss: 3.384170913696289\n",
            "Epoch: 89, Train Loss: 1.4820, Test Loss: 3.413664388656616\n",
            "Epoch: 90, Train Loss: 1.3364, Test Loss: 3.4044601202011107\n",
            "Epoch: 91, Train Loss: 1.3858, Test Loss: 3.4207929372787476\n",
            "Epoch: 92, Train Loss: 1.3288, Test Loss: 3.4386656284332275\n",
            "Epoch: 93, Train Loss: 1.3833, Test Loss: 3.3936540126800536\n",
            "Epoch: 94, Train Loss: 1.3702, Test Loss: 3.417285132408142\n",
            "Epoch: 95, Train Loss: 1.3198, Test Loss: 3.425175929069519\n",
            "Epoch: 96, Train Loss: 1.3540, Test Loss: 3.3709454774856566\n",
            "Epoch: 97, Train Loss: 1.4016, Test Loss: 3.4200987339019777\n",
            "Epoch: 98, Train Loss: 1.3683, Test Loss: 3.421304965019226\n",
            "Epoch: 99, Train Loss: 1.3381, Test Loss: 3.41494767665863\n",
            "Epoch: 100, Train Loss: 1.4361, Test Loss: 3.3442029476165773\n",
            "Epoch: 101, Train Loss: 1.3172, Test Loss: 3.3109151244163515\n",
            "Epoch: 102, Train Loss: 1.2994, Test Loss: 3.340044903755188\n",
            "Epoch: 103, Train Loss: 1.3092, Test Loss: 3.385002613067627\n",
            "Epoch: 104, Train Loss: 1.3949, Test Loss: 3.3661587476730346\n",
            "Epoch: 105, Train Loss: 1.3711, Test Loss: 3.3553244352340696\n",
            "Epoch: 106, Train Loss: 1.3843, Test Loss: 3.4663055181503295\n",
            "Epoch: 107, Train Loss: 1.2938, Test Loss: 3.3423640966415404\n",
            "Epoch: 108, Train Loss: 1.3528, Test Loss: 3.4017437219619753\n",
            "Epoch: 109, Train Loss: 1.2964, Test Loss: 3.3962725162506104\n",
            "Epoch: 110, Train Loss: 1.3617, Test Loss: 3.3948831081390383\n",
            "Epoch: 111, Train Loss: 1.3248, Test Loss: 3.3677059173583985\n",
            "Epoch: 112, Train Loss: 1.3119, Test Loss: 3.445972466468811\n",
            "Epoch: 113, Train Loss: 1.3128, Test Loss: 3.443635582923889\n",
            "Epoch: 114, Train Loss: 1.2922, Test Loss: 3.4254239559173585\n",
            "Epoch: 115, Train Loss: 1.2560, Test Loss: 3.410780191421509\n",
            "Epoch: 116, Train Loss: 1.2540, Test Loss: 3.392415404319763\n",
            "Epoch: 117, Train Loss: 1.2502, Test Loss: 3.44583055973053\n",
            "Epoch: 118, Train Loss: 1.3109, Test Loss: 3.397846984863281\n",
            "Epoch: 119, Train Loss: 1.2748, Test Loss: 3.3788421154022217\n",
            "Epoch: 120, Train Loss: 1.2503, Test Loss: 3.395128870010376\n",
            "Epoch: 121, Train Loss: 1.2220, Test Loss: 3.39616174697876\n",
            "Epoch: 122, Train Loss: 1.3358, Test Loss: 3.344470191001892\n",
            "Epoch: 123, Train Loss: 1.2649, Test Loss: 3.425373649597168\n",
            "Epoch: 124, Train Loss: 1.2912, Test Loss: 3.3607001066207887\n",
            "Epoch: 125, Train Loss: 1.2562, Test Loss: 3.3425012111663817\n",
            "Epoch: 126, Train Loss: 1.2292, Test Loss: 3.3543162107467652\n",
            "Epoch: 127, Train Loss: 1.2857, Test Loss: 3.3444540977478026\n",
            "Epoch: 128, Train Loss: 1.2257, Test Loss: 3.3709169387817384\n",
            "Epoch: 129, Train Loss: 1.1948, Test Loss: 3.3616838693618774\n",
            "Epoch: 130, Train Loss: 1.2503, Test Loss: 3.431555414199829\n",
            "Epoch: 131, Train Loss: 1.3256, Test Loss: 3.396380376815796\n",
            "Epoch: 132, Train Loss: 1.2630, Test Loss: 3.384647750854492\n",
            "Epoch: 133, Train Loss: 1.2071, Test Loss: 3.375967288017273\n",
            "Epoch: 134, Train Loss: 1.2164, Test Loss: 3.3977072715759276\n",
            "Epoch: 135, Train Loss: 1.2410, Test Loss: 3.348165488243103\n",
            "Epoch: 136, Train Loss: 1.2573, Test Loss: 3.339636945724487\n",
            "Epoch: 137, Train Loss: 1.2334, Test Loss: 3.3948384046554567\n",
            "Epoch: 138, Train Loss: 1.2837, Test Loss: 3.330537486076355\n",
            "Epoch: 139, Train Loss: 1.2370, Test Loss: 3.3284276723861694\n",
            "Epoch: 140, Train Loss: 1.1817, Test Loss: 3.3335798263549803\n",
            "Epoch: 141, Train Loss: 1.1865, Test Loss: 3.3376149415969847\n",
            "Epoch: 142, Train Loss: 1.2175, Test Loss: 3.3308724880218508\n",
            "Epoch: 143, Train Loss: 1.2255, Test Loss: 3.378600811958313\n",
            "Epoch: 144, Train Loss: 1.2206, Test Loss: 3.3833651781082152\n",
            "Epoch: 145, Train Loss: 1.2314, Test Loss: 3.3572171211242674\n",
            "Epoch: 146, Train Loss: 1.2423, Test Loss: 3.357946038246155\n",
            "Epoch: 147, Train Loss: 1.2346, Test Loss: 3.3349100589752196\n",
            "Epoch: 148, Train Loss: 1.1947, Test Loss: 3.3477222204208372\n",
            "Epoch: 149, Train Loss: 1.2513, Test Loss: 3.346803092956543\n",
            "Epoch: 150, Train Loss: 1.1719, Test Loss: 3.3480286598205566\n",
            "Epoch: 151, Train Loss: 1.1528, Test Loss: 3.4133862257003784\n",
            "Epoch: 152, Train Loss: 1.2039, Test Loss: 3.3674402952194216\n",
            "Epoch: 153, Train Loss: 1.2001, Test Loss: 3.3354464054107664\n",
            "Epoch: 154, Train Loss: 1.1776, Test Loss: 3.340765380859375\n",
            "Epoch: 155, Train Loss: 1.1905, Test Loss: 3.3322218894958495\n",
            "Epoch: 156, Train Loss: 1.1841, Test Loss: 3.3658962488174438\n",
            "Epoch: 157, Train Loss: 1.1369, Test Loss: 3.3417237043380736\n",
            "Epoch: 158, Train Loss: 1.1709, Test Loss: 3.373474287986755\n",
            "Epoch: 159, Train Loss: 1.1831, Test Loss: 3.3587936878204347\n",
            "Epoch: 160, Train Loss: 1.1799, Test Loss: 3.3314210414886474\n",
            "Epoch: 161, Train Loss: 1.1805, Test Loss: 3.307884120941162\n",
            "Epoch: 162, Train Loss: 1.1868, Test Loss: 3.3247742652893066\n",
            "Epoch: 163, Train Loss: 1.2008, Test Loss: 3.3355117797851563\n",
            "Epoch: 164, Train Loss: 1.2068, Test Loss: 3.3462347984313965\n",
            "Epoch: 165, Train Loss: 1.1529, Test Loss: 3.3161389589309693\n",
            "Epoch: 166, Train Loss: 1.1482, Test Loss: 3.3474595069885256\n",
            "Epoch: 167, Train Loss: 1.1540, Test Loss: 3.3374175071716308\n",
            "Epoch: 168, Train Loss: 1.1876, Test Loss: 3.3639846801757813\n",
            "Epoch: 169, Train Loss: 1.1678, Test Loss: 3.35520658493042\n",
            "Epoch: 170, Train Loss: 1.1568, Test Loss: 3.334979271888733\n",
            "Epoch: 171, Train Loss: 1.1609, Test Loss: 3.350896430015564\n",
            "Epoch: 172, Train Loss: 1.2126, Test Loss: 3.3249608516693114\n",
            "Epoch: 173, Train Loss: 1.1519, Test Loss: 3.3667909860610963\n",
            "Epoch: 174, Train Loss: 1.1339, Test Loss: 3.343150019645691\n",
            "Epoch: 175, Train Loss: 1.1239, Test Loss: 3.292816972732544\n",
            "Epoch: 176, Train Loss: 1.1587, Test Loss: 3.3494797706604005\n",
            "Epoch: 177, Train Loss: 1.2009, Test Loss: 3.3264158844947813\n",
            "Epoch: 178, Train Loss: 1.1420, Test Loss: 3.3496137619018556\n",
            "Epoch: 179, Train Loss: 1.1762, Test Loss: 3.319180798530579\n",
            "Epoch: 180, Train Loss: 1.1855, Test Loss: 3.3126190423965456\n",
            "Epoch: 181, Train Loss: 1.1448, Test Loss: 3.2931307554244995\n",
            "Epoch: 182, Train Loss: 1.1725, Test Loss: 3.304741644859314\n",
            "Epoch: 183, Train Loss: 1.0872, Test Loss: 3.380763578414917\n",
            "Epoch: 184, Train Loss: 1.1580, Test Loss: 3.3489046812057497\n",
            "Epoch: 185, Train Loss: 1.1959, Test Loss: 3.299974870681763\n",
            "Epoch: 186, Train Loss: 1.0966, Test Loss: 3.315031886100769\n",
            "Epoch: 187, Train Loss: 1.2087, Test Loss: 3.279180121421814\n",
            "Epoch: 188, Train Loss: 1.1827, Test Loss: 3.312782382965088\n",
            "Epoch: 189, Train Loss: 1.1620, Test Loss: 3.3290159463882447\n",
            "Epoch: 190, Train Loss: 1.0949, Test Loss: 3.301061141490936\n",
            "Epoch: 191, Train Loss: 1.1105, Test Loss: 3.3507432460784914\n",
            "Epoch: 192, Train Loss: 1.1565, Test Loss: 3.3230075597763062\n",
            "Epoch: 193, Train Loss: 1.1296, Test Loss: 3.3157552242279054\n",
            "Epoch: 194, Train Loss: 1.1754, Test Loss: 3.3086864948272705\n",
            "Epoch: 195, Train Loss: 1.0772, Test Loss: 3.3399783611297607\n",
            "Epoch: 196, Train Loss: 1.1229, Test Loss: 3.328530263900757\n",
            "Epoch: 197, Train Loss: 1.1128, Test Loss: 3.422794723510742\n",
            "Epoch: 198, Train Loss: 1.1639, Test Loss: 3.3133484363555907\n",
            "Epoch: 199, Train Loss: 1.1075, Test Loss: 3.3295753479003904\n",
            "Epoch: 200, Train Loss: 1.1175, Test Loss: 3.333008623123169\n",
            "Epoch: 201, Train Loss: 1.1550, Test Loss: 3.330142092704773\n",
            "Epoch: 202, Train Loss: 1.1912, Test Loss: 3.2673723340034484\n",
            "Epoch: 203, Train Loss: 1.0922, Test Loss: 3.3523937940597532\n",
            "Epoch: 204, Train Loss: 1.1176, Test Loss: 3.3111112117767334\n",
            "Epoch: 205, Train Loss: 1.1039, Test Loss: 3.322967219352722\n",
            "Epoch: 206, Train Loss: 1.0834, Test Loss: 3.3199272632598875\n",
            "Epoch: 207, Train Loss: 1.0716, Test Loss: 3.2978617906570435\n",
            "Epoch: 208, Train Loss: 1.0978, Test Loss: 3.304518222808838\n",
            "Epoch: 209, Train Loss: 1.1131, Test Loss: 3.310151243209839\n",
            "Epoch: 210, Train Loss: 1.1428, Test Loss: 3.3174965381622314\n",
            "Epoch: 211, Train Loss: 1.0680, Test Loss: 3.296396350860596\n",
            "Epoch: 212, Train Loss: 1.1119, Test Loss: 3.3027426481246946\n",
            "Epoch: 213, Train Loss: 1.1187, Test Loss: 3.2950713634490967\n",
            "Epoch: 214, Train Loss: 1.1045, Test Loss: 3.3163759231567385\n",
            "Epoch: 215, Train Loss: 1.1323, Test Loss: 3.3039464235305784\n",
            "Epoch: 216, Train Loss: 1.1064, Test Loss: 3.3031795978546143\n",
            "Epoch: 217, Train Loss: 1.0691, Test Loss: 3.346578335762024\n",
            "Epoch: 218, Train Loss: 1.0865, Test Loss: 3.3067198276519774\n",
            "Epoch: 219, Train Loss: 1.1099, Test Loss: 3.3345447778701782\n",
            "Epoch: 220, Train Loss: 1.1047, Test Loss: 3.342664885520935\n",
            "Epoch: 221, Train Loss: 1.0818, Test Loss: 3.322088432312012\n",
            "Epoch: 222, Train Loss: 1.1198, Test Loss: 3.4002455472946167\n",
            "Epoch: 223, Train Loss: 1.1154, Test Loss: 3.332693004608154\n",
            "Epoch: 224, Train Loss: 1.1445, Test Loss: 3.3645925998687742\n",
            "Epoch: 225, Train Loss: 1.1207, Test Loss: 3.3066619634628296\n",
            "Epoch: 226, Train Loss: 1.1015, Test Loss: 3.4069254398345947\n",
            "Epoch: 227, Train Loss: 1.1410, Test Loss: 3.312298393249512\n",
            "Epoch: 228, Train Loss: 1.0462, Test Loss: 3.321358156204224\n",
            "Epoch: 229, Train Loss: 1.0855, Test Loss: 3.3102520942687987\n",
            "Epoch: 230, Train Loss: 1.1718, Test Loss: 3.3816902160644533\n",
            "Epoch: 231, Train Loss: 1.1001, Test Loss: 3.3281270980834963\n",
            "Epoch: 232, Train Loss: 1.0415, Test Loss: 3.3388184547424316\n",
            "Epoch: 233, Train Loss: 1.0923, Test Loss: 3.3651882648468017\n",
            "Epoch: 234, Train Loss: 1.1227, Test Loss: 3.3576871156692505\n",
            "Epoch: 235, Train Loss: 1.1155, Test Loss: 3.318968152999878\n",
            "Epoch: 236, Train Loss: 1.1005, Test Loss: 3.343143606185913\n",
            "Epoch: 237, Train Loss: 1.0822, Test Loss: 3.3404953479766846\n",
            "Epoch: 238, Train Loss: 1.0541, Test Loss: 3.358410453796387\n",
            "Epoch: 239, Train Loss: 1.0913, Test Loss: 3.3384511709213256\n",
            "Epoch: 240, Train Loss: 1.0932, Test Loss: 3.3897718667984007\n",
            "Epoch: 241, Train Loss: 1.0666, Test Loss: 3.3255698680877686\n",
            "Epoch: 242, Train Loss: 1.0791, Test Loss: 3.2885571718215942\n",
            "Epoch: 243, Train Loss: 1.1205, Test Loss: 3.322922205924988\n",
            "Epoch: 244, Train Loss: 1.1234, Test Loss: 3.305480408668518\n",
            "Epoch: 245, Train Loss: 1.1691, Test Loss: 3.286661458015442\n",
            "Epoch: 246, Train Loss: 1.1060, Test Loss: 3.378320813179016\n",
            "Epoch: 247, Train Loss: 1.0961, Test Loss: 3.302136278152466\n",
            "Epoch: 248, Train Loss: 1.0352, Test Loss: 3.3066916942596434\n",
            "Epoch: 249, Train Loss: 1.0951, Test Loss: 3.274372673034668\n",
            "Epoch: 250, Train Loss: 1.1009, Test Loss: 3.341819334030151\n",
            "Epoch: 251, Train Loss: 1.0619, Test Loss: 3.305399179458618\n",
            "Epoch: 252, Train Loss: 1.0668, Test Loss: 3.3141729116439818\n",
            "Epoch: 253, Train Loss: 1.0865, Test Loss: 3.315094256401062\n",
            "Epoch: 254, Train Loss: 1.1354, Test Loss: 3.331837034225464\n",
            "Epoch: 255, Train Loss: 1.0709, Test Loss: 3.3525838613510133\n",
            "Epoch: 256, Train Loss: 1.0757, Test Loss: 3.3516923904418947\n",
            "Epoch: 257, Train Loss: 1.0791, Test Loss: 3.3626569509506226\n",
            "Epoch: 258, Train Loss: 1.1389, Test Loss: 3.3129599332809447\n",
            "Epoch: 259, Train Loss: 1.0726, Test Loss: 3.3338608503341676\n",
            "Epoch: 260, Train Loss: 1.0999, Test Loss: 3.354593515396118\n",
            "Epoch: 261, Train Loss: 1.0289, Test Loss: 3.332879877090454\n",
            "Epoch: 262, Train Loss: 1.1191, Test Loss: 3.326411557197571\n",
            "Epoch: 263, Train Loss: 1.0570, Test Loss: 3.272516298294067\n",
            "Epoch: 264, Train Loss: 1.0074, Test Loss: 3.309638738632202\n",
            "Epoch: 265, Train Loss: 1.1251, Test Loss: 3.3759428024291993\n",
            "Epoch: 266, Train Loss: 1.0303, Test Loss: 3.2924649238586428\n",
            "Epoch: 267, Train Loss: 1.0574, Test Loss: 3.3056796550750733\n",
            "Epoch: 268, Train Loss: 1.0560, Test Loss: 3.322218990325928\n",
            "Epoch: 269, Train Loss: 1.0927, Test Loss: 3.3341047286987306\n",
            "Epoch: 270, Train Loss: 1.1142, Test Loss: 3.324808156490326\n",
            "Epoch: 271, Train Loss: 1.0517, Test Loss: 3.3871379613876345\n",
            "Epoch: 272, Train Loss: 1.0846, Test Loss: 3.327725052833557\n",
            "Epoch: 273, Train Loss: 1.0822, Test Loss: 3.3562362432479858\n",
            "Epoch: 274, Train Loss: 1.0155, Test Loss: 3.323001575469971\n",
            "Epoch: 275, Train Loss: 1.0655, Test Loss: 3.371042585372925\n",
            "Epoch: 276, Train Loss: 1.0290, Test Loss: 3.33130259513855\n",
            "Epoch: 277, Train Loss: 1.0858, Test Loss: 3.3239547967910767\n",
            "Epoch: 278, Train Loss: 1.0487, Test Loss: 3.3110050439834593\n",
            "Epoch: 279, Train Loss: 1.0634, Test Loss: 3.3319762468338014\n",
            "Epoch: 280, Train Loss: 1.1368, Test Loss: 3.3004097938537598\n",
            "Epoch: 281, Train Loss: 1.0298, Test Loss: 3.316311240196228\n",
            "Epoch: 282, Train Loss: 1.0435, Test Loss: 3.311054730415344\n",
            "Epoch: 283, Train Loss: 1.0235, Test Loss: 3.3064591884613037\n",
            "Epoch: 284, Train Loss: 1.1093, Test Loss: 3.3501176834106445\n",
            "Epoch: 285, Train Loss: 1.0957, Test Loss: 3.36616051197052\n",
            "Epoch: 286, Train Loss: 1.0639, Test Loss: 3.3153329849243165\n",
            "Epoch: 287, Train Loss: 0.9931, Test Loss: 3.3034392833709716\n",
            "Epoch: 288, Train Loss: 1.0640, Test Loss: 3.3013281345367433\n",
            "Epoch: 289, Train Loss: 1.1263, Test Loss: 3.2868604183197023\n",
            "Epoch: 290, Train Loss: 1.1177, Test Loss: 3.2962167978286745\n",
            "Epoch: 291, Train Loss: 1.1259, Test Loss: 3.268678379058838\n",
            "Epoch: 292, Train Loss: 1.0196, Test Loss: 3.3165382623672484\n",
            "Epoch: 293, Train Loss: 1.0337, Test Loss: 3.3144389152526856\n",
            "Epoch: 294, Train Loss: 1.0158, Test Loss: 3.310174751281738\n",
            "Epoch: 295, Train Loss: 1.0638, Test Loss: 3.3187929153442384\n",
            "Epoch: 296, Train Loss: 1.0496, Test Loss: 3.302504777908325\n",
            "Epoch: 297, Train Loss: 1.0221, Test Loss: 3.2968777656555175\n",
            "Epoch: 298, Train Loss: 1.0710, Test Loss: 3.3347573280334473\n",
            "Epoch: 299, Train Loss: 1.0359, Test Loss: 3.3129327297210693\n",
            "Epoch: 300, Train Loss: 1.0269, Test Loss: 3.326546812057495\n",
            "Epoch: 301, Train Loss: 1.1112, Test Loss: 3.3885658979415894\n",
            "Epoch: 302, Train Loss: 1.0745, Test Loss: 3.3486226081848143\n",
            "Epoch: 303, Train Loss: 1.0698, Test Loss: 3.3530704498291017\n",
            "Epoch: 304, Train Loss: 1.0643, Test Loss: 3.3025917053222655\n",
            "Epoch: 305, Train Loss: 1.0212, Test Loss: 3.3488488674163817\n",
            "Epoch: 306, Train Loss: 1.1236, Test Loss: 3.3047959327697756\n",
            "Epoch: 307, Train Loss: 1.0171, Test Loss: 3.292671465873718\n",
            "Epoch: 308, Train Loss: 1.0118, Test Loss: 3.322221064567566\n",
            "Epoch: 309, Train Loss: 1.0973, Test Loss: 3.3237724781036375\n",
            "Epoch: 310, Train Loss: 1.0013, Test Loss: 3.3624077796936036\n",
            "Epoch: 311, Train Loss: 1.0055, Test Loss: 3.317443919181824\n",
            "Epoch: 312, Train Loss: 1.0090, Test Loss: 3.304210138320923\n",
            "Epoch: 313, Train Loss: 1.0171, Test Loss: 3.301462721824646\n",
            "Epoch: 314, Train Loss: 1.0787, Test Loss: 3.282439649105072\n",
            "Epoch: 315, Train Loss: 1.0405, Test Loss: 3.2859859228134156\n",
            "Epoch: 316, Train Loss: 1.0512, Test Loss: 3.3461812257766725\n",
            "Epoch: 317, Train Loss: 1.0399, Test Loss: 3.289529895782471\n",
            "Epoch: 318, Train Loss: 1.0154, Test Loss: 3.303491997718811\n",
            "Epoch: 319, Train Loss: 1.0020, Test Loss: 3.287453532218933\n",
            "Epoch: 320, Train Loss: 1.0601, Test Loss: 3.276539182662964\n",
            "Epoch: 321, Train Loss: 1.0026, Test Loss: 3.323990273475647\n",
            "Epoch: 322, Train Loss: 1.0398, Test Loss: 3.3158050537109376\n",
            "Epoch: 323, Train Loss: 1.1176, Test Loss: 3.2723623752593993\n",
            "Epoch: 324, Train Loss: 1.0195, Test Loss: 3.3031317472457884\n",
            "Epoch: 325, Train Loss: 1.0825, Test Loss: 3.279522180557251\n",
            "Epoch: 326, Train Loss: 1.0008, Test Loss: 3.2823979020118714\n",
            "Epoch: 327, Train Loss: 0.9916, Test Loss: 3.2942023992538454\n",
            "Epoch: 328, Train Loss: 1.0334, Test Loss: 3.2876747846603394\n",
            "Epoch: 329, Train Loss: 1.0920, Test Loss: 3.249077260494232\n",
            "Epoch: 330, Train Loss: 1.0474, Test Loss: 3.3454630613327025\n",
            "Epoch: 331, Train Loss: 0.9974, Test Loss: 3.2947182416915894\n",
            "Epoch: 332, Train Loss: 1.0282, Test Loss: 3.284822368621826\n",
            "Epoch: 333, Train Loss: 1.0039, Test Loss: 3.340972828865051\n",
            "Epoch: 334, Train Loss: 1.0219, Test Loss: 3.3061644792556764\n",
            "Epoch: 335, Train Loss: 0.9975, Test Loss: 3.2943461894989015\n",
            "Epoch: 336, Train Loss: 1.0041, Test Loss: 3.3319905281066893\n",
            "Epoch: 337, Train Loss: 1.0009, Test Loss: 3.340576434135437\n",
            "Epoch: 338, Train Loss: 1.0493, Test Loss: 3.3044005632400513\n",
            "Epoch: 339, Train Loss: 1.0871, Test Loss: 3.2814995646476746\n",
            "Epoch: 340, Train Loss: 0.9881, Test Loss: 3.2964078426361083\n",
            "Epoch: 341, Train Loss: 0.9563, Test Loss: 3.2864277839660643\n",
            "Epoch: 342, Train Loss: 0.9958, Test Loss: 3.314498281478882\n",
            "Epoch: 343, Train Loss: 1.0205, Test Loss: 3.323061490058899\n",
            "Epoch: 344, Train Loss: 1.0338, Test Loss: 3.2926565289497374\n",
            "Epoch: 345, Train Loss: 0.9527, Test Loss: 3.3197088718414305\n",
            "Epoch: 346, Train Loss: 1.0273, Test Loss: 3.3068015575408936\n",
            "Epoch: 347, Train Loss: 1.0130, Test Loss: 3.36006178855896\n",
            "Epoch: 348, Train Loss: 1.0355, Test Loss: 3.314849543571472\n",
            "Epoch: 349, Train Loss: 0.9816, Test Loss: 3.3171754121780395\n",
            "Epoch: 350, Train Loss: 1.0023, Test Loss: 3.2603505849838257\n",
            "Epoch: 351, Train Loss: 1.0364, Test Loss: 3.329031538963318\n",
            "Epoch: 352, Train Loss: 1.0289, Test Loss: 3.3131609678268434\n",
            "Epoch: 353, Train Loss: 1.0165, Test Loss: 3.2714118003845214\n",
            "Epoch: 354, Train Loss: 1.0142, Test Loss: 3.2673202395439147\n",
            "Epoch: 355, Train Loss: 0.9993, Test Loss: 3.3274932384490965\n",
            "Epoch: 356, Train Loss: 0.9876, Test Loss: 3.2947477221488954\n",
            "Epoch: 357, Train Loss: 0.9613, Test Loss: 3.322301959991455\n",
            "Epoch: 358, Train Loss: 0.9912, Test Loss: 3.2832809925079345\n",
            "Epoch: 359, Train Loss: 1.0200, Test Loss: 3.333448791503906\n",
            "Epoch: 360, Train Loss: 1.0245, Test Loss: 3.3036667108535767\n",
            "Epoch: 361, Train Loss: 1.0702, Test Loss: 3.3022531986236574\n",
            "Epoch: 362, Train Loss: 1.0168, Test Loss: 3.3088477849960327\n",
            "Epoch: 363, Train Loss: 1.0078, Test Loss: 3.315666151046753\n",
            "Epoch: 364, Train Loss: 1.0000, Test Loss: 3.3024217247962953\n",
            "Epoch: 365, Train Loss: 1.0161, Test Loss: 3.258882451057434\n",
            "Epoch: 366, Train Loss: 0.9418, Test Loss: 3.3163923740386965\n",
            "Epoch: 367, Train Loss: 1.0061, Test Loss: 3.2810479402542114\n",
            "Epoch: 368, Train Loss: 0.9544, Test Loss: 3.301594042778015\n",
            "Epoch: 369, Train Loss: 1.0717, Test Loss: 3.312493634223938\n",
            "Epoch: 370, Train Loss: 0.9533, Test Loss: 3.272150754928589\n",
            "Epoch: 371, Train Loss: 0.9832, Test Loss: 3.3102656602859497\n",
            "Epoch: 372, Train Loss: 0.9968, Test Loss: 3.3399826765060423\n",
            "Epoch: 373, Train Loss: 0.9905, Test Loss: 3.299294877052307\n",
            "Epoch: 374, Train Loss: 1.0485, Test Loss: 3.317040228843689\n",
            "Epoch: 375, Train Loss: 1.0508, Test Loss: 3.2534557819366454\n",
            "Epoch: 376, Train Loss: 1.0035, Test Loss: 3.3297709226608276\n",
            "Epoch: 377, Train Loss: 1.0208, Test Loss: 3.3004435539245605\n",
            "Epoch: 378, Train Loss: 1.0310, Test Loss: 3.2589051961898803\n",
            "Epoch: 379, Train Loss: 0.9550, Test Loss: 3.295127201080322\n",
            "Epoch: 380, Train Loss: 0.9800, Test Loss: 3.276070547103882\n",
            "Epoch: 381, Train Loss: 0.9989, Test Loss: 3.2825214862823486\n",
            "Epoch: 382, Train Loss: 1.0069, Test Loss: 3.2837021589279174\n",
            "Epoch: 383, Train Loss: 0.9782, Test Loss: 3.330619144439697\n",
            "Epoch: 384, Train Loss: 1.0485, Test Loss: 3.368596816062927\n",
            "Epoch: 385, Train Loss: 0.9704, Test Loss: 3.3184173345565795\n",
            "Epoch: 386, Train Loss: 0.9460, Test Loss: 3.2828673124313354\n",
            "Epoch: 387, Train Loss: 0.9801, Test Loss: 3.312613105773926\n",
            "Epoch: 388, Train Loss: 0.9941, Test Loss: 3.326076292991638\n",
            "Epoch: 389, Train Loss: 0.9814, Test Loss: 3.2970991134643555\n",
            "Epoch: 390, Train Loss: 1.0104, Test Loss: 3.311453914642334\n",
            "Epoch: 391, Train Loss: 1.0003, Test Loss: 3.315612554550171\n",
            "Epoch: 392, Train Loss: 0.9559, Test Loss: 3.330046510696411\n",
            "Epoch: 393, Train Loss: 1.0003, Test Loss: 3.3104289054870604\n",
            "Epoch: 394, Train Loss: 0.9869, Test Loss: 3.294061470031738\n",
            "Epoch: 395, Train Loss: 1.0255, Test Loss: 3.2664055824279785\n",
            "Epoch: 396, Train Loss: 0.9862, Test Loss: 3.3039744973182676\n",
            "Epoch: 397, Train Loss: 0.9253, Test Loss: 3.2598567724227907\n",
            "Epoch: 398, Train Loss: 1.0385, Test Loss: 3.2823125600814818\n",
            "Epoch: 399, Train Loss: 1.0061, Test Loss: 3.343556547164917\n",
            "Epoch: 400, Train Loss: 0.9342, Test Loss: 3.3200527667999267\n",
            "Epoch: 401, Train Loss: 0.9700, Test Loss: 3.29402334690094\n",
            "Epoch: 402, Train Loss: 0.9567, Test Loss: 3.325197863578796\n",
            "Epoch: 403, Train Loss: 0.9627, Test Loss: 3.3245242118835447\n",
            "Epoch: 404, Train Loss: 1.0053, Test Loss: 3.2845373153686523\n",
            "Epoch: 405, Train Loss: 0.9695, Test Loss: 3.2969367504119873\n",
            "Epoch: 406, Train Loss: 0.9209, Test Loss: 3.32847945690155\n",
            "Epoch: 407, Train Loss: 0.9829, Test Loss: 3.326486921310425\n",
            "Epoch: 408, Train Loss: 1.0301, Test Loss: 3.290047359466553\n",
            "Epoch: 409, Train Loss: 0.9645, Test Loss: 3.3109562158584596\n",
            "Epoch: 410, Train Loss: 0.9902, Test Loss: 3.313169264793396\n",
            "Epoch: 411, Train Loss: 1.0057, Test Loss: 3.313921666145325\n",
            "Epoch: 412, Train Loss: 0.9762, Test Loss: 3.29810836315155\n",
            "Epoch: 413, Train Loss: 0.9564, Test Loss: 3.2758283495903013\n",
            "Epoch: 414, Train Loss: 0.9213, Test Loss: 3.2931886672973634\n",
            "Epoch: 415, Train Loss: 0.9765, Test Loss: 3.2920910358428954\n",
            "Epoch: 416, Train Loss: 1.0648, Test Loss: 3.281689500808716\n",
            "Epoch: 417, Train Loss: 1.0329, Test Loss: 3.358846378326416\n",
            "Epoch: 418, Train Loss: 0.9803, Test Loss: 3.298420214653015\n",
            "Epoch: 419, Train Loss: 0.9158, Test Loss: 3.3015790700912477\n",
            "Epoch: 420, Train Loss: 1.0222, Test Loss: 3.286583364009857\n",
            "Epoch: 421, Train Loss: 0.9690, Test Loss: 3.269825839996338\n",
            "Epoch: 422, Train Loss: 1.0477, Test Loss: 3.2951924562454225\n",
            "Epoch: 423, Train Loss: 0.9957, Test Loss: 3.2862236738204955\n",
            "Epoch: 424, Train Loss: 0.9440, Test Loss: 3.2882148742675783\n",
            "Epoch: 425, Train Loss: 0.9374, Test Loss: 3.298196291923523\n",
            "Epoch: 426, Train Loss: 0.9089, Test Loss: 3.2864338874816896\n",
            "Epoch: 427, Train Loss: 0.9490, Test Loss: 3.2479189038276672\n",
            "Epoch: 428, Train Loss: 0.9654, Test Loss: 3.276116907596588\n",
            "Epoch: 429, Train Loss: 1.0612, Test Loss: 3.286002290248871\n",
            "Epoch: 430, Train Loss: 0.9573, Test Loss: 3.2664029121398928\n",
            "Epoch: 431, Train Loss: 1.0115, Test Loss: 3.2998716473579406\n",
            "Epoch: 432, Train Loss: 0.8978, Test Loss: 3.3121385097503664\n",
            "Epoch: 433, Train Loss: 1.0174, Test Loss: 3.317385697364807\n",
            "Epoch: 434, Train Loss: 1.0321, Test Loss: 3.2743385076522826\n",
            "Epoch: 435, Train Loss: 0.9823, Test Loss: 3.3021990060806274\n",
            "Epoch: 436, Train Loss: 0.9678, Test Loss: 3.2970027208328245\n",
            "Epoch: 437, Train Loss: 1.0196, Test Loss: 3.279087996482849\n",
            "Epoch: 438, Train Loss: 1.0291, Test Loss: 3.3170722246170046\n",
            "Epoch: 439, Train Loss: 1.0188, Test Loss: 3.2507099151611327\n",
            "Epoch: 440, Train Loss: 1.0075, Test Loss: 3.309118866920471\n",
            "Epoch: 441, Train Loss: 0.9796, Test Loss: 3.305214548110962\n",
            "Epoch: 442, Train Loss: 1.0092, Test Loss: 3.317374587059021\n",
            "Epoch: 443, Train Loss: 0.9725, Test Loss: 3.29391872882843\n",
            "Epoch: 444, Train Loss: 0.9531, Test Loss: 3.2827332258224486\n",
            "Epoch: 445, Train Loss: 1.0129, Test Loss: 3.2799046277999877\n",
            "Epoch: 446, Train Loss: 0.9631, Test Loss: 3.325634407997131\n",
            "Epoch: 447, Train Loss: 0.9858, Test Loss: 3.29947407245636\n",
            "Epoch: 448, Train Loss: 0.9447, Test Loss: 3.319336175918579\n",
            "Epoch: 449, Train Loss: 0.9476, Test Loss: 3.360502338409424\n",
            "Epoch: 450, Train Loss: 0.9858, Test Loss: 3.290459084510803\n",
            "Epoch: 451, Train Loss: 0.9247, Test Loss: 3.3173951148986816\n",
            "Epoch: 452, Train Loss: 0.9303, Test Loss: 3.2832500457763674\n",
            "Epoch: 453, Train Loss: 0.9805, Test Loss: 3.379651975631714\n",
            "Epoch: 454, Train Loss: 0.9814, Test Loss: 3.296248531341553\n",
            "Epoch: 455, Train Loss: 0.9214, Test Loss: 3.279468131065369\n",
            "Epoch: 456, Train Loss: 0.9685, Test Loss: 3.330034780502319\n",
            "Epoch: 457, Train Loss: 0.9663, Test Loss: 3.269611358642578\n",
            "Epoch: 458, Train Loss: 0.9224, Test Loss: 3.2906261682510376\n",
            "Epoch: 459, Train Loss: 0.9756, Test Loss: 3.262689733505249\n",
            "Epoch: 460, Train Loss: 0.9899, Test Loss: 3.283685255050659\n",
            "Epoch: 461, Train Loss: 1.0033, Test Loss: 3.3444433927536013\n",
            "Epoch: 462, Train Loss: 0.9882, Test Loss: 3.2855106592178345\n",
            "Epoch: 463, Train Loss: 1.0442, Test Loss: 3.2573994159698487\n",
            "Epoch: 464, Train Loss: 0.9208, Test Loss: 3.304507541656494\n",
            "Epoch: 465, Train Loss: 0.9733, Test Loss: 3.3336724281311034\n",
            "Epoch: 466, Train Loss: 0.9685, Test Loss: 3.3145144462585447\n",
            "Epoch: 467, Train Loss: 0.9828, Test Loss: 3.367949342727661\n",
            "Epoch: 468, Train Loss: 0.9668, Test Loss: 3.3209259271621705\n",
            "Epoch: 469, Train Loss: 0.9803, Test Loss: 3.273120164871216\n",
            "Epoch: 470, Train Loss: 0.9545, Test Loss: 3.2874794483184813\n",
            "Epoch: 471, Train Loss: 1.0232, Test Loss: 3.3146776437759398\n",
            "Epoch: 472, Train Loss: 0.9782, Test Loss: 3.2715842485427857\n",
            "Epoch: 473, Train Loss: 0.9765, Test Loss: 3.3256874561309813\n",
            "Epoch: 474, Train Loss: 0.9234, Test Loss: 3.331991267204285\n",
            "Epoch: 475, Train Loss: 1.0028, Test Loss: 3.3332140922546385\n",
            "Epoch: 476, Train Loss: 0.9214, Test Loss: 3.345313858985901\n",
            "Epoch: 477, Train Loss: 0.9814, Test Loss: 3.306282377243042\n",
            "Epoch: 478, Train Loss: 0.9093, Test Loss: 3.312159562110901\n",
            "Epoch: 479, Train Loss: 0.9737, Test Loss: 3.2996794462203978\n",
            "Epoch: 480, Train Loss: 0.9451, Test Loss: 3.2888203859329224\n",
            "Epoch: 481, Train Loss: 0.9487, Test Loss: 3.2990366220474243\n",
            "Epoch: 482, Train Loss: 0.9218, Test Loss: 3.317701721191406\n",
            "Epoch: 483, Train Loss: 0.9909, Test Loss: 3.3392563104629516\n",
            "Epoch: 484, Train Loss: 0.9435, Test Loss: 3.328610134124756\n",
            "Epoch: 485, Train Loss: 0.9590, Test Loss: 3.298156571388245\n",
            "Epoch: 486, Train Loss: 0.9550, Test Loss: 3.334539461135864\n",
            "Epoch: 487, Train Loss: 0.9352, Test Loss: 3.350274085998535\n",
            "Epoch: 488, Train Loss: 0.9838, Test Loss: 3.3003294706344604\n",
            "Epoch: 489, Train Loss: 0.9775, Test Loss: 3.340362215042114\n",
            "Epoch: 490, Train Loss: 0.9184, Test Loss: 3.299745726585388\n",
            "Epoch: 491, Train Loss: 0.9175, Test Loss: 3.3084110260009765\n",
            "Epoch: 492, Train Loss: 0.9873, Test Loss: 3.3183630228042604\n",
            "Epoch: 493, Train Loss: 0.8870, Test Loss: 3.298496723175049\n",
            "Epoch: 494, Train Loss: 0.9136, Test Loss: 3.3144797563552855\n",
            "Epoch: 495, Train Loss: 0.9473, Test Loss: 3.3075588941574097\n",
            "Epoch: 496, Train Loss: 0.9566, Test Loss: 3.3399608612060545\n",
            "Epoch: 497, Train Loss: 0.9271, Test Loss: 3.3010609626770018\n",
            "Epoch: 498, Train Loss: 0.9280, Test Loss: 3.3067480325698853\n",
            "Epoch: 499, Train Loss: 0.9723, Test Loss: 3.292395544052124\n",
            "Epoch: 500, Train Loss: 0.9497, Test Loss: 3.270511245727539\n",
            "Epoch: 501, Train Loss: 0.9315, Test Loss: 3.313432288169861\n",
            "Epoch: 502, Train Loss: 0.9603, Test Loss: 3.2957833051681518\n",
            "Epoch: 503, Train Loss: 0.9526, Test Loss: 3.318181777000427\n",
            "Epoch: 504, Train Loss: 0.9106, Test Loss: 3.28673996925354\n",
            "Epoch: 505, Train Loss: 0.9727, Test Loss: 3.2679930090904237\n",
            "Epoch: 506, Train Loss: 0.9539, Test Loss: 3.3102149248123167\n",
            "Epoch: 507, Train Loss: 0.9765, Test Loss: 3.28128342628479\n",
            "Epoch: 508, Train Loss: 0.9679, Test Loss: 3.3110342502593992\n",
            "Epoch: 509, Train Loss: 0.9308, Test Loss: 3.281958508491516\n",
            "Epoch: 510, Train Loss: 1.0324, Test Loss: 3.2814682483673097\n",
            "Epoch: 511, Train Loss: 0.9853, Test Loss: 3.3269203901290894\n",
            "Epoch: 512, Train Loss: 0.9665, Test Loss: 3.3024816274642945\n",
            "Epoch: 513, Train Loss: 1.0362, Test Loss: 3.273997354507446\n",
            "Epoch: 514, Train Loss: 0.9287, Test Loss: 3.2796466827392576\n",
            "Epoch: 515, Train Loss: 0.9914, Test Loss: 3.3168832302093505\n",
            "Epoch: 516, Train Loss: 0.9955, Test Loss: 3.3341758251190186\n",
            "Epoch: 517, Train Loss: 0.9060, Test Loss: 3.295903277397156\n",
            "Epoch: 518, Train Loss: 0.9739, Test Loss: 3.294856882095337\n",
            "Epoch: 519, Train Loss: 0.9413, Test Loss: 3.3066079378128053\n",
            "Epoch: 520, Train Loss: 0.9400, Test Loss: 3.3141947269439695\n",
            "Epoch: 521, Train Loss: 0.9845, Test Loss: 3.330497932434082\n",
            "Epoch: 522, Train Loss: 0.9782, Test Loss: 3.3022611856460573\n",
            "Epoch: 523, Train Loss: 1.0347, Test Loss: 3.272471880912781\n",
            "Epoch: 524, Train Loss: 0.9707, Test Loss: 3.2924995183944703\n",
            "Epoch: 525, Train Loss: 0.9428, Test Loss: 3.3420669078826903\n",
            "Epoch: 526, Train Loss: 0.9596, Test Loss: 3.30508177280426\n",
            "Epoch: 527, Train Loss: 1.0160, Test Loss: 3.261820936203003\n",
            "Epoch: 528, Train Loss: 0.9757, Test Loss: 3.288655090332031\n",
            "Epoch: 529, Train Loss: 0.9155, Test Loss: 3.318236517906189\n",
            "Epoch: 530, Train Loss: 0.9383, Test Loss: 3.2965285539627076\n",
            "Epoch: 531, Train Loss: 0.9179, Test Loss: 3.287011671066284\n",
            "Epoch: 532, Train Loss: 0.9190, Test Loss: 3.2838072299957277\n",
            "Epoch: 533, Train Loss: 0.9164, Test Loss: 3.285895347595215\n",
            "Epoch: 534, Train Loss: 0.9486, Test Loss: 3.2836527109146116\n",
            "Epoch: 535, Train Loss: 0.9113, Test Loss: 3.3396031141281126\n",
            "Epoch: 536, Train Loss: 0.9474, Test Loss: 3.2716941833496094\n",
            "Epoch: 537, Train Loss: 0.9866, Test Loss: 3.2765252113342287\n",
            "Epoch: 538, Train Loss: 0.9118, Test Loss: 3.3196625232696535\n",
            "Epoch: 539, Train Loss: 0.8830, Test Loss: 3.2703218936920164\n",
            "Epoch: 540, Train Loss: 1.0242, Test Loss: 3.305321979522705\n",
            "Epoch: 541, Train Loss: 1.0071, Test Loss: 3.2801867723464966\n",
            "Epoch: 542, Train Loss: 0.9271, Test Loss: 3.298639488220215\n",
            "Epoch: 543, Train Loss: 0.9189, Test Loss: 3.308221220970154\n",
            "Epoch: 544, Train Loss: 0.9027, Test Loss: 3.3272934913635255\n",
            "Epoch: 545, Train Loss: 0.9568, Test Loss: 3.3047832012176515\n",
            "Epoch: 546, Train Loss: 0.8839, Test Loss: 3.3085673332214354\n",
            "Epoch: 547, Train Loss: 0.9641, Test Loss: 3.272253227233887\n",
            "Epoch: 548, Train Loss: 0.8803, Test Loss: 3.3058649778366087\n",
            "Epoch: 549, Train Loss: 0.9379, Test Loss: 3.31613290309906\n",
            "Epoch: 550, Train Loss: 0.9812, Test Loss: 3.289151120185852\n",
            "Epoch: 551, Train Loss: 0.9473, Test Loss: 3.3021196603775023\n",
            "Epoch: 552, Train Loss: 0.8661, Test Loss: 3.296935534477234\n",
            "Epoch: 553, Train Loss: 0.9287, Test Loss: 3.3553884029388428\n",
            "Epoch: 554, Train Loss: 0.9423, Test Loss: 3.2904558181762695\n",
            "Epoch: 555, Train Loss: 0.9098, Test Loss: 3.2974080085754394\n",
            "Epoch: 556, Train Loss: 0.9358, Test Loss: 3.307921028137207\n",
            "Epoch: 557, Train Loss: 0.9905, Test Loss: 3.3057958364486693\n",
            "Epoch: 558, Train Loss: 1.0132, Test Loss: 3.2892045974731445\n",
            "Epoch: 559, Train Loss: 0.9458, Test Loss: 3.339807152748108\n",
            "Epoch: 560, Train Loss: 0.9102, Test Loss: 3.356141519546509\n",
            "Epoch: 561, Train Loss: 0.9386, Test Loss: 3.2949376583099363\n",
            "Epoch: 562, Train Loss: 0.9101, Test Loss: 3.2881216764450074\n",
            "Epoch: 563, Train Loss: 0.9654, Test Loss: 3.313411331176758\n",
            "Epoch: 564, Train Loss: 0.9022, Test Loss: 3.3016916036605837\n",
            "Epoch: 565, Train Loss: 0.8905, Test Loss: 3.2981582403182985\n",
            "Epoch: 566, Train Loss: 0.8835, Test Loss: 3.3181338787078856\n",
            "Epoch: 567, Train Loss: 0.9053, Test Loss: 3.3041048526763914\n",
            "Epoch: 568, Train Loss: 1.0126, Test Loss: 3.281750702857971\n",
            "Epoch: 569, Train Loss: 0.9547, Test Loss: 3.2820012092590334\n",
            "Epoch: 570, Train Loss: 0.9375, Test Loss: 3.3118058681488036\n",
            "Epoch: 571, Train Loss: 0.9333, Test Loss: 3.3235123634338377\n",
            "Epoch: 572, Train Loss: 0.9244, Test Loss: 3.296485948562622\n",
            "Epoch: 573, Train Loss: 0.9338, Test Loss: 3.299893355369568\n",
            "Epoch: 574, Train Loss: 0.9167, Test Loss: 3.297976279258728\n",
            "Epoch: 575, Train Loss: 0.9737, Test Loss: 3.3144315481185913\n",
            "Epoch: 576, Train Loss: 0.9074, Test Loss: 3.2892279624938965\n",
            "Epoch: 577, Train Loss: 0.9624, Test Loss: 3.2842479228973387\n",
            "Epoch: 578, Train Loss: 0.9266, Test Loss: 3.303224802017212\n",
            "Epoch: 579, Train Loss: 0.9442, Test Loss: 3.3374294519424437\n",
            "Epoch: 580, Train Loss: 0.9785, Test Loss: 3.3058841228485107\n",
            "Epoch: 581, Train Loss: 0.9397, Test Loss: 3.30287127494812\n",
            "Epoch: 582, Train Loss: 0.9166, Test Loss: 3.2832902908325194\n",
            "Epoch: 583, Train Loss: 0.8945, Test Loss: 3.2824854850769043\n",
            "Epoch: 584, Train Loss: 0.8937, Test Loss: 3.3010788202285766\n",
            "Epoch: 585, Train Loss: 0.8674, Test Loss: 3.3062705278396605\n",
            "Epoch: 586, Train Loss: 0.9728, Test Loss: 3.3121432065963745\n",
            "Epoch: 587, Train Loss: 0.9822, Test Loss: 3.3724261283874513\n",
            "Epoch: 588, Train Loss: 0.9146, Test Loss: 3.3087246656417846\n",
            "Epoch: 589, Train Loss: 0.9029, Test Loss: 3.288433003425598\n",
            "Epoch: 590, Train Loss: 0.9881, Test Loss: 3.2870223760604858\n",
            "Epoch: 591, Train Loss: 0.9365, Test Loss: 3.3216695547103883\n",
            "Epoch: 592, Train Loss: 1.0004, Test Loss: 3.2864678382873533\n",
            "Epoch: 593, Train Loss: 0.9584, Test Loss: 3.3069237947463987\n",
            "Epoch: 594, Train Loss: 0.9298, Test Loss: 3.3190030574798586\n",
            "Epoch: 595, Train Loss: 0.8802, Test Loss: 3.2885467052459716\n",
            "Epoch: 596, Train Loss: 0.9264, Test Loss: 3.2971431493759153\n",
            "Epoch: 597, Train Loss: 0.8938, Test Loss: 3.3495049476623535\n",
            "Epoch: 598, Train Loss: 0.8911, Test Loss: 3.273884963989258\n",
            "Epoch: 599, Train Loss: 0.9268, Test Loss: 3.2978644132614137\n",
            "Epoch: 600, Train Loss: 0.9136, Test Loss: 3.2673532485961916\n",
            "Epoch: 601, Train Loss: 0.8830, Test Loss: 3.2754597425460816\n",
            "Epoch: 602, Train Loss: 0.9643, Test Loss: 3.2640388011932373\n",
            "Epoch: 603, Train Loss: 0.9008, Test Loss: 3.3096096754074096\n",
            "Epoch: 604, Train Loss: 0.9534, Test Loss: 3.3330130577087402\n",
            "Epoch: 605, Train Loss: 0.9273, Test Loss: 3.2977967500686645\n",
            "Epoch: 606, Train Loss: 0.9366, Test Loss: 3.292763900756836\n",
            "Epoch: 607, Train Loss: 0.9007, Test Loss: 3.2947981357574463\n",
            "Epoch: 608, Train Loss: 0.9047, Test Loss: 3.2858124017715453\n",
            "Epoch: 609, Train Loss: 0.8449, Test Loss: 3.3179333209991455\n",
            "Epoch: 610, Train Loss: 0.9176, Test Loss: 3.3078786373138427\n",
            "Epoch: 611, Train Loss: 0.9024, Test Loss: 3.287330985069275\n",
            "Epoch: 612, Train Loss: 0.9088, Test Loss: 3.310731792449951\n",
            "Epoch: 613, Train Loss: 0.9156, Test Loss: 3.2871271133422852\n",
            "Epoch: 614, Train Loss: 0.9039, Test Loss: 3.3193989753723145\n",
            "Epoch: 615, Train Loss: 0.8807, Test Loss: 3.3163758754730224\n",
            "Epoch: 616, Train Loss: 0.9532, Test Loss: 3.3095376253128053\n",
            "Epoch: 617, Train Loss: 0.9706, Test Loss: 3.2786545038223265\n",
            "Epoch: 618, Train Loss: 0.8635, Test Loss: 3.3215551376342773\n",
            "Epoch: 619, Train Loss: 0.9348, Test Loss: 3.3197959899902343\n",
            "Epoch: 620, Train Loss: 0.9522, Test Loss: 3.339746904373169\n",
            "Epoch: 621, Train Loss: 0.9402, Test Loss: 3.3400240898132325\n",
            "Epoch: 622, Train Loss: 0.9371, Test Loss: 3.2668312788009644\n",
            "Epoch: 623, Train Loss: 0.8839, Test Loss: 3.27886848449707\n",
            "Epoch: 624, Train Loss: 0.9437, Test Loss: 3.321981906890869\n",
            "Epoch: 625, Train Loss: 0.9257, Test Loss: 3.2867305517196654\n",
            "Epoch: 626, Train Loss: 0.8831, Test Loss: 3.2862640619277954\n",
            "Epoch: 627, Train Loss: 0.9284, Test Loss: 3.299640488624573\n",
            "Epoch: 628, Train Loss: 0.8920, Test Loss: 3.3031375408172607\n",
            "Epoch: 629, Train Loss: 0.8771, Test Loss: 3.303523635864258\n",
            "Epoch: 630, Train Loss: 0.9798, Test Loss: 3.2808291912078857\n",
            "Epoch: 631, Train Loss: 0.9370, Test Loss: 3.305070996284485\n",
            "Epoch: 632, Train Loss: 0.9191, Test Loss: 3.3199553966522215\n",
            "Epoch: 633, Train Loss: 0.9383, Test Loss: 3.357280707359314\n",
            "Epoch: 634, Train Loss: 0.9041, Test Loss: 3.3183667182922365\n",
            "Epoch: 635, Train Loss: 0.9846, Test Loss: 3.393595480918884\n",
            "Epoch: 636, Train Loss: 0.9475, Test Loss: 3.3152162790298463\n",
            "Epoch: 637, Train Loss: 0.9096, Test Loss: 3.314830684661865\n",
            "Epoch: 638, Train Loss: 0.9408, Test Loss: 3.3129143714904785\n",
            "Epoch: 639, Train Loss: 0.9504, Test Loss: 3.334137749671936\n",
            "Epoch: 640, Train Loss: 0.8699, Test Loss: 3.3018887758255007\n",
            "Epoch: 641, Train Loss: 0.8872, Test Loss: 3.3075790405273438\n",
            "Epoch: 642, Train Loss: 1.0064, Test Loss: 3.2858062744140626\n",
            "Epoch: 643, Train Loss: 0.9639, Test Loss: 3.3292768239974975\n",
            "Epoch: 644, Train Loss: 0.9275, Test Loss: 3.3242716789245605\n",
            "Epoch: 645, Train Loss: 0.9276, Test Loss: 3.3381334066390993\n",
            "Epoch: 646, Train Loss: 0.8946, Test Loss: 3.2995455980300905\n",
            "Epoch: 647, Train Loss: 0.8757, Test Loss: 3.3226429224014282\n",
            "Epoch: 648, Train Loss: 0.8981, Test Loss: 3.3108567714691164\n",
            "Epoch: 649, Train Loss: 0.9003, Test Loss: 3.323861503601074\n",
            "Epoch: 650, Train Loss: 0.9471, Test Loss: 3.365034747123718\n",
            "Epoch: 651, Train Loss: 0.9285, Test Loss: 3.335446095466614\n",
            "Epoch: 652, Train Loss: 0.9147, Test Loss: 3.319291043281555\n",
            "Epoch: 653, Train Loss: 0.8668, Test Loss: 3.298730993270874\n",
            "Epoch: 654, Train Loss: 0.9408, Test Loss: 3.276696705818176\n",
            "Epoch: 655, Train Loss: 0.8560, Test Loss: 3.323493981361389\n",
            "Epoch: 656, Train Loss: 0.9002, Test Loss: 3.30780885219574\n",
            "Epoch: 657, Train Loss: 0.9300, Test Loss: 3.30604829788208\n",
            "Epoch: 658, Train Loss: 0.9042, Test Loss: 3.3051140308380127\n",
            "Epoch: 659, Train Loss: 0.9379, Test Loss: 3.303846311569214\n",
            "Epoch: 660, Train Loss: 0.9041, Test Loss: 3.286514329910278\n",
            "Epoch: 661, Train Loss: 0.9705, Test Loss: 3.370127272605896\n",
            "Epoch: 662, Train Loss: 0.9105, Test Loss: 3.31168053150177\n",
            "Epoch: 663, Train Loss: 0.9187, Test Loss: 3.2995599269866944\n",
            "Epoch: 664, Train Loss: 0.8670, Test Loss: 3.3001063108444213\n",
            "Epoch: 665, Train Loss: 0.8649, Test Loss: 3.3121358156204224\n",
            "Epoch: 666, Train Loss: 0.9373, Test Loss: 3.361406087875366\n",
            "Epoch: 667, Train Loss: 0.9042, Test Loss: 3.31624059677124\n",
            "Epoch: 668, Train Loss: 0.8739, Test Loss: 3.3061782360076903\n",
            "Epoch: 669, Train Loss: 0.9789, Test Loss: 3.3177280187606812\n",
            "Epoch: 670, Train Loss: 0.8923, Test Loss: 3.3427456855773925\n",
            "Epoch: 671, Train Loss: 0.8698, Test Loss: 3.32239830493927\n",
            "Epoch: 672, Train Loss: 0.8665, Test Loss: 3.315830111503601\n",
            "Epoch: 673, Train Loss: 0.9282, Test Loss: 3.3143683433532716\n",
            "Epoch: 674, Train Loss: 0.8904, Test Loss: 3.3001004219055177\n",
            "Epoch: 675, Train Loss: 0.9032, Test Loss: 3.3041277885437013\n",
            "Epoch: 676, Train Loss: 0.8860, Test Loss: 3.3181286096572875\n",
            "Epoch: 677, Train Loss: 0.8546, Test Loss: 3.316976523399353\n",
            "Epoch: 678, Train Loss: 0.9562, Test Loss: 3.2944189071655274\n",
            "Epoch: 679, Train Loss: 0.9309, Test Loss: 3.3223272800445556\n",
            "Epoch: 680, Train Loss: 0.9017, Test Loss: 3.318553900718689\n",
            "Epoch: 681, Train Loss: 0.8876, Test Loss: 3.3025654792785644\n",
            "Epoch: 682, Train Loss: 0.9378, Test Loss: 3.2911134243011473\n",
            "Epoch: 683, Train Loss: 0.9153, Test Loss: 3.339524579048157\n",
            "Epoch: 684, Train Loss: 0.9136, Test Loss: 3.2756043910980224\n",
            "Epoch: 685, Train Loss: 0.9512, Test Loss: 3.308052897453308\n",
            "Epoch: 686, Train Loss: 0.8877, Test Loss: 3.285496377944946\n",
            "Epoch: 687, Train Loss: 0.9273, Test Loss: 3.3224184274673463\n",
            "Epoch: 688, Train Loss: 0.8692, Test Loss: 3.332813024520874\n",
            "Epoch: 689, Train Loss: 0.8946, Test Loss: 3.3506720542907713\n",
            "Epoch: 690, Train Loss: 0.9060, Test Loss: 3.318908143043518\n",
            "Epoch: 691, Train Loss: 0.9023, Test Loss: 3.3081556081771852\n",
            "Epoch: 692, Train Loss: 0.8703, Test Loss: 3.2829940795898436\n",
            "Epoch: 693, Train Loss: 0.9665, Test Loss: 3.3303369522094726\n",
            "Epoch: 694, Train Loss: 0.9019, Test Loss: 3.310224485397339\n",
            "Epoch: 695, Train Loss: 0.9436, Test Loss: 3.293533515930176\n",
            "Epoch: 696, Train Loss: 0.8777, Test Loss: 3.338064193725586\n",
            "Epoch: 697, Train Loss: 0.9010, Test Loss: 3.284038519859314\n",
            "Epoch: 698, Train Loss: 0.8983, Test Loss: 3.2832601070404053\n",
            "Epoch: 699, Train Loss: 0.8761, Test Loss: 3.2831454038619996\n",
            "Epoch: 700, Train Loss: 1.0058, Test Loss: 3.320255708694458\n",
            "Epoch: 701, Train Loss: 0.8808, Test Loss: 3.3224408626556396\n",
            "Epoch: 702, Train Loss: 0.8628, Test Loss: 3.27329363822937\n",
            "Epoch: 703, Train Loss: 0.9009, Test Loss: 3.296424126625061\n",
            "Epoch: 704, Train Loss: 0.8860, Test Loss: 3.2857460498809816\n",
            "Epoch: 705, Train Loss: 0.9514, Test Loss: 3.320500707626343\n",
            "Epoch: 706, Train Loss: 0.9924, Test Loss: 3.3838000774383543\n",
            "Epoch: 707, Train Loss: 0.8824, Test Loss: 3.2736645221710203\n",
            "Epoch: 708, Train Loss: 0.8486, Test Loss: 3.341318225860596\n",
            "Epoch: 709, Train Loss: 0.9026, Test Loss: 3.301490592956543\n",
            "Epoch: 710, Train Loss: 0.9198, Test Loss: 3.2878196477890014\n",
            "Epoch: 711, Train Loss: 0.9489, Test Loss: 3.2985283851623537\n",
            "Epoch: 712, Train Loss: 0.9207, Test Loss: 3.3055323123931886\n",
            "Epoch: 713, Train Loss: 0.8912, Test Loss: 3.3290418863296507\n",
            "Epoch: 714, Train Loss: 0.9274, Test Loss: 3.2904284238815307\n",
            "Epoch: 715, Train Loss: 0.9062, Test Loss: 3.2917145013809206\n",
            "Epoch: 716, Train Loss: 0.9218, Test Loss: 3.3495304346084596\n",
            "Epoch: 717, Train Loss: 0.8559, Test Loss: 3.3288788557052613\n",
            "Epoch: 718, Train Loss: 0.9629, Test Loss: 3.3521909952163695\n",
            "Epoch: 719, Train Loss: 1.0002, Test Loss: 3.2980413675308227\n",
            "Epoch: 720, Train Loss: 0.8724, Test Loss: 3.307937836647034\n",
            "Epoch: 721, Train Loss: 0.9151, Test Loss: 3.370459723472595\n",
            "Epoch: 722, Train Loss: 0.8704, Test Loss: 3.3044591188430785\n",
            "Epoch: 723, Train Loss: 0.9228, Test Loss: 3.3180360317230226\n",
            "Epoch: 724, Train Loss: 0.9193, Test Loss: 3.3010720729827883\n",
            "Epoch: 725, Train Loss: 0.8839, Test Loss: 3.2762101888656616\n",
            "Epoch: 726, Train Loss: 0.9430, Test Loss: 3.2819045305252077\n",
            "Epoch: 727, Train Loss: 0.9057, Test Loss: 3.2932475328445436\n",
            "Epoch: 728, Train Loss: 0.9424, Test Loss: 3.2861131191253663\n",
            "Epoch: 729, Train Loss: 0.8921, Test Loss: 3.3225327491760255\n",
            "Epoch: 730, Train Loss: 0.8433, Test Loss: 3.3080963611602785\n",
            "Epoch: 731, Train Loss: 0.8764, Test Loss: 3.2887000322341917\n",
            "Epoch: 732, Train Loss: 0.9209, Test Loss: 3.3237399816513062\n",
            "Epoch: 733, Train Loss: 0.8457, Test Loss: 3.3028352737426756\n",
            "Epoch: 734, Train Loss: 0.8833, Test Loss: 3.285147547721863\n",
            "Epoch: 735, Train Loss: 0.8782, Test Loss: 3.3243539571762084\n",
            "Epoch: 736, Train Loss: 0.9080, Test Loss: 3.295884609222412\n",
            "Epoch: 737, Train Loss: 0.9909, Test Loss: 3.2795443773269652\n",
            "Epoch: 738, Train Loss: 0.8448, Test Loss: 3.2939468383789063\n",
            "Epoch: 739, Train Loss: 0.8665, Test Loss: 3.295421671867371\n",
            "Epoch: 740, Train Loss: 0.9313, Test Loss: 3.2804429054260256\n",
            "Epoch: 741, Train Loss: 0.8781, Test Loss: 3.296968102455139\n",
            "Epoch: 742, Train Loss: 0.9063, Test Loss: 3.3086849212646485\n",
            "Epoch: 743, Train Loss: 0.8758, Test Loss: 3.314506459236145\n",
            "Epoch: 744, Train Loss: 0.8992, Test Loss: 3.298711156845093\n",
            "Epoch: 745, Train Loss: 0.9002, Test Loss: 3.310895562171936\n",
            "Epoch: 746, Train Loss: 0.9323, Test Loss: 3.31575345993042\n",
            "Epoch: 747, Train Loss: 0.9398, Test Loss: 3.323909378051758\n",
            "Epoch: 748, Train Loss: 0.9341, Test Loss: 3.301127624511719\n",
            "Epoch: 749, Train Loss: 0.9050, Test Loss: 3.326152777671814\n",
            "Epoch: 750, Train Loss: 0.9148, Test Loss: 3.295986771583557\n",
            "Epoch: 751, Train Loss: 0.9188, Test Loss: 3.2804832458496094\n",
            "Epoch: 752, Train Loss: 0.9570, Test Loss: 3.3250473499298097\n",
            "Epoch: 753, Train Loss: 0.9191, Test Loss: 3.278758502006531\n",
            "Epoch: 754, Train Loss: 0.9278, Test Loss: 3.2949581146240234\n",
            "Epoch: 755, Train Loss: 0.8240, Test Loss: 3.3125191211700438\n",
            "Epoch: 756, Train Loss: 0.8847, Test Loss: 3.330033302307129\n",
            "Epoch: 757, Train Loss: 0.8672, Test Loss: 3.322345328330994\n",
            "Epoch: 758, Train Loss: 0.9118, Test Loss: 3.3232452392578127\n",
            "Epoch: 759, Train Loss: 0.8934, Test Loss: 3.3156240463256834\n",
            "Epoch: 760, Train Loss: 0.8629, Test Loss: 3.3314239978790283\n",
            "Epoch: 761, Train Loss: 0.8865, Test Loss: 3.3587284326553344\n",
            "Epoch: 762, Train Loss: 0.9698, Test Loss: 3.3056575059890747\n",
            "Epoch: 763, Train Loss: 0.8808, Test Loss: 3.3126073122024535\n",
            "Epoch: 764, Train Loss: 0.9327, Test Loss: 3.2809027433395386\n",
            "Epoch: 765, Train Loss: 0.8891, Test Loss: 3.2983896732330322\n",
            "Epoch: 766, Train Loss: 0.8729, Test Loss: 3.320236420631409\n",
            "Epoch: 767, Train Loss: 0.8420, Test Loss: 3.2964237689971925\n",
            "Epoch: 768, Train Loss: 0.9948, Test Loss: 3.344006299972534\n",
            "Epoch: 769, Train Loss: 0.9486, Test Loss: 3.3245828628540037\n",
            "Epoch: 770, Train Loss: 0.8909, Test Loss: 3.2858933925628664\n",
            "Epoch: 771, Train Loss: 0.8770, Test Loss: 3.320268440246582\n",
            "Epoch: 772, Train Loss: 0.8776, Test Loss: 3.3177924871444704\n",
            "Epoch: 773, Train Loss: 0.8961, Test Loss: 3.281192922592163\n",
            "Epoch: 774, Train Loss: 0.9808, Test Loss: 3.2879214763641356\n",
            "Epoch: 775, Train Loss: 0.8544, Test Loss: 3.316089391708374\n",
            "Epoch: 776, Train Loss: 0.8880, Test Loss: 3.2903259992599487\n",
            "Epoch: 777, Train Loss: 0.9076, Test Loss: 3.2884974241256715\n",
            "Epoch: 778, Train Loss: 0.8848, Test Loss: 3.2867409944534303\n",
            "Epoch: 779, Train Loss: 0.9054, Test Loss: 3.2674952268600466\n",
            "Epoch: 780, Train Loss: 0.8786, Test Loss: 3.305430269241333\n",
            "Epoch: 781, Train Loss: 0.8738, Test Loss: 3.3049388885498048\n",
            "Epoch: 782, Train Loss: 0.9533, Test Loss: 3.272730588912964\n",
            "Epoch: 783, Train Loss: 0.8648, Test Loss: 3.284760308265686\n",
            "Epoch: 784, Train Loss: 0.8815, Test Loss: 3.2945519924163817\n",
            "Epoch: 785, Train Loss: 0.8991, Test Loss: 3.3374059200286865\n",
            "Epoch: 786, Train Loss: 0.8885, Test Loss: 3.2970155239105225\n",
            "Epoch: 787, Train Loss: 0.8645, Test Loss: 3.3305142879486085\n",
            "Epoch: 788, Train Loss: 0.9126, Test Loss: 3.3962687730789183\n",
            "Epoch: 789, Train Loss: 0.9075, Test Loss: 3.2825965881347656\n",
            "Epoch: 790, Train Loss: 0.8653, Test Loss: 3.3118292570114134\n",
            "Epoch: 791, Train Loss: 0.8805, Test Loss: 3.285963034629822\n",
            "Epoch: 792, Train Loss: 0.8997, Test Loss: 3.258956551551819\n",
            "Epoch: 793, Train Loss: 0.8892, Test Loss: 3.2912904977798463\n",
            "Epoch: 794, Train Loss: 0.8573, Test Loss: 3.3274376153945924\n",
            "Epoch: 795, Train Loss: 0.8692, Test Loss: 3.3365366458892822\n",
            "Epoch: 796, Train Loss: 0.8803, Test Loss: 3.315107989311218\n",
            "Epoch: 797, Train Loss: 0.8974, Test Loss: 3.2882253646850588\n",
            "Epoch: 798, Train Loss: 0.8653, Test Loss: 3.28252170085907\n",
            "Epoch: 799, Train Loss: 0.8596, Test Loss: 3.319294238090515\n",
            "Epoch: 800, Train Loss: 0.8787, Test Loss: 3.2983872413635256\n",
            "Epoch: 801, Train Loss: 0.9245, Test Loss: 3.2883405447006226\n",
            "Epoch: 802, Train Loss: 0.9059, Test Loss: 3.287370777130127\n",
            "Epoch: 803, Train Loss: 0.8288, Test Loss: 3.2872899770736694\n",
            "Epoch: 804, Train Loss: 0.8606, Test Loss: 3.304978775978088\n",
            "Epoch: 805, Train Loss: 0.8516, Test Loss: 3.320121693611145\n",
            "Epoch: 806, Train Loss: 0.8753, Test Loss: 3.306593894958496\n",
            "Epoch: 807, Train Loss: 0.9033, Test Loss: 3.300007629394531\n",
            "Epoch: 808, Train Loss: 0.9021, Test Loss: 3.3013960838317873\n",
            "Epoch: 809, Train Loss: 0.8893, Test Loss: 3.308699679374695\n",
            "Epoch: 810, Train Loss: 0.9243, Test Loss: 3.285255217552185\n",
            "Epoch: 811, Train Loss: 0.9308, Test Loss: 3.286914753913879\n",
            "Epoch: 812, Train Loss: 0.8862, Test Loss: 3.3226613283157347\n",
            "Epoch: 813, Train Loss: 0.8382, Test Loss: 3.321177935600281\n",
            "Epoch: 814, Train Loss: 0.8814, Test Loss: 3.3466222286224365\n",
            "Epoch: 815, Train Loss: 0.8758, Test Loss: 3.310410714149475\n",
            "Epoch: 816, Train Loss: 0.8669, Test Loss: 3.3131263971328737\n",
            "Epoch: 817, Train Loss: 0.9176, Test Loss: 3.311907196044922\n",
            "Epoch: 818, Train Loss: 0.8814, Test Loss: 3.290441131591797\n",
            "Epoch: 819, Train Loss: 0.8509, Test Loss: 3.319038963317871\n",
            "Epoch: 820, Train Loss: 0.9439, Test Loss: 3.2950867414474487\n",
            "Epoch: 821, Train Loss: 0.9357, Test Loss: 3.2784897327423095\n",
            "Epoch: 822, Train Loss: 0.9079, Test Loss: 3.2866474390029907\n",
            "Epoch: 823, Train Loss: 0.8413, Test Loss: 3.297861933708191\n",
            "Epoch: 824, Train Loss: 1.0004, Test Loss: 3.2992026805877686\n",
            "Epoch: 825, Train Loss: 0.8969, Test Loss: 3.2905416011810305\n",
            "Epoch: 826, Train Loss: 0.8359, Test Loss: 3.277202820777893\n",
            "Epoch: 827, Train Loss: 0.8617, Test Loss: 3.284184432029724\n",
            "Epoch: 828, Train Loss: 0.8759, Test Loss: 3.2714784383773803\n",
            "Epoch: 829, Train Loss: 0.9415, Test Loss: 3.3165510416030886\n",
            "Epoch: 830, Train Loss: 0.9002, Test Loss: 3.283239412307739\n",
            "Epoch: 831, Train Loss: 0.9489, Test Loss: 3.283710765838623\n",
            "Epoch: 832, Train Loss: 0.9434, Test Loss: 3.352152609825134\n",
            "Epoch: 833, Train Loss: 0.8878, Test Loss: 3.30190486907959\n",
            "Epoch: 834, Train Loss: 0.9229, Test Loss: 3.287825512886047\n",
            "Epoch: 835, Train Loss: 0.8286, Test Loss: 3.303255319595337\n",
            "Epoch: 836, Train Loss: 0.8654, Test Loss: 3.3229881525039673\n",
            "Epoch: 837, Train Loss: 0.8642, Test Loss: 3.3137945175170898\n",
            "Epoch: 838, Train Loss: 0.8977, Test Loss: 3.3198673009872435\n",
            "Epoch: 839, Train Loss: 0.8315, Test Loss: 3.300840377807617\n",
            "Epoch: 840, Train Loss: 0.8828, Test Loss: 3.308137536048889\n",
            "Epoch: 841, Train Loss: 0.8121, Test Loss: 3.295629858970642\n",
            "Epoch: 842, Train Loss: 0.8744, Test Loss: 3.290697717666626\n",
            "Epoch: 843, Train Loss: 0.9365, Test Loss: 3.269772243499756\n",
            "Epoch: 844, Train Loss: 0.8829, Test Loss: 3.3014527559280396\n",
            "Epoch: 845, Train Loss: 0.8752, Test Loss: 3.307694149017334\n",
            "Epoch: 846, Train Loss: 0.8678, Test Loss: 3.3137304544448853\n",
            "Epoch: 847, Train Loss: 0.9236, Test Loss: 3.351884293556213\n",
            "Epoch: 848, Train Loss: 0.8639, Test Loss: 3.320653772354126\n",
            "Epoch: 849, Train Loss: 0.8918, Test Loss: 3.276972508430481\n",
            "Epoch: 850, Train Loss: 0.8900, Test Loss: 3.293705701828003\n",
            "Epoch: 851, Train Loss: 0.8655, Test Loss: 3.3261144161224365\n",
            "Epoch: 852, Train Loss: 0.9322, Test Loss: 3.284016180038452\n",
            "Epoch: 853, Train Loss: 0.8675, Test Loss: 3.328143095970154\n",
            "Epoch: 854, Train Loss: 0.8756, Test Loss: 3.292181873321533\n",
            "Epoch: 855, Train Loss: 0.8308, Test Loss: 3.3245253801345824\n",
            "Epoch: 856, Train Loss: 0.8413, Test Loss: 3.3157721042633055\n",
            "Epoch: 857, Train Loss: 0.9089, Test Loss: 3.3032990455627442\n",
            "Epoch: 858, Train Loss: 0.8696, Test Loss: 3.2888190507888795\n",
            "Epoch: 859, Train Loss: 0.8445, Test Loss: 3.291681170463562\n",
            "Epoch: 860, Train Loss: 0.8824, Test Loss: 3.3390321254730226\n",
            "Epoch: 861, Train Loss: 0.8938, Test Loss: 3.335468125343323\n",
            "Epoch: 862, Train Loss: 0.8961, Test Loss: 3.3441325664520263\n",
            "Epoch: 863, Train Loss: 0.8561, Test Loss: 3.3216549873352053\n",
            "Epoch: 864, Train Loss: 0.8841, Test Loss: 3.2928279876708983\n",
            "Epoch: 865, Train Loss: 0.8620, Test Loss: 3.2922032117843627\n",
            "Epoch: 866, Train Loss: 0.8741, Test Loss: 3.3291919231414795\n",
            "Epoch: 867, Train Loss: 0.8983, Test Loss: 3.337449145317078\n",
            "Epoch: 868, Train Loss: 0.8542, Test Loss: 3.309428834915161\n",
            "Epoch: 869, Train Loss: 0.8858, Test Loss: 3.313582706451416\n",
            "Epoch: 870, Train Loss: 0.8882, Test Loss: 3.28913471698761\n",
            "Epoch: 871, Train Loss: 0.8802, Test Loss: 3.3093040943145753\n",
            "Epoch: 872, Train Loss: 0.8710, Test Loss: 3.291602039337158\n",
            "Epoch: 873, Train Loss: 0.8883, Test Loss: 3.2726673603057863\n",
            "Epoch: 874, Train Loss: 0.9196, Test Loss: 3.318601679801941\n",
            "Epoch: 875, Train Loss: 0.8793, Test Loss: 3.313103437423706\n",
            "Epoch: 876, Train Loss: 0.8989, Test Loss: 3.3065792322158813\n",
            "Epoch: 877, Train Loss: 0.8002, Test Loss: 3.2802350521087646\n",
            "Epoch: 878, Train Loss: 0.8485, Test Loss: 3.312081217765808\n",
            "Epoch: 879, Train Loss: 0.8730, Test Loss: 3.292234992980957\n",
            "Epoch: 880, Train Loss: 0.8376, Test Loss: 3.29865140914917\n",
            "Epoch: 881, Train Loss: 0.8356, Test Loss: 3.2805749416351317\n",
            "Epoch: 882, Train Loss: 0.8874, Test Loss: 3.2852050065994263\n",
            "Epoch: 883, Train Loss: 0.8929, Test Loss: 3.29243381023407\n",
            "Epoch: 884, Train Loss: 0.8361, Test Loss: 3.2882495403289793\n",
            "Epoch: 885, Train Loss: 0.9295, Test Loss: 3.284265375137329\n",
            "Epoch: 886, Train Loss: 0.8242, Test Loss: 3.3137356758117678\n",
            "Epoch: 887, Train Loss: 0.8777, Test Loss: 3.3332255601882936\n",
            "Epoch: 888, Train Loss: 0.8968, Test Loss: 3.366927981376648\n",
            "Epoch: 889, Train Loss: 0.8969, Test Loss: 3.309794807434082\n",
            "Epoch: 890, Train Loss: 0.9230, Test Loss: 3.286256742477417\n",
            "Epoch: 891, Train Loss: 0.8691, Test Loss: 3.3172017097473145\n",
            "Epoch: 892, Train Loss: 0.8986, Test Loss: 3.3042571544647217\n",
            "Epoch: 893, Train Loss: 0.9177, Test Loss: 3.309877610206604\n",
            "Epoch: 894, Train Loss: 0.8759, Test Loss: 3.3169276237487795\n",
            "Epoch: 895, Train Loss: 0.8800, Test Loss: 3.308707118034363\n",
            "Epoch: 896, Train Loss: 0.8467, Test Loss: 3.3175884008407595\n",
            "Epoch: 897, Train Loss: 0.8461, Test Loss: 3.2868587017059325\n",
            "Epoch: 898, Train Loss: 0.8724, Test Loss: 3.286135959625244\n",
            "Epoch: 899, Train Loss: 0.8707, Test Loss: 3.2793416023254394\n",
            "Epoch: 900, Train Loss: 0.8521, Test Loss: 3.2878987312316896\n",
            "Epoch: 901, Train Loss: 0.8647, Test Loss: 3.30233747959137\n",
            "Epoch: 902, Train Loss: 0.9151, Test Loss: 3.282659482955933\n",
            "Epoch: 903, Train Loss: 0.8216, Test Loss: 3.3137253284454347\n",
            "Epoch: 904, Train Loss: 0.8314, Test Loss: 3.339733839035034\n",
            "Epoch: 905, Train Loss: 0.8544, Test Loss: 3.3086918354034425\n",
            "Epoch: 906, Train Loss: 0.8714, Test Loss: 3.273753261566162\n",
            "Epoch: 907, Train Loss: 0.8224, Test Loss: 3.298947834968567\n",
            "Epoch: 908, Train Loss: 0.9372, Test Loss: 3.2952635526657104\n",
            "Epoch: 909, Train Loss: 0.8974, Test Loss: 3.2740546226501466\n",
            "Epoch: 910, Train Loss: 0.9576, Test Loss: 3.272191572189331\n",
            "Epoch: 911, Train Loss: 0.9148, Test Loss: 3.3525867462158203\n",
            "Epoch: 912, Train Loss: 0.8295, Test Loss: 3.3031750917434692\n",
            "Epoch: 913, Train Loss: 0.8667, Test Loss: 3.274032378196716\n",
            "Epoch: 914, Train Loss: 0.8458, Test Loss: 3.354904842376709\n",
            "Epoch: 915, Train Loss: 0.8294, Test Loss: 3.3032707929611207\n",
            "Epoch: 916, Train Loss: 0.8308, Test Loss: 3.3125903606414795\n",
            "Epoch: 917, Train Loss: 0.8715, Test Loss: 3.3173685550689695\n",
            "Epoch: 918, Train Loss: 0.8661, Test Loss: 3.3172552585601807\n",
            "Epoch: 919, Train Loss: 0.8768, Test Loss: 3.3196134328842164\n",
            "Epoch: 920, Train Loss: 0.8563, Test Loss: 3.31271116733551\n",
            "Epoch: 921, Train Loss: 0.9142, Test Loss: 3.303753876686096\n",
            "Epoch: 922, Train Loss: 0.8789, Test Loss: 3.296077823638916\n",
            "Epoch: 923, Train Loss: 0.9108, Test Loss: 3.2852980375289915\n",
            "Epoch: 924, Train Loss: 0.8768, Test Loss: 3.306338667869568\n",
            "Epoch: 925, Train Loss: 0.8812, Test Loss: 3.330675196647644\n",
            "Epoch: 926, Train Loss: 0.8817, Test Loss: 3.310039520263672\n",
            "Epoch: 927, Train Loss: 0.8302, Test Loss: 3.3332380056381226\n",
            "Epoch: 928, Train Loss: 0.9011, Test Loss: 3.3092271089553833\n",
            "Epoch: 929, Train Loss: 0.8588, Test Loss: 3.317690634727478\n",
            "Epoch: 930, Train Loss: 0.8692, Test Loss: 3.3219123601913454\n",
            "Epoch: 931, Train Loss: 0.9055, Test Loss: 3.3139620780944825\n",
            "Epoch: 932, Train Loss: 0.8498, Test Loss: 3.333650732040405\n",
            "Epoch: 933, Train Loss: 0.8993, Test Loss: 3.3368947505950928\n",
            "Epoch: 934, Train Loss: 0.8439, Test Loss: 3.278943943977356\n",
            "Epoch: 935, Train Loss: 0.8411, Test Loss: 3.3188815355300902\n",
            "Epoch: 936, Train Loss: 0.9033, Test Loss: 3.294999527931213\n",
            "Epoch: 937, Train Loss: 0.8189, Test Loss: 3.2954675912857057\n",
            "Epoch: 938, Train Loss: 0.9089, Test Loss: 3.288105916976929\n",
            "Epoch: 939, Train Loss: 0.8203, Test Loss: 3.3143916606903074\n",
            "Epoch: 940, Train Loss: 0.8488, Test Loss: 3.2953206062316895\n",
            "Epoch: 941, Train Loss: 0.8672, Test Loss: 3.3013442754745483\n",
            "Epoch: 942, Train Loss: 0.9379, Test Loss: 3.333833122253418\n",
            "Epoch: 943, Train Loss: 0.8588, Test Loss: 3.2825081586837768\n",
            "Epoch: 944, Train Loss: 0.8276, Test Loss: 3.322629928588867\n",
            "Epoch: 945, Train Loss: 0.8622, Test Loss: 3.3017674684524536\n",
            "Epoch: 946, Train Loss: 0.8473, Test Loss: 3.293932580947876\n",
            "Epoch: 947, Train Loss: 0.8726, Test Loss: 3.3003251791000365\n",
            "Epoch: 948, Train Loss: 0.8648, Test Loss: 3.3161163091659547\n",
            "Epoch: 949, Train Loss: 0.8454, Test Loss: 3.287350606918335\n",
            "Epoch: 950, Train Loss: 0.8561, Test Loss: 3.3124223709106446\n",
            "Epoch: 951, Train Loss: 0.8556, Test Loss: 3.2960875749588014\n",
            "Epoch: 952, Train Loss: 0.8341, Test Loss: 3.3165050506591798\n",
            "Epoch: 953, Train Loss: 0.9028, Test Loss: 3.282007670402527\n",
            "Epoch: 954, Train Loss: 0.8737, Test Loss: 3.3030861139297487\n",
            "Epoch: 955, Train Loss: 0.8273, Test Loss: 3.30176637172699\n",
            "Epoch: 956, Train Loss: 0.8642, Test Loss: 3.300174593925476\n",
            "Epoch: 957, Train Loss: 0.9545, Test Loss: 3.312793469429016\n",
            "Epoch: 958, Train Loss: 0.8756, Test Loss: 3.3054186344146728\n",
            "Epoch: 959, Train Loss: 0.8457, Test Loss: 3.3042471408843994\n",
            "Epoch: 960, Train Loss: 0.8283, Test Loss: 3.347995162010193\n",
            "Epoch: 961, Train Loss: 0.8540, Test Loss: 3.301896810531616\n",
            "Epoch: 962, Train Loss: 0.8222, Test Loss: 3.298819422721863\n",
            "Epoch: 963, Train Loss: 0.9090, Test Loss: 3.311264681816101\n",
            "Epoch: 964, Train Loss: 0.9104, Test Loss: 3.3353837013244627\n",
            "Epoch: 965, Train Loss: 0.8573, Test Loss: 3.271231436729431\n",
            "Epoch: 966, Train Loss: 0.9493, Test Loss: 3.3017569303512575\n",
            "Epoch: 967, Train Loss: 0.9060, Test Loss: 3.3195476293563844\n",
            "Epoch: 968, Train Loss: 0.8426, Test Loss: 3.314285469055176\n",
            "Epoch: 969, Train Loss: 0.9809, Test Loss: 3.312734580039978\n",
            "Epoch: 970, Train Loss: 0.8602, Test Loss: 3.295296478271484\n",
            "Epoch: 971, Train Loss: 0.8873, Test Loss: 3.3184084415435793\n",
            "Epoch: 972, Train Loss: 0.8435, Test Loss: 3.288422870635986\n",
            "Epoch: 973, Train Loss: 0.9192, Test Loss: 3.2969046115875242\n",
            "Epoch: 974, Train Loss: 0.8608, Test Loss: 3.2750979900360107\n",
            "Epoch: 975, Train Loss: 0.8446, Test Loss: 3.3044848442077637\n",
            "Epoch: 976, Train Loss: 0.8835, Test Loss: 3.289990496635437\n",
            "Epoch: 977, Train Loss: 0.8692, Test Loss: 3.314055156707764\n",
            "Epoch: 978, Train Loss: 0.8534, Test Loss: 3.298136281967163\n",
            "Epoch: 979, Train Loss: 0.8634, Test Loss: 3.320106339454651\n",
            "Epoch: 980, Train Loss: 0.8884, Test Loss: 3.279082155227661\n",
            "Epoch: 981, Train Loss: 0.8483, Test Loss: 3.2960586309432984\n",
            "Epoch: 982, Train Loss: 0.8956, Test Loss: 3.313843297958374\n",
            "Epoch: 983, Train Loss: 0.8603, Test Loss: 3.2996636629104614\n",
            "Epoch: 984, Train Loss: 0.8147, Test Loss: 3.2908543825149534\n",
            "Epoch: 985, Train Loss: 0.9053, Test Loss: 3.304801368713379\n",
            "Epoch: 986, Train Loss: 0.8564, Test Loss: 3.292188024520874\n",
            "Epoch: 987, Train Loss: 0.8812, Test Loss: 3.296643853187561\n",
            "Epoch: 988, Train Loss: 0.9259, Test Loss: 3.2838891744613647\n",
            "Epoch: 989, Train Loss: 0.8359, Test Loss: 3.2898670196533204\n",
            "Epoch: 990, Train Loss: 0.8784, Test Loss: 3.29432008266449\n",
            "Epoch: 991, Train Loss: 0.8201, Test Loss: 3.3054915189743044\n",
            "Epoch: 992, Train Loss: 0.8685, Test Loss: 3.3078280687332153\n",
            "Epoch: 993, Train Loss: 0.8726, Test Loss: 3.2960269689559936\n",
            "Epoch: 994, Train Loss: 0.9049, Test Loss: 3.302090859413147\n",
            "Epoch: 995, Train Loss: 0.8025, Test Loss: 3.290038990974426\n",
            "Epoch: 996, Train Loss: 0.8231, Test Loss: 3.296211099624634\n",
            "Epoch: 997, Train Loss: 0.8784, Test Loss: 3.2833133697509767\n",
            "Epoch: 998, Train Loss: 0.8756, Test Loss: 3.283533787727356\n",
            "Epoch: 999, Train Loss: 0.9379, Test Loss: 3.3262911319732664\n",
            "Epoch: 1000, Train Loss: 0.8310, Test Loss: 3.3176167011260986\n",
            "Epoch: 1001, Train Loss: 0.9065, Test Loss: 3.302470874786377\n",
            "Epoch: 1002, Train Loss: 0.8574, Test Loss: 3.2769264221191405\n",
            "Epoch: 1003, Train Loss: 0.8354, Test Loss: 3.281888723373413\n",
            "Epoch: 1004, Train Loss: 0.8367, Test Loss: 3.2749802350997923\n",
            "Epoch: 1005, Train Loss: 0.9054, Test Loss: 3.271783709526062\n",
            "Epoch: 1006, Train Loss: 0.8229, Test Loss: 3.2780445337295534\n",
            "Epoch: 1007, Train Loss: 0.8179, Test Loss: 3.2976764678955077\n",
            "Epoch: 1008, Train Loss: 0.8698, Test Loss: 3.3000855922698973\n",
            "Epoch: 1009, Train Loss: 0.9092, Test Loss: 3.3134366989135744\n",
            "Epoch: 1010, Train Loss: 0.8407, Test Loss: 3.3102025508880617\n",
            "Epoch: 1011, Train Loss: 0.8597, Test Loss: 3.3164015293121336\n",
            "Epoch: 1012, Train Loss: 0.9399, Test Loss: 3.2823243141174316\n",
            "Epoch: 1013, Train Loss: 0.9480, Test Loss: 3.27201132774353\n",
            "Epoch: 1014, Train Loss: 0.8533, Test Loss: 3.2874197721481324\n",
            "Epoch: 1015, Train Loss: 0.8947, Test Loss: 3.273397707939148\n",
            "Epoch: 1016, Train Loss: 0.8484, Test Loss: 3.3294721603393556\n",
            "Epoch: 1017, Train Loss: 0.8739, Test Loss: 3.2890705347061155\n",
            "Epoch: 1018, Train Loss: 0.8803, Test Loss: 3.283617925643921\n",
            "Epoch: 1019, Train Loss: 0.8181, Test Loss: 3.2834044694900513\n",
            "Epoch: 1020, Train Loss: 0.8137, Test Loss: 3.2687307834625243\n",
            "Epoch: 1021, Train Loss: 0.8372, Test Loss: 3.311035418510437\n",
            "Epoch: 1022, Train Loss: 0.8347, Test Loss: 3.2816048860549927\n",
            "Epoch: 1023, Train Loss: 0.8996, Test Loss: 3.295266032218933\n",
            "Epoch: 1024, Train Loss: 0.8814, Test Loss: 3.277140665054321\n",
            "Epoch: 1025, Train Loss: 0.8438, Test Loss: 3.3370322942733766\n",
            "Epoch: 1026, Train Loss: 0.8422, Test Loss: 3.28721170425415\n",
            "Epoch: 1027, Train Loss: 0.8101, Test Loss: 3.3049956798553466\n",
            "Epoch: 1028, Train Loss: 0.8584, Test Loss: 3.2711257219314573\n",
            "Epoch: 1029, Train Loss: 0.8595, Test Loss: 3.3075713157653808\n",
            "Epoch: 1030, Train Loss: 0.8347, Test Loss: 3.3085309982299806\n",
            "Epoch: 1031, Train Loss: 0.8206, Test Loss: 3.305553126335144\n",
            "Epoch: 1032, Train Loss: 0.8963, Test Loss: 3.274847626686096\n",
            "Epoch: 1033, Train Loss: 0.8373, Test Loss: 3.280241298675537\n",
            "Epoch: 1034, Train Loss: 0.8567, Test Loss: 3.2980361700057985\n",
            "Epoch: 1035, Train Loss: 0.8396, Test Loss: 3.291995072364807\n",
            "Epoch: 1036, Train Loss: 0.8671, Test Loss: 3.3198380947113035\n",
            "Epoch: 1037, Train Loss: 0.8860, Test Loss: 3.3100964069366454\n",
            "Epoch: 1038, Train Loss: 0.8462, Test Loss: 3.320358228683472\n",
            "Epoch: 1039, Train Loss: 0.8425, Test Loss: 3.3005781412124633\n",
            "Epoch: 1040, Train Loss: 0.8194, Test Loss: 3.283592700958252\n",
            "Epoch: 1041, Train Loss: 0.9116, Test Loss: 3.285999035835266\n",
            "Epoch: 1042, Train Loss: 0.8811, Test Loss: 3.2884753942489624\n",
            "Epoch: 1043, Train Loss: 0.8614, Test Loss: 3.2878259658813476\n",
            "Epoch: 1044, Train Loss: 0.8553, Test Loss: 3.3138458013534544\n",
            "Epoch: 1045, Train Loss: 0.8346, Test Loss: 3.288178873062134\n",
            "Epoch: 1046, Train Loss: 0.8190, Test Loss: 3.2783594369888305\n",
            "Epoch: 1047, Train Loss: 0.8288, Test Loss: 3.2812779903411866\n",
            "Epoch: 1048, Train Loss: 0.8253, Test Loss: 3.2924251317977906\n",
            "Epoch: 1049, Train Loss: 0.8612, Test Loss: 3.29998574256897\n",
            "Epoch: 1050, Train Loss: 0.8174, Test Loss: 3.3216269969940186\n",
            "Epoch: 1051, Train Loss: 0.8086, Test Loss: 3.2779549837112425\n",
            "Epoch: 1052, Train Loss: 0.8573, Test Loss: 3.271449065208435\n",
            "Epoch: 1053, Train Loss: 0.8133, Test Loss: 3.2907811403274536\n",
            "Epoch: 1054, Train Loss: 0.8422, Test Loss: 3.266013479232788\n",
            "Epoch: 1055, Train Loss: 0.8885, Test Loss: 3.315713095664978\n",
            "Epoch: 1056, Train Loss: 0.9210, Test Loss: 3.2899187564849854\n",
            "Epoch: 1057, Train Loss: 0.8630, Test Loss: 3.261675477027893\n",
            "Epoch: 1058, Train Loss: 0.8831, Test Loss: 3.2883701801300047\n",
            "Epoch: 1059, Train Loss: 0.9083, Test Loss: 3.2882623434066773\n",
            "Epoch: 1060, Train Loss: 0.9224, Test Loss: 3.2643263578414916\n",
            "Epoch: 1061, Train Loss: 0.8666, Test Loss: 3.299366998672485\n",
            "Epoch: 1062, Train Loss: 0.8571, Test Loss: 3.2693366765975953\n",
            "Epoch: 1063, Train Loss: 0.8601, Test Loss: 3.287798047065735\n",
            "Epoch: 1064, Train Loss: 0.8043, Test Loss: 3.276423621177673\n",
            "Epoch: 1065, Train Loss: 0.8482, Test Loss: 3.281815528869629\n",
            "Epoch: 1066, Train Loss: 0.8696, Test Loss: 3.276087427139282\n",
            "Epoch: 1067, Train Loss: 0.7759, Test Loss: 3.289590334892273\n",
            "Epoch: 1068, Train Loss: 0.8626, Test Loss: 3.31611967086792\n",
            "Epoch: 1069, Train Loss: 0.8511, Test Loss: 3.3349143743515013\n",
            "Epoch: 1070, Train Loss: 0.9508, Test Loss: 3.274121642112732\n",
            "Epoch: 1071, Train Loss: 0.8370, Test Loss: 3.295297908782959\n",
            "Epoch: 1072, Train Loss: 0.8931, Test Loss: 3.2596071004867553\n",
            "Epoch: 1073, Train Loss: 0.8243, Test Loss: 3.2791035175323486\n",
            "Epoch: 1074, Train Loss: 0.8683, Test Loss: 3.2777733325958254\n",
            "Epoch: 1075, Train Loss: 0.8621, Test Loss: 3.3122813940048217\n",
            "Epoch: 1076, Train Loss: 0.8510, Test Loss: 3.2912421226501465\n",
            "Epoch: 1077, Train Loss: 0.8329, Test Loss: 3.307196307182312\n",
            "Epoch: 1078, Train Loss: 0.8603, Test Loss: 3.3124696731567385\n",
            "Epoch: 1079, Train Loss: 0.8110, Test Loss: 3.2944581270217896\n",
            "Epoch: 1080, Train Loss: 0.8719, Test Loss: 3.289234471321106\n",
            "Epoch: 1081, Train Loss: 0.8136, Test Loss: 3.3117733716964723\n",
            "Epoch: 1082, Train Loss: 0.8255, Test Loss: 3.274889850616455\n",
            "Epoch: 1083, Train Loss: 0.8024, Test Loss: 3.278608274459839\n",
            "Epoch: 1084, Train Loss: 0.8659, Test Loss: 3.2749514102935793\n",
            "Epoch: 1085, Train Loss: 0.8197, Test Loss: 3.2821604013442993\n",
            "Epoch: 1086, Train Loss: 0.8281, Test Loss: 3.2886223554611207\n",
            "Epoch: 1087, Train Loss: 0.8745, Test Loss: 3.286396288871765\n",
            "Epoch: 1088, Train Loss: 0.7999, Test Loss: 3.2858028173446656\n",
            "Epoch: 1089, Train Loss: 0.8298, Test Loss: 3.293194818496704\n",
            "Epoch: 1090, Train Loss: 0.8481, Test Loss: 3.288623595237732\n",
            "Epoch: 1091, Train Loss: 0.8454, Test Loss: 3.27383439540863\n",
            "Epoch: 1092, Train Loss: 0.8431, Test Loss: 3.2812418699264527\n",
            "Epoch: 1093, Train Loss: 0.8635, Test Loss: 3.327767753601074\n",
            "Epoch: 1094, Train Loss: 0.8486, Test Loss: 3.3029120683670046\n",
            "Epoch: 1095, Train Loss: 0.8069, Test Loss: 3.2809225797653196\n",
            "Epoch: 1096, Train Loss: 0.8249, Test Loss: 3.288829803466797\n",
            "Epoch: 1097, Train Loss: 0.8514, Test Loss: 3.2721559524536135\n",
            "Epoch: 1098, Train Loss: 0.7965, Test Loss: 3.3095544576644897\n",
            "Epoch: 1099, Train Loss: 0.8290, Test Loss: 3.2928894996643066\n",
            "Epoch: 1100, Train Loss: 0.8558, Test Loss: 3.3129266023635866\n",
            "Epoch: 1101, Train Loss: 0.8932, Test Loss: 3.350038170814514\n",
            "Epoch: 1102, Train Loss: 0.8637, Test Loss: 3.290954256057739\n",
            "Epoch: 1103, Train Loss: 0.7901, Test Loss: 3.2951515436172487\n",
            "Epoch: 1104, Train Loss: 0.8474, Test Loss: 3.3269880056381225\n",
            "Epoch: 1105, Train Loss: 0.8896, Test Loss: 3.312920880317688\n",
            "Epoch: 1106, Train Loss: 0.8603, Test Loss: 3.275755262374878\n",
            "Epoch: 1107, Train Loss: 0.8777, Test Loss: 3.29297354221344\n",
            "Epoch: 1108, Train Loss: 0.8265, Test Loss: 3.288223648071289\n",
            "Epoch: 1109, Train Loss: 0.9107, Test Loss: 3.2728405952453614\n",
            "Epoch: 1110, Train Loss: 0.8341, Test Loss: 3.293546223640442\n",
            "Epoch: 1111, Train Loss: 0.8171, Test Loss: 3.320317244529724\n",
            "Epoch: 1112, Train Loss: 0.9389, Test Loss: 3.2692725896835326\n",
            "Epoch: 1113, Train Loss: 0.8118, Test Loss: 3.294177770614624\n",
            "Epoch: 1114, Train Loss: 0.8763, Test Loss: 3.296903133392334\n",
            "Epoch: 1115, Train Loss: 0.8110, Test Loss: 3.3099928855895997\n",
            "Epoch: 1116, Train Loss: 0.9285, Test Loss: 3.265332317352295\n",
            "Epoch: 1117, Train Loss: 0.8760, Test Loss: 3.3609937191009522\n",
            "Epoch: 1118, Train Loss: 0.8967, Test Loss: 3.3171234130859375\n",
            "Epoch: 1119, Train Loss: 0.8459, Test Loss: 3.2987279415130617\n",
            "Epoch: 1120, Train Loss: 0.8230, Test Loss: 3.29972825050354\n",
            "Epoch: 1121, Train Loss: 0.8202, Test Loss: 3.294811797142029\n",
            "Epoch: 1122, Train Loss: 0.8282, Test Loss: 3.2892348527908326\n",
            "Epoch: 1123, Train Loss: 0.8395, Test Loss: 3.292512321472168\n",
            "Epoch: 1124, Train Loss: 0.8843, Test Loss: 3.274624466896057\n",
            "Epoch: 1125, Train Loss: 0.8656, Test Loss: 3.3385438919067383\n",
            "Epoch: 1126, Train Loss: 0.9002, Test Loss: 3.26800057888031\n",
            "Epoch: 1127, Train Loss: 0.8484, Test Loss: 3.2762609004974363\n",
            "Epoch: 1128, Train Loss: 0.9201, Test Loss: 3.269698071479797\n",
            "Epoch: 1129, Train Loss: 0.8711, Test Loss: 3.292867159843445\n",
            "Epoch: 1130, Train Loss: 0.8221, Test Loss: 3.2865508794784546\n",
            "Epoch: 1131, Train Loss: 0.8324, Test Loss: 3.3446974039077757\n",
            "Epoch: 1132, Train Loss: 0.8938, Test Loss: 3.274627184867859\n",
            "Epoch: 1133, Train Loss: 0.8678, Test Loss: 3.3062079906463624\n",
            "Epoch: 1134, Train Loss: 0.8674, Test Loss: 3.297407698631287\n",
            "Epoch: 1135, Train Loss: 0.8624, Test Loss: 3.2846177577972413\n",
            "Epoch: 1136, Train Loss: 0.7868, Test Loss: 3.3004790782928466\n",
            "Epoch: 1137, Train Loss: 0.8923, Test Loss: 3.279695177078247\n",
            "Epoch: 1138, Train Loss: 0.7574, Test Loss: 3.295340061187744\n",
            "Epoch: 1139, Train Loss: 0.8626, Test Loss: 3.2803355932235716\n",
            "Epoch: 1140, Train Loss: 0.8260, Test Loss: 3.287532591819763\n",
            "Epoch: 1141, Train Loss: 0.8571, Test Loss: 3.2870316982269285\n",
            "Epoch: 1142, Train Loss: 0.8459, Test Loss: 3.2918875217437744\n",
            "Epoch: 1143, Train Loss: 0.8760, Test Loss: 3.2796502113342285\n",
            "Epoch: 1144, Train Loss: 0.8241, Test Loss: 3.282497501373291\n",
            "Epoch: 1145, Train Loss: 0.8563, Test Loss: 3.3076092481613157\n",
            "Epoch: 1146, Train Loss: 0.8356, Test Loss: 3.306483817100525\n",
            "Epoch: 1147, Train Loss: 0.8633, Test Loss: 3.2952269077301026\n",
            "Epoch: 1148, Train Loss: 0.7912, Test Loss: 3.288613510131836\n",
            "Epoch: 1149, Train Loss: 0.8023, Test Loss: 3.272887444496155\n",
            "Epoch: 1150, Train Loss: 0.8320, Test Loss: 3.274827170372009\n",
            "Epoch: 1151, Train Loss: 0.8848, Test Loss: 3.3222603797912598\n",
            "Epoch: 1152, Train Loss: 0.8302, Test Loss: 3.2938268899917604\n",
            "Epoch: 1153, Train Loss: 0.7940, Test Loss: 3.3101698637008665\n",
            "Epoch: 1154, Train Loss: 0.8475, Test Loss: 3.2646878957748413\n",
            "Epoch: 1155, Train Loss: 0.8465, Test Loss: 3.3401519775390627\n",
            "Epoch: 1156, Train Loss: 0.8543, Test Loss: 3.2778709888458253\n",
            "Epoch: 1157, Train Loss: 0.8997, Test Loss: 3.2982515335083007\n",
            "Epoch: 1158, Train Loss: 0.8989, Test Loss: 3.2702654123306276\n",
            "Epoch: 1159, Train Loss: 0.8211, Test Loss: 3.2765533924102783\n",
            "Epoch: 1160, Train Loss: 0.8581, Test Loss: 3.3356104135513305\n",
            "Epoch: 1161, Train Loss: 0.8776, Test Loss: 3.400688982009888\n",
            "Epoch: 1162, Train Loss: 0.8176, Test Loss: 3.2827280521392823\n",
            "Epoch: 1163, Train Loss: 0.8699, Test Loss: 3.287140655517578\n",
            "Epoch: 1164, Train Loss: 0.9194, Test Loss: 3.3273279428482057\n",
            "Epoch: 1165, Train Loss: 0.7816, Test Loss: 3.29587869644165\n",
            "Epoch: 1166, Train Loss: 0.8811, Test Loss: 3.291477084159851\n",
            "Epoch: 1167, Train Loss: 0.8311, Test Loss: 3.3001299142837524\n",
            "Epoch: 1168, Train Loss: 0.8278, Test Loss: 3.2761507987976075\n",
            "Epoch: 1169, Train Loss: 0.8245, Test Loss: 3.3152994394302366\n",
            "Epoch: 1170, Train Loss: 0.8294, Test Loss: 3.312944531440735\n",
            "Epoch: 1171, Train Loss: 0.8835, Test Loss: 3.308042073249817\n",
            "Epoch: 1172, Train Loss: 0.8445, Test Loss: 3.2924832582473753\n",
            "Epoch: 1173, Train Loss: 0.8793, Test Loss: 3.2616408109664916\n",
            "Epoch: 1174, Train Loss: 0.8192, Test Loss: 3.2746964931488036\n",
            "Epoch: 1175, Train Loss: 0.7873, Test Loss: 3.288482642173767\n",
            "Epoch: 1176, Train Loss: 0.8669, Test Loss: 3.299737882614136\n",
            "Epoch: 1177, Train Loss: 0.8491, Test Loss: 3.320199704170227\n",
            "Epoch: 1178, Train Loss: 0.8066, Test Loss: 3.2679665327072143\n",
            "Epoch: 1179, Train Loss: 0.8630, Test Loss: 3.304470753669739\n",
            "Epoch: 1180, Train Loss: 0.8339, Test Loss: 3.325120043754578\n",
            "Epoch: 1181, Train Loss: 0.8515, Test Loss: 3.3119016170501707\n",
            "Epoch: 1182, Train Loss: 0.8249, Test Loss: 3.294491243362427\n",
            "Epoch: 1183, Train Loss: 0.8471, Test Loss: 3.2864233016967774\n",
            "Epoch: 1184, Train Loss: 0.8775, Test Loss: 3.3248101711273192\n",
            "Epoch: 1185, Train Loss: 0.8773, Test Loss: 3.309879493713379\n",
            "Epoch: 1186, Train Loss: 0.8171, Test Loss: 3.282268786430359\n",
            "Epoch: 1187, Train Loss: 0.8301, Test Loss: 3.300017309188843\n",
            "Epoch: 1188, Train Loss: 0.8256, Test Loss: 3.295929789543152\n",
            "Epoch: 1189, Train Loss: 0.8315, Test Loss: 3.279814124107361\n",
            "Epoch: 1190, Train Loss: 0.8370, Test Loss: 3.2978556394577025\n",
            "Epoch: 1191, Train Loss: 0.8309, Test Loss: 3.2973390579223634\n",
            "Epoch: 1192, Train Loss: 0.8551, Test Loss: 3.3262909412384034\n",
            "Epoch: 1193, Train Loss: 0.8908, Test Loss: 3.3294930458068848\n",
            "Epoch: 1194, Train Loss: 0.8479, Test Loss: 3.3323574542999266\n",
            "Epoch: 1195, Train Loss: 0.8355, Test Loss: 3.2980946779251097\n",
            "Epoch: 1196, Train Loss: 0.9361, Test Loss: 3.2791159391403197\n",
            "Epoch: 1197, Train Loss: 0.8728, Test Loss: 3.29312264919281\n",
            "Epoch: 1198, Train Loss: 0.8210, Test Loss: 3.3161819696426393\n",
            "Epoch: 1199, Train Loss: 0.8252, Test Loss: 3.3124306440353393\n",
            "Epoch: 1200, Train Loss: 0.8338, Test Loss: 3.3101916790008543\n",
            "Epoch: 1201, Train Loss: 0.8240, Test Loss: 3.268173933029175\n",
            "Epoch: 1202, Train Loss: 0.8183, Test Loss: 3.3169190168380736\n",
            "Epoch: 1203, Train Loss: 0.7934, Test Loss: 3.318922019004822\n",
            "Epoch: 1204, Train Loss: 0.8982, Test Loss: 3.268699860572815\n",
            "Epoch: 1205, Train Loss: 0.8537, Test Loss: 3.2950860261917114\n",
            "Epoch: 1206, Train Loss: 0.8591, Test Loss: 3.28695125579834\n",
            "Epoch: 1207, Train Loss: 0.7971, Test Loss: 3.2828973531723022\n",
            "Epoch: 1208, Train Loss: 0.8360, Test Loss: 3.2736985445022584\n",
            "Epoch: 1209, Train Loss: 0.8807, Test Loss: 3.308515119552612\n",
            "Epoch: 1210, Train Loss: 0.8644, Test Loss: 3.2736114025115968\n",
            "Epoch: 1211, Train Loss: 0.8975, Test Loss: 3.316687512397766\n",
            "Epoch: 1212, Train Loss: 0.9365, Test Loss: 3.2668773889541627\n",
            "Epoch: 1213, Train Loss: 0.7786, Test Loss: 3.269294762611389\n",
            "Epoch: 1214, Train Loss: 0.8731, Test Loss: 3.2700538635253906\n",
            "Epoch: 1215, Train Loss: 0.8093, Test Loss: 3.2793577671051026\n",
            "Epoch: 1216, Train Loss: 0.7970, Test Loss: 3.32141969203949\n",
            "Epoch: 1217, Train Loss: 0.8635, Test Loss: 3.2909722566604613\n",
            "Epoch: 1218, Train Loss: 0.8572, Test Loss: 3.281990098953247\n",
            "Epoch: 1219, Train Loss: 0.8292, Test Loss: 3.304036998748779\n",
            "Epoch: 1220, Train Loss: 0.8128, Test Loss: 3.30344820022583\n",
            "Epoch: 1221, Train Loss: 0.9405, Test Loss: 3.25190749168396\n",
            "Epoch: 1222, Train Loss: 0.8714, Test Loss: 3.269198679924011\n",
            "Epoch: 1223, Train Loss: 0.8068, Test Loss: 3.2688883543014526\n",
            "Epoch: 1224, Train Loss: 0.8478, Test Loss: 3.261951756477356\n",
            "Epoch: 1225, Train Loss: 0.7946, Test Loss: 3.292797875404358\n",
            "Epoch: 1226, Train Loss: 0.9043, Test Loss: 3.284896469116211\n",
            "Epoch: 1227, Train Loss: 0.8877, Test Loss: 3.284860849380493\n",
            "Epoch: 1228, Train Loss: 0.8557, Test Loss: 3.294612693786621\n",
            "Epoch: 1229, Train Loss: 0.8313, Test Loss: 3.2772652864456178\n",
            "Epoch: 1230, Train Loss: 0.8916, Test Loss: 3.2740251541137697\n",
            "Epoch: 1231, Train Loss: 0.7988, Test Loss: 3.2787625312805178\n",
            "Epoch: 1232, Train Loss: 0.7776, Test Loss: 3.2768787384033202\n",
            "Epoch: 1233, Train Loss: 0.8365, Test Loss: 3.264057493209839\n",
            "Epoch: 1234, Train Loss: 0.7977, Test Loss: 3.3059014081954956\n",
            "Epoch: 1235, Train Loss: 0.8278, Test Loss: 3.296809458732605\n",
            "Epoch: 1236, Train Loss: 0.7901, Test Loss: 3.311280059814453\n",
            "Epoch: 1237, Train Loss: 0.8280, Test Loss: 3.2826859951019287\n",
            "Epoch: 1238, Train Loss: 0.8850, Test Loss: 3.2734662532806396\n",
            "Epoch: 1239, Train Loss: 0.8127, Test Loss: 3.272757315635681\n",
            "Epoch: 1240, Train Loss: 0.8593, Test Loss: 3.2884193897247314\n",
            "Epoch: 1241, Train Loss: 0.8040, Test Loss: 3.3079288959503175\n",
            "Epoch: 1242, Train Loss: 0.8177, Test Loss: 3.2984068393707275\n",
            "Epoch: 1243, Train Loss: 0.8220, Test Loss: 3.289497232437134\n",
            "Epoch: 1244, Train Loss: 0.8074, Test Loss: 3.283488059043884\n",
            "Epoch: 1245, Train Loss: 0.8478, Test Loss: 3.29248161315918\n",
            "Epoch: 1246, Train Loss: 0.8091, Test Loss: 3.279569625854492\n",
            "Epoch: 1247, Train Loss: 0.8786, Test Loss: 3.261791133880615\n",
            "Epoch: 1248, Train Loss: 0.8588, Test Loss: 3.2736737012863157\n",
            "Epoch: 1249, Train Loss: 0.8171, Test Loss: 3.2708108901977537\n",
            "Epoch: 1250, Train Loss: 0.8439, Test Loss: 3.2755482673645018\n",
            "Epoch: 1251, Train Loss: 0.8932, Test Loss: 3.2851843357086183\n",
            "Epoch: 1252, Train Loss: 0.8458, Test Loss: 3.3038178205490114\n",
            "Epoch: 1253, Train Loss: 0.7806, Test Loss: 3.2736814498901365\n",
            "Epoch: 1254, Train Loss: 0.8697, Test Loss: 3.2624412775039673\n",
            "Epoch: 1255, Train Loss: 0.7781, Test Loss: 3.2670021772384645\n",
            "Epoch: 1256, Train Loss: 0.7956, Test Loss: 3.2964609384536745\n",
            "Epoch: 1257, Train Loss: 0.8373, Test Loss: 3.263802671432495\n",
            "Epoch: 1258, Train Loss: 0.8164, Test Loss: 3.2659133434295655\n",
            "Epoch: 1259, Train Loss: 0.8652, Test Loss: 3.290954899787903\n",
            "Epoch: 1260, Train Loss: 0.8260, Test Loss: 3.319441890716553\n",
            "Epoch: 1261, Train Loss: 0.8588, Test Loss: 3.272219252586365\n",
            "Epoch: 1262, Train Loss: 0.7952, Test Loss: 3.270546889305115\n",
            "Epoch: 1263, Train Loss: 0.7995, Test Loss: 3.2800305843353272\n",
            "Epoch: 1264, Train Loss: 0.8347, Test Loss: 3.2751793384552004\n",
            "Epoch: 1265, Train Loss: 0.8057, Test Loss: 3.278846526145935\n",
            "Epoch: 1266, Train Loss: 0.8268, Test Loss: 3.2617877960205077\n",
            "Epoch: 1267, Train Loss: 0.8054, Test Loss: 3.276789999008179\n",
            "Epoch: 1268, Train Loss: 0.7569, Test Loss: 3.2657269477844237\n",
            "Epoch: 1269, Train Loss: 0.8626, Test Loss: 3.255917012691498\n",
            "Epoch: 1270, Train Loss: 0.8484, Test Loss: 3.2814482688903808\n",
            "Epoch: 1271, Train Loss: 0.7990, Test Loss: 3.2925095558166504\n",
            "Epoch: 1272, Train Loss: 0.8414, Test Loss: 3.301696300506592\n",
            "Epoch: 1273, Train Loss: 0.8289, Test Loss: 3.289092493057251\n",
            "Epoch: 1274, Train Loss: 0.9084, Test Loss: 3.305210566520691\n",
            "Epoch: 1275, Train Loss: 0.8617, Test Loss: 3.2784040212631225\n",
            "Epoch: 1276, Train Loss: 0.8544, Test Loss: 3.286953020095825\n",
            "Epoch: 1277, Train Loss: 0.8273, Test Loss: 3.261442852020264\n",
            "Epoch: 1278, Train Loss: 0.9508, Test Loss: 3.273112988471985\n",
            "Epoch: 1279, Train Loss: 0.8155, Test Loss: 3.283967447280884\n",
            "Epoch: 1280, Train Loss: 0.8409, Test Loss: 3.2715975284576415\n",
            "Epoch: 1281, Train Loss: 0.9211, Test Loss: 3.280812883377075\n",
            "Epoch: 1282, Train Loss: 0.8386, Test Loss: 3.3107030630111693\n",
            "Epoch: 1283, Train Loss: 0.8454, Test Loss: 3.2800257921218874\n",
            "Epoch: 1284, Train Loss: 0.8037, Test Loss: 3.2820780754089354\n",
            "Epoch: 1285, Train Loss: 0.8488, Test Loss: 3.274945878982544\n",
            "Epoch: 1286, Train Loss: 0.8170, Test Loss: 3.2791470766067503\n",
            "Epoch: 1287, Train Loss: 0.7862, Test Loss: 3.2659074544906614\n",
            "Epoch: 1288, Train Loss: 0.8864, Test Loss: 3.2927748918533326\n",
            "Epoch: 1289, Train Loss: 0.9001, Test Loss: 3.2793986320495607\n",
            "Epoch: 1290, Train Loss: 0.8195, Test Loss: 3.269732356071472\n",
            "Epoch: 1291, Train Loss: 0.8119, Test Loss: 3.30217969417572\n",
            "Epoch: 1292, Train Loss: 0.8015, Test Loss: 3.2637747526168823\n",
            "Epoch: 1293, Train Loss: 0.8140, Test Loss: 3.2638949275016786\n",
            "Epoch: 1294, Train Loss: 0.8229, Test Loss: 3.261965274810791\n",
            "Epoch: 1295, Train Loss: 0.8767, Test Loss: 3.3021324634552003\n",
            "Epoch: 1296, Train Loss: 0.7973, Test Loss: 3.2892489194869996\n",
            "Epoch: 1297, Train Loss: 0.7977, Test Loss: 3.279579997062683\n",
            "Epoch: 1298, Train Loss: 0.8589, Test Loss: 3.349640679359436\n",
            "Epoch: 1299, Train Loss: 0.7915, Test Loss: 3.2750239849090574\n",
            "Epoch: 1300, Train Loss: 0.8531, Test Loss: 3.313035559654236\n",
            "Epoch: 1301, Train Loss: 0.8114, Test Loss: 3.2943724393844604\n",
            "Epoch: 1302, Train Loss: 0.9149, Test Loss: 3.2863765478134157\n",
            "Epoch: 1303, Train Loss: 0.7864, Test Loss: 3.2886335849761963\n",
            "Epoch: 1304, Train Loss: 0.8090, Test Loss: 3.320200705528259\n",
            "Epoch: 1305, Train Loss: 0.8443, Test Loss: 3.308259057998657\n",
            "Epoch: 1306, Train Loss: 0.8701, Test Loss: 3.337226915359497\n",
            "Epoch: 1307, Train Loss: 0.8357, Test Loss: 3.304767203330994\n",
            "Epoch: 1308, Train Loss: 0.8482, Test Loss: 3.2838553190231323\n",
            "Epoch: 1309, Train Loss: 0.8832, Test Loss: 3.3044312953948975\n",
            "Epoch: 1310, Train Loss: 0.8853, Test Loss: 3.288326597213745\n",
            "Epoch: 1311, Train Loss: 0.7818, Test Loss: 3.270973634719849\n",
            "Epoch: 1312, Train Loss: 0.8086, Test Loss: 3.273190641403198\n",
            "Epoch: 1313, Train Loss: 0.8322, Test Loss: 3.277321529388428\n",
            "Epoch: 1314, Train Loss: 0.8328, Test Loss: 3.324795651435852\n",
            "Epoch: 1315, Train Loss: 0.8177, Test Loss: 3.3476192474365236\n",
            "Epoch: 1316, Train Loss: 0.8559, Test Loss: 3.296770215034485\n",
            "Epoch: 1317, Train Loss: 0.8353, Test Loss: 3.2858052492141723\n",
            "Epoch: 1318, Train Loss: 0.8428, Test Loss: 3.2979963541030886\n",
            "Epoch: 1319, Train Loss: 0.8104, Test Loss: 3.286428189277649\n",
            "Epoch: 1320, Train Loss: 0.8304, Test Loss: 3.3282570123672484\n",
            "Epoch: 1321, Train Loss: 0.8328, Test Loss: 3.2839242935180666\n",
            "Epoch: 1322, Train Loss: 0.8011, Test Loss: 3.2920533418655396\n",
            "Epoch: 1323, Train Loss: 0.7898, Test Loss: 3.293417501449585\n",
            "Epoch: 1324, Train Loss: 0.8494, Test Loss: 3.3378978252410887\n",
            "Epoch: 1325, Train Loss: 0.8303, Test Loss: 3.3030508518218995\n",
            "Epoch: 1326, Train Loss: 0.8226, Test Loss: 3.2812336921691894\n",
            "Epoch: 1327, Train Loss: 0.8654, Test Loss: 3.306633949279785\n",
            "Epoch: 1328, Train Loss: 0.8502, Test Loss: 3.2788849830627442\n",
            "Epoch: 1329, Train Loss: 0.8070, Test Loss: 3.3121501445770263\n",
            "Epoch: 1330, Train Loss: 0.8294, Test Loss: 3.2783806800842283\n",
            "Epoch: 1331, Train Loss: 0.8461, Test Loss: 3.3334249019622804\n",
            "Epoch: 1332, Train Loss: 0.8774, Test Loss: 3.2953680038452147\n",
            "Epoch: 1333, Train Loss: 0.8255, Test Loss: 3.3009224414825438\n",
            "Epoch: 1334, Train Loss: 0.8722, Test Loss: 3.2884129524230956\n",
            "Epoch: 1335, Train Loss: 0.8382, Test Loss: 3.302858066558838\n",
            "Epoch: 1336, Train Loss: 0.8280, Test Loss: 3.2840439558029173\n",
            "Epoch: 1337, Train Loss: 0.8213, Test Loss: 3.2960139513015747\n",
            "Epoch: 1338, Train Loss: 0.8451, Test Loss: 3.3361788511276247\n",
            "Epoch: 1339, Train Loss: 0.8572, Test Loss: 3.300167751312256\n",
            "Epoch: 1340, Train Loss: 0.7926, Test Loss: 3.3160735845565794\n",
            "Epoch: 1341, Train Loss: 0.8244, Test Loss: 3.2896966457366945\n",
            "Epoch: 1342, Train Loss: 0.8302, Test Loss: 3.306537938117981\n",
            "Epoch: 1343, Train Loss: 0.8051, Test Loss: 3.2800507068634035\n",
            "Epoch: 1344, Train Loss: 0.9203, Test Loss: 3.317227029800415\n",
            "Epoch: 1345, Train Loss: 0.7955, Test Loss: 3.2905709505081178\n",
            "Epoch: 1346, Train Loss: 0.8046, Test Loss: 3.3053022146224977\n",
            "Epoch: 1347, Train Loss: 0.8563, Test Loss: 3.3039480447769165\n",
            "Epoch: 1348, Train Loss: 0.8487, Test Loss: 3.293050694465637\n",
            "Epoch: 1349, Train Loss: 0.7863, Test Loss: 3.2772709608078\n",
            "Epoch: 1350, Train Loss: 0.9484, Test Loss: 3.283091831207275\n",
            "Epoch: 1351, Train Loss: 0.8697, Test Loss: 3.3339404582977297\n",
            "Epoch: 1352, Train Loss: 0.8351, Test Loss: 3.267292594909668\n",
            "Epoch: 1353, Train Loss: 0.8643, Test Loss: 3.3245269060134888\n",
            "Epoch: 1354, Train Loss: 0.7939, Test Loss: 3.2817880392074583\n",
            "Epoch: 1355, Train Loss: 0.8479, Test Loss: 3.296794867515564\n",
            "Epoch: 1356, Train Loss: 0.8417, Test Loss: 3.2921547412872316\n",
            "Epoch: 1357, Train Loss: 0.8071, Test Loss: 3.287882661819458\n",
            "Epoch: 1358, Train Loss: 0.8140, Test Loss: 3.2982937335968017\n",
            "Epoch: 1359, Train Loss: 0.8409, Test Loss: 3.2947100162506104\n",
            "Epoch: 1360, Train Loss: 0.7609, Test Loss: 3.3035793781280516\n",
            "Epoch: 1361, Train Loss: 0.7625, Test Loss: 3.307477116584778\n",
            "Epoch: 1362, Train Loss: 0.8212, Test Loss: 3.2622046947479246\n",
            "Epoch: 1363, Train Loss: 0.8578, Test Loss: 3.2713648319244384\n",
            "Epoch: 1364, Train Loss: 0.7858, Test Loss: 3.269054484367371\n",
            "Epoch: 1365, Train Loss: 0.8219, Test Loss: 3.276917314529419\n",
            "Epoch: 1366, Train Loss: 0.8338, Test Loss: 3.2825695753097532\n",
            "Epoch: 1367, Train Loss: 0.8561, Test Loss: 3.26642529964447\n",
            "Epoch: 1368, Train Loss: 0.8004, Test Loss: 3.2774122953414917\n",
            "Epoch: 1369, Train Loss: 0.8278, Test Loss: 3.2795413017272947\n",
            "Epoch: 1370, Train Loss: 0.7666, Test Loss: 3.281313443183899\n",
            "Epoch: 1371, Train Loss: 0.8061, Test Loss: 3.2693372488021852\n",
            "Epoch: 1372, Train Loss: 0.8323, Test Loss: 3.288999009132385\n",
            "Epoch: 1373, Train Loss: 0.7929, Test Loss: 3.2906866312026977\n",
            "Epoch: 1374, Train Loss: 0.8557, Test Loss: 3.2784162521362306\n",
            "Epoch: 1375, Train Loss: 0.7998, Test Loss: 3.2798147201538086\n",
            "Epoch: 1376, Train Loss: 0.8572, Test Loss: 3.31350953578949\n",
            "Epoch: 1377, Train Loss: 0.8383, Test Loss: 3.31064293384552\n",
            "Epoch: 1378, Train Loss: 0.8521, Test Loss: 3.277544116973877\n",
            "Epoch: 1379, Train Loss: 0.8945, Test Loss: 3.303085517883301\n",
            "Epoch: 1380, Train Loss: 0.8197, Test Loss: 3.304350662231445\n",
            "Epoch: 1381, Train Loss: 0.8430, Test Loss: 3.326252746582031\n",
            "Epoch: 1382, Train Loss: 0.8863, Test Loss: 3.365171217918396\n",
            "Epoch: 1383, Train Loss: 0.8287, Test Loss: 3.2913376808166506\n",
            "Epoch: 1384, Train Loss: 0.7889, Test Loss: 3.277243161201477\n",
            "Epoch: 1385, Train Loss: 0.9289, Test Loss: 3.288857626914978\n",
            "Epoch: 1386, Train Loss: 0.8276, Test Loss: 3.3122885465621947\n",
            "Epoch: 1387, Train Loss: 0.8212, Test Loss: 3.3086182117462157\n",
            "Epoch: 1388, Train Loss: 0.8743, Test Loss: 3.2759416341781615\n",
            "Epoch: 1389, Train Loss: 0.8173, Test Loss: 3.2918927907943725\n",
            "Epoch: 1390, Train Loss: 0.8952, Test Loss: 3.3199183464050295\n",
            "Epoch: 1391, Train Loss: 0.8971, Test Loss: 3.359197211265564\n",
            "Epoch: 1392, Train Loss: 0.8737, Test Loss: 3.3313090801239014\n",
            "Epoch: 1393, Train Loss: 0.7973, Test Loss: 3.2932268142700196\n",
            "Epoch: 1394, Train Loss: 0.7780, Test Loss: 3.2754453659057616\n",
            "Epoch: 1395, Train Loss: 0.7778, Test Loss: 3.2676727056503294\n",
            "Epoch: 1396, Train Loss: 0.8425, Test Loss: 3.311393404006958\n",
            "Epoch: 1397, Train Loss: 0.8882, Test Loss: 3.3081364154815676\n",
            "Epoch: 1398, Train Loss: 0.8723, Test Loss: 3.3096047163009645\n",
            "Epoch: 1399, Train Loss: 0.8664, Test Loss: 3.3008529424667357\n",
            "Epoch: 1400, Train Loss: 0.7327, Test Loss: 3.3063827991485595\n",
            "Epoch: 1401, Train Loss: 0.8866, Test Loss: 3.279625940322876\n",
            "Epoch: 1402, Train Loss: 0.8101, Test Loss: 3.284563136100769\n",
            "Epoch: 1403, Train Loss: 0.7879, Test Loss: 3.285292458534241\n",
            "Epoch: 1404, Train Loss: 0.8394, Test Loss: 3.277511477470398\n",
            "Epoch: 1405, Train Loss: 0.8375, Test Loss: 3.3010544776916504\n",
            "Epoch: 1406, Train Loss: 0.8217, Test Loss: 3.293548059463501\n",
            "Epoch: 1407, Train Loss: 0.8661, Test Loss: 3.328875207901001\n",
            "Epoch: 1408, Train Loss: 0.8846, Test Loss: 3.2696380376815797\n",
            "Epoch: 1409, Train Loss: 0.8474, Test Loss: 3.2776687860488893\n",
            "Epoch: 1410, Train Loss: 0.8447, Test Loss: 3.276572036743164\n",
            "Epoch: 1411, Train Loss: 0.8396, Test Loss: 3.3000255823135376\n",
            "Epoch: 1412, Train Loss: 0.8471, Test Loss: 3.2851224660873415\n",
            "Epoch: 1413, Train Loss: 0.8148, Test Loss: 3.2866189241409303\n",
            "Epoch: 1414, Train Loss: 0.8472, Test Loss: 3.2858460187911986\n",
            "Epoch: 1415, Train Loss: 0.8160, Test Loss: 3.2894585609436033\n",
            "Epoch: 1416, Train Loss: 0.8174, Test Loss: 3.2700287342071532\n",
            "Epoch: 1417, Train Loss: 0.8724, Test Loss: 3.275407099723816\n",
            "Epoch: 1418, Train Loss: 0.8084, Test Loss: 3.2526818752288817\n",
            "Epoch: 1419, Train Loss: 0.8018, Test Loss: 3.284599852561951\n",
            "Epoch: 1420, Train Loss: 0.8304, Test Loss: 3.2668383598327635\n",
            "Epoch: 1421, Train Loss: 0.8514, Test Loss: 3.274911046028137\n",
            "Epoch: 1422, Train Loss: 0.8449, Test Loss: 3.3045273542404177\n",
            "Epoch: 1423, Train Loss: 0.8386, Test Loss: 3.2983739137649537\n",
            "Epoch: 1424, Train Loss: 0.8472, Test Loss: 3.296205496788025\n",
            "Epoch: 1425, Train Loss: 0.7724, Test Loss: 3.3088361263275146\n",
            "Epoch: 1426, Train Loss: 0.7836, Test Loss: 3.2837185144424437\n",
            "Epoch: 1427, Train Loss: 0.8333, Test Loss: 3.2723402261734007\n",
            "Epoch: 1428, Train Loss: 0.8344, Test Loss: 3.2930924892425537\n",
            "Epoch: 1429, Train Loss: 0.8019, Test Loss: 3.262005829811096\n",
            "Epoch: 1430, Train Loss: 0.8685, Test Loss: 3.2566649198532103\n",
            "Epoch: 1431, Train Loss: 0.7765, Test Loss: 3.3038116216659548\n",
            "Epoch: 1432, Train Loss: 0.7835, Test Loss: 3.2562942504882812\n",
            "Epoch: 1433, Train Loss: 0.8519, Test Loss: 3.2515403985977174\n",
            "Epoch: 1434, Train Loss: 0.8390, Test Loss: 3.273865795135498\n",
            "Epoch: 1435, Train Loss: 0.8461, Test Loss: 3.302421474456787\n",
            "Epoch: 1436, Train Loss: 0.8305, Test Loss: 3.296952152252197\n",
            "Epoch: 1437, Train Loss: 0.7767, Test Loss: 3.2583435773849487\n",
            "Epoch: 1438, Train Loss: 0.8737, Test Loss: 3.2773171186447145\n",
            "Epoch: 1439, Train Loss: 0.8405, Test Loss: 3.266352677345276\n",
            "Epoch: 1440, Train Loss: 0.9160, Test Loss: 3.29057981967926\n",
            "Epoch: 1441, Train Loss: 0.8574, Test Loss: 3.276068902015686\n",
            "Epoch: 1442, Train Loss: 0.8161, Test Loss: 3.283724617958069\n",
            "Epoch: 1443, Train Loss: 0.9293, Test Loss: 3.2673213720321654\n",
            "Epoch: 1444, Train Loss: 0.8192, Test Loss: 3.2727455615997316\n",
            "Epoch: 1445, Train Loss: 0.8470, Test Loss: 3.2764408111572267\n",
            "Epoch: 1446, Train Loss: 0.8348, Test Loss: 3.2713446378707887\n",
            "Epoch: 1447, Train Loss: 0.8427, Test Loss: 3.288928818702698\n",
            "Epoch: 1448, Train Loss: 0.8332, Test Loss: 3.2561474084854125\n",
            "Epoch: 1449, Train Loss: 0.7690, Test Loss: 3.2614469051361086\n",
            "Epoch: 1450, Train Loss: 0.8418, Test Loss: 3.2861993312835693\n",
            "Epoch: 1451, Train Loss: 0.7758, Test Loss: 3.2706926822662354\n",
            "Epoch: 1452, Train Loss: 0.8595, Test Loss: 3.277925968170166\n",
            "Epoch: 1453, Train Loss: 0.8086, Test Loss: 3.278232049942017\n",
            "Epoch: 1454, Train Loss: 0.8560, Test Loss: 3.2635679483413695\n",
            "Epoch: 1455, Train Loss: 0.8300, Test Loss: 3.2790760040283202\n",
            "Epoch: 1456, Train Loss: 0.8131, Test Loss: 3.3246846199035645\n",
            "Epoch: 1457, Train Loss: 0.8672, Test Loss: 3.302436590194702\n",
            "Epoch: 1458, Train Loss: 0.8220, Test Loss: 3.303456211090088\n",
            "Epoch: 1459, Train Loss: 0.7920, Test Loss: 3.2699650049209597\n",
            "Epoch: 1460, Train Loss: 0.8054, Test Loss: 3.273743915557861\n",
            "Epoch: 1461, Train Loss: 0.8724, Test Loss: 3.2836698055267335\n",
            "Epoch: 1462, Train Loss: 0.8576, Test Loss: 3.294079613685608\n",
            "Epoch: 1463, Train Loss: 0.7988, Test Loss: 3.296347975730896\n",
            "Epoch: 1464, Train Loss: 0.8267, Test Loss: 3.321625542640686\n",
            "Epoch: 1465, Train Loss: 0.8330, Test Loss: 3.2844127655029296\n",
            "Epoch: 1466, Train Loss: 0.7974, Test Loss: 3.29621422290802\n",
            "Epoch: 1467, Train Loss: 0.8489, Test Loss: 3.34946072101593\n",
            "Epoch: 1468, Train Loss: 0.8668, Test Loss: 3.27283091545105\n",
            "Epoch: 1469, Train Loss: 0.8156, Test Loss: 3.3022396326065064\n",
            "Epoch: 1470, Train Loss: 0.7999, Test Loss: 3.2958828687667845\n",
            "Epoch: 1471, Train Loss: 0.8597, Test Loss: 3.2796213150024416\n",
            "Epoch: 1472, Train Loss: 0.7922, Test Loss: 3.281247067451477\n",
            "Epoch: 1473, Train Loss: 0.8186, Test Loss: 3.300329303741455\n",
            "Epoch: 1474, Train Loss: 0.8026, Test Loss: 3.339178442955017\n",
            "Epoch: 1475, Train Loss: 0.7705, Test Loss: 3.2724345922470093\n",
            "Epoch: 1476, Train Loss: 0.8282, Test Loss: 3.2789511919021606\n",
            "Epoch: 1477, Train Loss: 0.8500, Test Loss: 3.2789291858673097\n",
            "Epoch: 1478, Train Loss: 0.8290, Test Loss: 3.2652866363525392\n",
            "Epoch: 1479, Train Loss: 0.8489, Test Loss: 3.271549129486084\n",
            "Epoch: 1480, Train Loss: 0.7988, Test Loss: 3.2718759536743165\n",
            "Epoch: 1481, Train Loss: 0.8329, Test Loss: 3.278420090675354\n",
            "Epoch: 1482, Train Loss: 0.8177, Test Loss: 3.278139281272888\n",
            "Epoch: 1483, Train Loss: 0.8511, Test Loss: 3.2688294649124146\n",
            "Epoch: 1484, Train Loss: 0.7790, Test Loss: 3.2575796365737917\n",
            "Epoch: 1485, Train Loss: 0.7763, Test Loss: 3.2851062774658204\n",
            "Epoch: 1486, Train Loss: 0.8175, Test Loss: 3.2958181858062745\n",
            "Epoch: 1487, Train Loss: 0.8804, Test Loss: 3.3214362144470213\n",
            "Epoch: 1488, Train Loss: 0.7902, Test Loss: 3.3231203079223635\n",
            "Epoch: 1489, Train Loss: 0.7764, Test Loss: 3.295643854141235\n",
            "Epoch: 1490, Train Loss: 0.8431, Test Loss: 3.272761917114258\n",
            "Epoch: 1491, Train Loss: 0.8159, Test Loss: 3.2817060232162474\n",
            "Epoch: 1492, Train Loss: 0.8058, Test Loss: 3.286365842819214\n",
            "Epoch: 1493, Train Loss: 0.8286, Test Loss: 3.3060772895812987\n",
            "Epoch: 1494, Train Loss: 0.8431, Test Loss: 3.2779802560806273\n",
            "Epoch: 1495, Train Loss: 0.7865, Test Loss: 3.293224048614502\n",
            "Epoch: 1496, Train Loss: 0.8219, Test Loss: 3.2897844552993774\n",
            "Epoch: 1497, Train Loss: 0.8036, Test Loss: 3.2927032947540282\n",
            "Epoch: 1498, Train Loss: 0.9054, Test Loss: 3.2951666116714478\n",
            "Epoch: 1499, Train Loss: 0.8251, Test Loss: 3.3189679861068724\n",
            "Epoch: 1500, Train Loss: 0.7776, Test Loss: 3.290825080871582\n",
            "Epoch: 1501, Train Loss: 0.8508, Test Loss: 3.2850256681442263\n",
            "Epoch: 1502, Train Loss: 0.8025, Test Loss: 3.3107104778289793\n",
            "Epoch: 1503, Train Loss: 0.8309, Test Loss: 3.287711334228516\n",
            "Epoch: 1504, Train Loss: 0.8086, Test Loss: 3.278629016876221\n",
            "Epoch: 1505, Train Loss: 0.8270, Test Loss: 3.295520806312561\n",
            "Epoch: 1506, Train Loss: 0.8602, Test Loss: 3.272523045539856\n",
            "Epoch: 1507, Train Loss: 0.7770, Test Loss: 3.296239948272705\n",
            "Epoch: 1508, Train Loss: 0.8031, Test Loss: 3.312226891517639\n",
            "Epoch: 1509, Train Loss: 0.8049, Test Loss: 3.3075210571289064\n",
            "Epoch: 1510, Train Loss: 0.9079, Test Loss: 3.256559872627258\n",
            "Epoch: 1511, Train Loss: 0.8367, Test Loss: 3.284803867340088\n",
            "Epoch: 1512, Train Loss: 0.7934, Test Loss: 3.27515230178833\n",
            "Epoch: 1513, Train Loss: 0.8337, Test Loss: 3.2762366056442263\n",
            "Epoch: 1514, Train Loss: 0.8393, Test Loss: 3.2835683107376097\n",
            "Epoch: 1515, Train Loss: 0.8141, Test Loss: 3.272457551956177\n",
            "Epoch: 1516, Train Loss: 0.8699, Test Loss: 3.2647950649261475\n",
            "Epoch: 1517, Train Loss: 0.7669, Test Loss: 3.296284794807434\n",
            "Epoch: 1518, Train Loss: 0.8156, Test Loss: 3.2886339902877806\n",
            "Epoch: 1519, Train Loss: 0.8105, Test Loss: 3.2636088132858276\n",
            "Epoch: 1520, Train Loss: 0.7942, Test Loss: 3.2658272981643677\n",
            "Epoch: 1521, Train Loss: 0.7952, Test Loss: 3.2916019916534425\n",
            "Epoch: 1522, Train Loss: 0.7926, Test Loss: 3.2920871019363402\n",
            "Epoch: 1523, Train Loss: 0.8304, Test Loss: 3.3306298732757567\n",
            "Epoch: 1524, Train Loss: 0.7929, Test Loss: 3.2789069414138794\n",
            "Epoch: 1525, Train Loss: 0.8431, Test Loss: 3.2906906604766846\n",
            "Epoch: 1526, Train Loss: 0.9153, Test Loss: 3.295078253746033\n",
            "Epoch: 1527, Train Loss: 0.8551, Test Loss: 3.3071522235870363\n",
            "Epoch: 1528, Train Loss: 0.8142, Test Loss: 3.2971126556396486\n",
            "Epoch: 1529, Train Loss: 0.8513, Test Loss: 3.2839678525924683\n",
            "Epoch: 1530, Train Loss: 0.8344, Test Loss: 3.278115749359131\n",
            "Epoch: 1531, Train Loss: 0.8067, Test Loss: 3.2941903114318847\n",
            "Epoch: 1532, Train Loss: 0.8242, Test Loss: 3.3366404294967653\n",
            "Epoch: 1533, Train Loss: 0.8833, Test Loss: 3.2706061601638794\n",
            "Epoch: 1534, Train Loss: 0.8057, Test Loss: 3.282781219482422\n",
            "Epoch: 1535, Train Loss: 0.8545, Test Loss: 3.30633282661438\n",
            "Epoch: 1536, Train Loss: 0.8704, Test Loss: 3.2846493482589723\n",
            "Epoch: 1537, Train Loss: 0.7811, Test Loss: 3.308599662780762\n",
            "Epoch: 1538, Train Loss: 0.9096, Test Loss: 3.273164415359497\n",
            "Epoch: 1539, Train Loss: 0.7637, Test Loss: 3.271568751335144\n",
            "Epoch: 1540, Train Loss: 0.8601, Test Loss: 3.27333517074585\n",
            "Epoch: 1541, Train Loss: 0.8712, Test Loss: 3.2824642419815064\n",
            "Epoch: 1542, Train Loss: 0.8083, Test Loss: 3.279710555076599\n",
            "Epoch: 1543, Train Loss: 0.8047, Test Loss: 3.2938454389572143\n",
            "Epoch: 1544, Train Loss: 0.7813, Test Loss: 3.2862351894378663\n",
            "Epoch: 1545, Train Loss: 0.8617, Test Loss: 3.3165566682815553\n",
            "Epoch: 1546, Train Loss: 0.8944, Test Loss: 3.300346302986145\n",
            "Epoch: 1547, Train Loss: 0.8537, Test Loss: 3.273972821235657\n",
            "Epoch: 1548, Train Loss: 0.8561, Test Loss: 3.2809120655059814\n",
            "Epoch: 1549, Train Loss: 0.8042, Test Loss: 3.2797932386398316\n",
            "Epoch: 1550, Train Loss: 0.8977, Test Loss: 3.270142388343811\n",
            "Epoch: 1551, Train Loss: 0.8687, Test Loss: 3.2688152313232424\n",
            "Epoch: 1552, Train Loss: 0.8132, Test Loss: 3.2709572076797486\n",
            "Epoch: 1553, Train Loss: 0.8362, Test Loss: 3.2660641431808473\n",
            "Epoch: 1554, Train Loss: 0.7931, Test Loss: 3.2617682933807375\n",
            "Epoch: 1555, Train Loss: 0.7871, Test Loss: 3.2811633825302122\n",
            "Epoch: 1556, Train Loss: 0.8211, Test Loss: 3.290664553642273\n",
            "Epoch: 1557, Train Loss: 0.8264, Test Loss: 3.256583309173584\n",
            "Epoch: 1558, Train Loss: 0.7837, Test Loss: 3.297885799407959\n",
            "Epoch: 1559, Train Loss: 0.8412, Test Loss: 3.291898584365845\n",
            "Epoch: 1560, Train Loss: 0.7975, Test Loss: 3.280087184906006\n",
            "Epoch: 1561, Train Loss: 0.8915, Test Loss: 3.312032675743103\n",
            "Epoch: 1562, Train Loss: 0.8084, Test Loss: 3.2867960214614866\n",
            "Epoch: 1563, Train Loss: 0.8368, Test Loss: 3.329107975959778\n",
            "Epoch: 1564, Train Loss: 0.8084, Test Loss: 3.2878923654556274\n",
            "Epoch: 1565, Train Loss: 0.8540, Test Loss: 3.3304697275161743\n",
            "Epoch: 1566, Train Loss: 0.8002, Test Loss: 3.292924332618713\n",
            "Epoch: 1567, Train Loss: 0.8892, Test Loss: 3.312998867034912\n",
            "Epoch: 1568, Train Loss: 0.8717, Test Loss: 3.249015212059021\n",
            "Epoch: 1569, Train Loss: 0.8872, Test Loss: 3.2766178131103514\n",
            "Epoch: 1570, Train Loss: 0.8574, Test Loss: 3.2684982299804686\n",
            "Epoch: 1571, Train Loss: 0.8223, Test Loss: 3.3148950338363647\n",
            "Epoch: 1572, Train Loss: 0.7734, Test Loss: 3.304493451118469\n",
            "Epoch: 1573, Train Loss: 0.7403, Test Loss: 3.275260806083679\n",
            "Epoch: 1574, Train Loss: 0.7999, Test Loss: 3.29023756980896\n",
            "Epoch: 1575, Train Loss: 0.8214, Test Loss: 3.3039782762527468\n",
            "Epoch: 1576, Train Loss: 0.8012, Test Loss: 3.29928240776062\n",
            "Epoch: 1577, Train Loss: 0.8477, Test Loss: 3.2805115938186646\n",
            "Epoch: 1578, Train Loss: 0.7999, Test Loss: 3.2940369844436646\n",
            "Epoch: 1579, Train Loss: 0.7983, Test Loss: 3.283638072013855\n",
            "Epoch: 1580, Train Loss: 0.8179, Test Loss: 3.278519034385681\n",
            "Epoch: 1581, Train Loss: 0.8259, Test Loss: 3.279871964454651\n",
            "Epoch: 1582, Train Loss: 0.8499, Test Loss: 3.2955551862716677\n",
            "Epoch: 1583, Train Loss: 0.7886, Test Loss: 3.2601138591766357\n",
            "Epoch: 1584, Train Loss: 0.8153, Test Loss: 3.2722169876098635\n",
            "Epoch: 1585, Train Loss: 0.8025, Test Loss: 3.2873147249221804\n",
            "Epoch: 1586, Train Loss: 0.8786, Test Loss: 3.309586763381958\n",
            "Epoch: 1587, Train Loss: 0.9119, Test Loss: 3.258600854873657\n",
            "Epoch: 1588, Train Loss: 0.8563, Test Loss: 3.244602584838867\n",
            "Epoch: 1589, Train Loss: 0.7822, Test Loss: 3.259783983230591\n",
            "Epoch: 1590, Train Loss: 0.7857, Test Loss: 3.279293084144592\n",
            "Epoch: 1591, Train Loss: 0.7927, Test Loss: 3.2954015731811523\n",
            "Epoch: 1592, Train Loss: 0.8274, Test Loss: 3.302881360054016\n",
            "Epoch: 1593, Train Loss: 0.7486, Test Loss: 3.283150386810303\n",
            "Epoch: 1594, Train Loss: 0.7777, Test Loss: 3.272046995162964\n",
            "Epoch: 1595, Train Loss: 0.8015, Test Loss: 3.2924007415771483\n",
            "Epoch: 1596, Train Loss: 0.8017, Test Loss: 3.313787341117859\n",
            "Epoch: 1597, Train Loss: 0.8271, Test Loss: 3.292328381538391\n",
            "Epoch: 1598, Train Loss: 0.8097, Test Loss: 3.27519371509552\n",
            "Epoch: 1599, Train Loss: 0.8038, Test Loss: 3.3033117294311523\n",
            "Epoch: 1600, Train Loss: 0.8059, Test Loss: 3.3289007902145387\n",
            "Epoch: 1601, Train Loss: 0.7757, Test Loss: 3.3008567571640013\n",
            "Epoch: 1602, Train Loss: 0.8464, Test Loss: 3.301179122924805\n",
            "Epoch: 1603, Train Loss: 0.7963, Test Loss: 3.3045829057693483\n",
            "Epoch: 1604, Train Loss: 0.8374, Test Loss: 3.28740234375\n",
            "Epoch: 1605, Train Loss: 0.8248, Test Loss: 3.28771333694458\n",
            "Epoch: 1606, Train Loss: 0.8116, Test Loss: 3.2897918939590456\n",
            "Epoch: 1607, Train Loss: 0.8298, Test Loss: 3.2864267110824583\n",
            "Epoch: 1608, Train Loss: 0.7901, Test Loss: 3.295633840560913\n",
            "Epoch: 1609, Train Loss: 0.7565, Test Loss: 3.2788117647171022\n",
            "Epoch: 1610, Train Loss: 0.7937, Test Loss: 3.277590203285217\n",
            "Epoch: 1611, Train Loss: 0.7582, Test Loss: 3.2737451791763306\n",
            "Epoch: 1612, Train Loss: 0.8359, Test Loss: 3.2969970703125\n",
            "Epoch: 1613, Train Loss: 0.7982, Test Loss: 3.278528332710266\n",
            "Epoch: 1614, Train Loss: 0.8600, Test Loss: 3.3187587022781373\n",
            "Epoch: 1615, Train Loss: 0.8191, Test Loss: 3.29620943069458\n",
            "Epoch: 1616, Train Loss: 0.8112, Test Loss: 3.2805702447891236\n",
            "Epoch: 1617, Train Loss: 0.8304, Test Loss: 3.2893247842788695\n",
            "Epoch: 1618, Train Loss: 0.8630, Test Loss: 3.287265658378601\n",
            "Epoch: 1619, Train Loss: 0.8171, Test Loss: 3.2915607929229735\n",
            "Epoch: 1620, Train Loss: 0.7905, Test Loss: 3.2927267789840697\n",
            "Epoch: 1621, Train Loss: 0.7709, Test Loss: 3.3341378927230836\n",
            "Epoch: 1622, Train Loss: 0.7947, Test Loss: 3.2710095405578614\n",
            "Epoch: 1623, Train Loss: 0.8715, Test Loss: 3.2719281673431397\n",
            "Epoch: 1624, Train Loss: 0.7973, Test Loss: 3.2708144664764403\n",
            "Epoch: 1625, Train Loss: 0.7818, Test Loss: 3.27647225856781\n",
            "Epoch: 1626, Train Loss: 0.7762, Test Loss: 3.2812983989715576\n",
            "Epoch: 1627, Train Loss: 0.8071, Test Loss: 3.298417663574219\n",
            "Epoch: 1628, Train Loss: 0.8529, Test Loss: 3.28294780254364\n",
            "Epoch: 1629, Train Loss: 0.8650, Test Loss: 3.297366404533386\n",
            "Epoch: 1630, Train Loss: 0.8134, Test Loss: 3.278715658187866\n",
            "Epoch: 1631, Train Loss: 0.8269, Test Loss: 3.2944876670837404\n",
            "Epoch: 1632, Train Loss: 0.8654, Test Loss: 3.31764235496521\n",
            "Epoch: 1633, Train Loss: 0.8192, Test Loss: 3.2955818653106688\n",
            "Epoch: 1634, Train Loss: 0.8214, Test Loss: 3.288954019546509\n",
            "Epoch: 1635, Train Loss: 0.7705, Test Loss: 3.2947690963745115\n",
            "Epoch: 1636, Train Loss: 0.7994, Test Loss: 3.295713210105896\n",
            "Epoch: 1637, Train Loss: 0.8207, Test Loss: 3.332615828514099\n",
            "Epoch: 1638, Train Loss: 0.7876, Test Loss: 3.271790361404419\n",
            "Epoch: 1639, Train Loss: 0.8599, Test Loss: 3.2969958782196045\n",
            "Epoch: 1640, Train Loss: 0.8357, Test Loss: 3.2974785804748534\n",
            "Epoch: 1641, Train Loss: 0.8103, Test Loss: 3.2957573413848875\n",
            "Epoch: 1642, Train Loss: 0.7793, Test Loss: 3.306241035461426\n",
            "Epoch: 1643, Train Loss: 0.8783, Test Loss: 3.3051054000854494\n",
            "Epoch: 1644, Train Loss: 0.9005, Test Loss: 3.2918105602264403\n",
            "Epoch: 1645, Train Loss: 0.8367, Test Loss: 3.296493315696716\n",
            "Epoch: 1646, Train Loss: 0.7892, Test Loss: 3.290877866744995\n",
            "Epoch: 1647, Train Loss: 0.8073, Test Loss: 3.289592456817627\n",
            "Epoch: 1648, Train Loss: 0.8392, Test Loss: 3.2744136095046996\n",
            "Epoch: 1649, Train Loss: 0.7553, Test Loss: 3.2998775005340577\n",
            "Epoch: 1650, Train Loss: 0.8424, Test Loss: 3.287115979194641\n",
            "Epoch: 1651, Train Loss: 0.8267, Test Loss: 3.280850863456726\n",
            "Epoch: 1652, Train Loss: 0.8198, Test Loss: 3.2735600233078004\n",
            "Epoch: 1653, Train Loss: 0.8639, Test Loss: 3.269951009750366\n",
            "Epoch: 1654, Train Loss: 0.8172, Test Loss: 3.279351902008057\n",
            "Epoch: 1655, Train Loss: 0.9298, Test Loss: 3.2765787839889526\n",
            "Epoch: 1656, Train Loss: 0.8352, Test Loss: 3.280737853050232\n",
            "Epoch: 1657, Train Loss: 0.8259, Test Loss: 3.2914325475692747\n",
            "Epoch: 1658, Train Loss: 0.8051, Test Loss: 3.280164623260498\n",
            "Epoch: 1659, Train Loss: 0.8154, Test Loss: 3.274565815925598\n",
            "Epoch: 1660, Train Loss: 0.8412, Test Loss: 3.2781741619110107\n",
            "Epoch: 1661, Train Loss: 0.7461, Test Loss: 3.2928304195404055\n",
            "Epoch: 1662, Train Loss: 0.8238, Test Loss: 3.315790367126465\n",
            "Epoch: 1663, Train Loss: 0.8262, Test Loss: 3.3010233640670776\n",
            "Epoch: 1664, Train Loss: 0.7624, Test Loss: 3.288099837303162\n",
            "Epoch: 1665, Train Loss: 0.8294, Test Loss: 3.283635449409485\n",
            "Epoch: 1666, Train Loss: 0.7859, Test Loss: 3.2754283905029298\n",
            "Epoch: 1667, Train Loss: 0.8588, Test Loss: 3.291176462173462\n",
            "Epoch: 1668, Train Loss: 0.7536, Test Loss: 3.2726950883865356\n",
            "Epoch: 1669, Train Loss: 0.8570, Test Loss: 3.3490362644195555\n",
            "Epoch: 1670, Train Loss: 0.8353, Test Loss: 3.29685697555542\n",
            "Epoch: 1671, Train Loss: 0.9069, Test Loss: 3.286670517921448\n",
            "Epoch: 1672, Train Loss: 0.7616, Test Loss: 3.2956261157989504\n",
            "Epoch: 1673, Train Loss: 0.7483, Test Loss: 3.287892293930054\n",
            "Epoch: 1674, Train Loss: 0.7875, Test Loss: 3.307546091079712\n",
            "Epoch: 1675, Train Loss: 0.7907, Test Loss: 3.2871838092803953\n",
            "Epoch: 1676, Train Loss: 0.8554, Test Loss: 3.2778069734573365\n",
            "Epoch: 1677, Train Loss: 0.8291, Test Loss: 3.287644362449646\n",
            "Epoch: 1678, Train Loss: 0.8467, Test Loss: 3.260622334480286\n",
            "Epoch: 1679, Train Loss: 0.7862, Test Loss: 3.315794897079468\n",
            "Epoch: 1680, Train Loss: 0.7858, Test Loss: 3.29146990776062\n",
            "Epoch: 1681, Train Loss: 0.8348, Test Loss: 3.3155508756637575\n",
            "Epoch: 1682, Train Loss: 0.7829, Test Loss: 3.311277961730957\n",
            "Epoch: 1683, Train Loss: 0.7916, Test Loss: 3.285668134689331\n",
            "Epoch: 1684, Train Loss: 0.7772, Test Loss: 3.2797806739807127\n",
            "Epoch: 1685, Train Loss: 0.8130, Test Loss: 3.2772237062454224\n",
            "Epoch: 1686, Train Loss: 0.8073, Test Loss: 3.2656818628311157\n",
            "Epoch: 1687, Train Loss: 0.8271, Test Loss: 3.285099411010742\n",
            "Epoch: 1688, Train Loss: 0.7670, Test Loss: 3.2962902307510378\n",
            "Epoch: 1689, Train Loss: 0.8014, Test Loss: 3.290053939819336\n",
            "Epoch: 1690, Train Loss: 0.8560, Test Loss: 3.2992169857025146\n",
            "Epoch: 1691, Train Loss: 0.7865, Test Loss: 3.2955739974975584\n",
            "Epoch: 1692, Train Loss: 0.7846, Test Loss: 3.307091760635376\n",
            "Epoch: 1693, Train Loss: 0.8413, Test Loss: 3.3018776178359985\n",
            "Epoch: 1694, Train Loss: 0.8176, Test Loss: 3.2965806245803835\n",
            "Epoch: 1695, Train Loss: 0.8956, Test Loss: 3.3300714015960695\n",
            "Epoch: 1696, Train Loss: 0.8357, Test Loss: 3.2921567440032957\n",
            "Epoch: 1697, Train Loss: 0.7764, Test Loss: 3.2941341161727906\n",
            "Epoch: 1698, Train Loss: 0.7949, Test Loss: 3.287878179550171\n",
            "Epoch: 1699, Train Loss: 0.7783, Test Loss: 3.2965826988220215\n",
            "Epoch: 1700, Train Loss: 0.8324, Test Loss: 3.288708710670471\n",
            "Epoch: 1701, Train Loss: 0.7667, Test Loss: 3.2970433473587035\n",
            "Epoch: 1702, Train Loss: 0.8216, Test Loss: 3.2703760862350464\n",
            "Epoch: 1703, Train Loss: 0.7822, Test Loss: 3.3034839630126953\n",
            "Epoch: 1704, Train Loss: 0.7637, Test Loss: 3.3000297069549562\n",
            "Epoch: 1705, Train Loss: 0.7903, Test Loss: 3.299064373970032\n",
            "Epoch: 1706, Train Loss: 0.8272, Test Loss: 3.2748644828796385\n",
            "Epoch: 1707, Train Loss: 0.7856, Test Loss: 3.289656400680542\n",
            "Epoch: 1708, Train Loss: 0.7923, Test Loss: 3.3073693037033083\n",
            "Epoch: 1709, Train Loss: 0.8000, Test Loss: 3.293072509765625\n",
            "Epoch: 1710, Train Loss: 0.8200, Test Loss: 3.3366567373275755\n",
            "Epoch: 1711, Train Loss: 0.8529, Test Loss: 3.290334963798523\n",
            "Epoch: 1712, Train Loss: 0.7901, Test Loss: 3.2965238809585573\n",
            "Epoch: 1713, Train Loss: 0.8190, Test Loss: 3.2823163986206056\n",
            "Epoch: 1714, Train Loss: 0.7885, Test Loss: 3.274630546569824\n",
            "Epoch: 1715, Train Loss: 0.7942, Test Loss: 3.282139253616333\n",
            "Epoch: 1716, Train Loss: 0.8210, Test Loss: 3.2837817668914795\n",
            "Epoch: 1717, Train Loss: 0.7774, Test Loss: 3.2787344694137572\n",
            "Epoch: 1718, Train Loss: 0.7914, Test Loss: 3.2817774534225466\n",
            "Epoch: 1719, Train Loss: 0.8031, Test Loss: 3.2912060737609865\n",
            "Epoch: 1720, Train Loss: 0.7850, Test Loss: 3.2854974985122682\n",
            "Epoch: 1721, Train Loss: 0.8474, Test Loss: 3.336857557296753\n",
            "Epoch: 1722, Train Loss: 0.8454, Test Loss: 3.3085116863250734\n",
            "Epoch: 1723, Train Loss: 0.8501, Test Loss: 3.308660459518433\n",
            "Epoch: 1724, Train Loss: 0.8331, Test Loss: 3.3025095462799072\n",
            "Epoch: 1725, Train Loss: 0.8226, Test Loss: 3.291857147216797\n",
            "Epoch: 1726, Train Loss: 0.7406, Test Loss: 3.2936898946762083\n",
            "Epoch: 1727, Train Loss: 0.8758, Test Loss: 3.2825925588607787\n",
            "Epoch: 1728, Train Loss: 0.8218, Test Loss: 3.275358033180237\n",
            "Epoch: 1729, Train Loss: 0.8012, Test Loss: 3.2973142147064207\n",
            "Epoch: 1730, Train Loss: 0.8194, Test Loss: 3.3018877267837525\n",
            "Epoch: 1731, Train Loss: 0.8174, Test Loss: 3.248175311088562\n",
            "Epoch: 1732, Train Loss: 0.8001, Test Loss: 3.2783260107040406\n",
            "Epoch: 1733, Train Loss: 0.8313, Test Loss: 3.272936224937439\n",
            "Epoch: 1734, Train Loss: 0.7829, Test Loss: 3.280479598045349\n",
            "Epoch: 1735, Train Loss: 0.7849, Test Loss: 3.2776307582855226\n",
            "Epoch: 1736, Train Loss: 0.8841, Test Loss: 3.2702019929885866\n",
            "Epoch: 1737, Train Loss: 0.8122, Test Loss: 3.287736654281616\n",
            "Epoch: 1738, Train Loss: 0.8518, Test Loss: 3.3143484830856322\n",
            "Epoch: 1739, Train Loss: 0.7791, Test Loss: 3.2694926261901855\n",
            "Epoch: 1740, Train Loss: 0.7618, Test Loss: 3.2823550701141357\n",
            "Epoch: 1741, Train Loss: 0.8667, Test Loss: 3.2875596046447755\n",
            "Epoch: 1742, Train Loss: 0.7831, Test Loss: 3.287716603279114\n",
            "Epoch: 1743, Train Loss: 0.7856, Test Loss: 3.27812397480011\n",
            "Epoch: 1744, Train Loss: 0.7800, Test Loss: 3.302417349815369\n",
            "Epoch: 1745, Train Loss: 0.8336, Test Loss: 3.272721028327942\n",
            "Epoch: 1746, Train Loss: 0.7842, Test Loss: 3.290925121307373\n",
            "Epoch: 1747, Train Loss: 0.7666, Test Loss: 3.279895710945129\n",
            "Epoch: 1748, Train Loss: 0.8083, Test Loss: 3.2889667749404907\n",
            "Epoch: 1749, Train Loss: 0.7660, Test Loss: 3.26930251121521\n",
            "Epoch: 1750, Train Loss: 0.8042, Test Loss: 3.28688588142395\n",
            "Epoch: 1751, Train Loss: 0.8068, Test Loss: 3.2697722196578978\n",
            "Epoch: 1752, Train Loss: 0.7914, Test Loss: 3.2709707736968996\n",
            "Epoch: 1753, Train Loss: 0.7920, Test Loss: 3.286846375465393\n",
            "Epoch: 1754, Train Loss: 0.8280, Test Loss: 3.293998050689697\n",
            "Epoch: 1755, Train Loss: 0.7724, Test Loss: 3.2743247509002686\n",
            "Epoch: 1756, Train Loss: 0.7909, Test Loss: 3.288886618614197\n",
            "Epoch: 1757, Train Loss: 0.7727, Test Loss: 3.3160849809646606\n",
            "Epoch: 1758, Train Loss: 0.7876, Test Loss: 3.2754068851470945\n",
            "Epoch: 1759, Train Loss: 0.7950, Test Loss: 3.288070034980774\n",
            "Epoch: 1760, Train Loss: 0.7208, Test Loss: 3.2827765464782717\n",
            "Epoch: 1761, Train Loss: 0.8448, Test Loss: 3.3199312925338744\n",
            "Epoch: 1762, Train Loss: 0.8271, Test Loss: 3.2852842807769775\n",
            "Epoch: 1763, Train Loss: 0.7981, Test Loss: 3.2852856874465943\n",
            "Epoch: 1764, Train Loss: 0.7671, Test Loss: 3.2784848690032957\n",
            "Epoch: 1765, Train Loss: 0.7771, Test Loss: 3.2953840255737306\n",
            "Epoch: 1766, Train Loss: 0.8131, Test Loss: 3.3146644115447996\n",
            "Epoch: 1767, Train Loss: 0.7532, Test Loss: 3.3166301250457764\n",
            "Epoch: 1768, Train Loss: 0.8335, Test Loss: 3.2900609016418456\n",
            "Epoch: 1769, Train Loss: 0.7539, Test Loss: 3.2824084520339967\n",
            "Epoch: 1770, Train Loss: 0.7784, Test Loss: 3.282336139678955\n",
            "Epoch: 1771, Train Loss: 0.8318, Test Loss: 3.265841341018677\n",
            "Epoch: 1772, Train Loss: 0.7800, Test Loss: 3.2955071210861204\n",
            "Epoch: 1773, Train Loss: 0.7768, Test Loss: 3.2729117631912232\n",
            "Epoch: 1774, Train Loss: 0.8824, Test Loss: 3.3040756225585937\n",
            "Epoch: 1775, Train Loss: 0.8553, Test Loss: 3.340360713005066\n",
            "Epoch: 1776, Train Loss: 0.8343, Test Loss: 3.3103579759597777\n",
            "Epoch: 1777, Train Loss: 0.7866, Test Loss: 3.325814628601074\n",
            "Epoch: 1778, Train Loss: 0.8862, Test Loss: 3.2865744829177856\n",
            "Epoch: 1779, Train Loss: 0.7799, Test Loss: 3.2972037553787232\n",
            "Epoch: 1780, Train Loss: 0.8802, Test Loss: 3.266528844833374\n",
            "Epoch: 1781, Train Loss: 0.8136, Test Loss: 3.2815083026885987\n",
            "Epoch: 1782, Train Loss: 0.8081, Test Loss: 3.2906712770462034\n",
            "Epoch: 1783, Train Loss: 0.8102, Test Loss: 3.27269446849823\n",
            "Epoch: 1784, Train Loss: 0.7767, Test Loss: 3.277802896499634\n",
            "Epoch: 1785, Train Loss: 0.7994, Test Loss: 3.284827375411987\n",
            "Epoch: 1786, Train Loss: 0.8656, Test Loss: 3.3059973239898683\n",
            "Epoch: 1787, Train Loss: 0.8555, Test Loss: 3.2743383407592774\n",
            "Epoch: 1788, Train Loss: 0.7926, Test Loss: 3.332508182525635\n",
            "Epoch: 1789, Train Loss: 0.8346, Test Loss: 3.286535859107971\n",
            "Epoch: 1790, Train Loss: 0.8504, Test Loss: 3.294413185119629\n",
            "Epoch: 1791, Train Loss: 0.7766, Test Loss: 3.2745201110839846\n",
            "Epoch: 1792, Train Loss: 0.7539, Test Loss: 3.284785270690918\n",
            "Epoch: 1793, Train Loss: 0.7969, Test Loss: 3.2643826246261596\n",
            "Epoch: 1794, Train Loss: 0.7837, Test Loss: 3.2733996629714968\n",
            "Epoch: 1795, Train Loss: 0.8052, Test Loss: 3.3260122776031493\n",
            "Epoch: 1796, Train Loss: 0.7886, Test Loss: 3.2686968564987184\n",
            "Epoch: 1797, Train Loss: 0.8012, Test Loss: 3.265543746948242\n",
            "Epoch: 1798, Train Loss: 0.7781, Test Loss: 3.2849965333938598\n",
            "Epoch: 1799, Train Loss: 0.7912, Test Loss: 3.2957547903060913\n",
            "Epoch: 1800, Train Loss: 0.7720, Test Loss: 3.284713864326477\n",
            "Epoch: 1801, Train Loss: 0.8442, Test Loss: 3.320867967605591\n",
            "Epoch: 1802, Train Loss: 0.8700, Test Loss: 3.2591922283172607\n",
            "Epoch: 1803, Train Loss: 0.7979, Test Loss: 3.273528003692627\n",
            "Epoch: 1804, Train Loss: 0.7672, Test Loss: 3.2998167514801025\n",
            "Epoch: 1805, Train Loss: 0.7863, Test Loss: 3.283944916725159\n",
            "Epoch: 1806, Train Loss: 0.7617, Test Loss: 3.3101761817932127\n",
            "Epoch: 1807, Train Loss: 0.7610, Test Loss: 3.2888239860534667\n",
            "Epoch: 1808, Train Loss: 0.8936, Test Loss: 3.273302984237671\n",
            "Epoch: 1809, Train Loss: 0.8271, Test Loss: 3.2968034505844117\n",
            "Epoch: 1810, Train Loss: 0.8803, Test Loss: 3.2645715951919554\n",
            "Epoch: 1811, Train Loss: 0.8287, Test Loss: 3.340154695510864\n",
            "Epoch: 1812, Train Loss: 0.8643, Test Loss: 3.3787403106689453\n",
            "Epoch: 1813, Train Loss: 0.8618, Test Loss: 3.3240675926208496\n",
            "Epoch: 1814, Train Loss: 0.8315, Test Loss: 3.292223501205444\n",
            "Epoch: 1815, Train Loss: 0.8027, Test Loss: 3.292834687232971\n",
            "Epoch: 1816, Train Loss: 0.7781, Test Loss: 3.28283953666687\n",
            "Epoch: 1817, Train Loss: 0.8636, Test Loss: 3.27185173034668\n",
            "Epoch: 1818, Train Loss: 0.8424, Test Loss: 3.288968062400818\n",
            "Epoch: 1819, Train Loss: 0.7664, Test Loss: 3.2801387786865233\n",
            "Epoch: 1820, Train Loss: 0.8004, Test Loss: 3.2781405687332152\n",
            "Epoch: 1821, Train Loss: 0.7808, Test Loss: 3.295112895965576\n",
            "Epoch: 1822, Train Loss: 0.8282, Test Loss: 3.275922703742981\n",
            "Epoch: 1823, Train Loss: 0.8225, Test Loss: 3.2765113592147825\n",
            "Epoch: 1824, Train Loss: 0.8349, Test Loss: 3.276476740837097\n",
            "Epoch: 1825, Train Loss: 0.7684, Test Loss: 3.2774339437484743\n",
            "Epoch: 1826, Train Loss: 0.8609, Test Loss: 3.2672200202941895\n",
            "Epoch: 1827, Train Loss: 0.8063, Test Loss: 3.3051498174667358\n",
            "Epoch: 1828, Train Loss: 0.8009, Test Loss: 3.30311324596405\n",
            "Epoch: 1829, Train Loss: 0.7913, Test Loss: 3.272372078895569\n",
            "Epoch: 1830, Train Loss: 0.9213, Test Loss: 3.265556049346924\n",
            "Epoch: 1831, Train Loss: 0.7752, Test Loss: 3.273759603500366\n",
            "Epoch: 1832, Train Loss: 0.8577, Test Loss: 3.3087769508361817\n",
            "Epoch: 1833, Train Loss: 0.8119, Test Loss: 3.2861351490020754\n",
            "Epoch: 1834, Train Loss: 0.7527, Test Loss: 3.276334524154663\n",
            "Epoch: 1835, Train Loss: 0.8147, Test Loss: 3.2853362560272217\n",
            "Epoch: 1836, Train Loss: 0.8222, Test Loss: 3.2969682455062865\n",
            "Epoch: 1837, Train Loss: 0.8015, Test Loss: 3.2891489028930665\n",
            "Epoch: 1838, Train Loss: 0.7554, Test Loss: 3.268844795227051\n",
            "Epoch: 1839, Train Loss: 0.8848, Test Loss: 3.3191688299179076\n",
            "Epoch: 1840, Train Loss: 0.8367, Test Loss: 3.330641984939575\n",
            "Epoch: 1841, Train Loss: 0.8591, Test Loss: 3.288832402229309\n",
            "Epoch: 1842, Train Loss: 0.8435, Test Loss: 3.3151218414306642\n",
            "Epoch: 1843, Train Loss: 0.7804, Test Loss: 3.2985900402069093\n",
            "Epoch: 1844, Train Loss: 0.7395, Test Loss: 3.279714894294739\n",
            "Epoch: 1845, Train Loss: 0.7928, Test Loss: 3.276644396781921\n",
            "Epoch: 1846, Train Loss: 0.8108, Test Loss: 3.281897497177124\n",
            "Epoch: 1847, Train Loss: 0.7977, Test Loss: 3.28750159740448\n",
            "Epoch: 1848, Train Loss: 0.8556, Test Loss: 3.284068059921265\n",
            "Epoch: 1849, Train Loss: 0.7945, Test Loss: 3.266068458557129\n",
            "Epoch: 1850, Train Loss: 0.8231, Test Loss: 3.2738874912261964\n",
            "Epoch: 1851, Train Loss: 0.7843, Test Loss: 3.2924222469329836\n",
            "Epoch: 1852, Train Loss: 0.7984, Test Loss: 3.3207363843917848\n",
            "Epoch: 1853, Train Loss: 0.8582, Test Loss: 3.3079164743423464\n",
            "Epoch: 1854, Train Loss: 0.8151, Test Loss: 3.275101661682129\n",
            "Epoch: 1855, Train Loss: 0.7973, Test Loss: 3.283056211471558\n",
            "Epoch: 1856, Train Loss: 0.7687, Test Loss: 3.273688292503357\n",
            "Epoch: 1857, Train Loss: 0.7585, Test Loss: 3.276646852493286\n",
            "Epoch: 1858, Train Loss: 0.7188, Test Loss: 3.2719799041748048\n",
            "Epoch: 1859, Train Loss: 0.8531, Test Loss: 3.2855010747909548\n",
            "Epoch: 1860, Train Loss: 0.8480, Test Loss: 3.281618046760559\n",
            "Epoch: 1861, Train Loss: 0.8168, Test Loss: 3.2775118827819822\n",
            "Epoch: 1862, Train Loss: 0.7687, Test Loss: 3.293563485145569\n",
            "Epoch: 1863, Train Loss: 0.8257, Test Loss: 3.3019386529922485\n",
            "Epoch: 1864, Train Loss: 0.7684, Test Loss: 3.3000707387924195\n",
            "Epoch: 1865, Train Loss: 0.8247, Test Loss: 3.275842380523682\n",
            "Epoch: 1866, Train Loss: 0.7975, Test Loss: 3.296637487411499\n",
            "Epoch: 1867, Train Loss: 0.7920, Test Loss: 3.2853068113327026\n",
            "Epoch: 1868, Train Loss: 0.7562, Test Loss: 3.2930304527282717\n",
            "Epoch: 1869, Train Loss: 0.8285, Test Loss: 3.28371000289917\n",
            "Epoch: 1870, Train Loss: 0.8620, Test Loss: 3.2666484832763674\n",
            "Epoch: 1871, Train Loss: 0.8570, Test Loss: 3.286683440208435\n",
            "Epoch: 1872, Train Loss: 0.8082, Test Loss: 3.2830962419509886\n",
            "Epoch: 1873, Train Loss: 0.8205, Test Loss: 3.2973906278610228\n",
            "Epoch: 1874, Train Loss: 0.7473, Test Loss: 3.30267539024353\n",
            "Epoch: 1875, Train Loss: 0.8418, Test Loss: 3.3181660413742065\n",
            "Epoch: 1876, Train Loss: 0.7808, Test Loss: 3.2824557781219483\n",
            "Epoch: 1877, Train Loss: 0.8560, Test Loss: 3.2899409770965575\n",
            "Epoch: 1878, Train Loss: 0.7981, Test Loss: 3.287073516845703\n",
            "Epoch: 1879, Train Loss: 0.7734, Test Loss: 3.275398826599121\n",
            "Epoch: 1880, Train Loss: 0.8354, Test Loss: 3.3355453491210936\n",
            "Epoch: 1881, Train Loss: 0.8614, Test Loss: 3.3304004192352297\n",
            "Epoch: 1882, Train Loss: 0.8426, Test Loss: 3.2949017763137816\n",
            "Epoch: 1883, Train Loss: 0.7370, Test Loss: 3.2733307838439942\n",
            "Epoch: 1884, Train Loss: 0.8171, Test Loss: 3.266471838951111\n",
            "Epoch: 1885, Train Loss: 0.7975, Test Loss: 3.2619606018066407\n",
            "Epoch: 1886, Train Loss: 0.7731, Test Loss: 3.256313920021057\n",
            "Epoch: 1887, Train Loss: 0.8469, Test Loss: 3.3199517250061037\n",
            "Epoch: 1888, Train Loss: 0.7942, Test Loss: 3.2813865661621096\n",
            "Epoch: 1889, Train Loss: 0.7766, Test Loss: 3.297304153442383\n",
            "Epoch: 1890, Train Loss: 0.8769, Test Loss: 3.314239001274109\n",
            "Epoch: 1891, Train Loss: 0.8122, Test Loss: 3.28817994594574\n",
            "Epoch: 1892, Train Loss: 0.7840, Test Loss: 3.2887763023376464\n",
            "Epoch: 1893, Train Loss: 0.8269, Test Loss: 3.2836361169815063\n",
            "Epoch: 1894, Train Loss: 0.8213, Test Loss: 3.310347080230713\n",
            "Epoch: 1895, Train Loss: 0.8447, Test Loss: 3.2723754405975343\n",
            "Epoch: 1896, Train Loss: 0.7438, Test Loss: 3.2806119680404664\n",
            "Epoch: 1897, Train Loss: 0.8350, Test Loss: 3.273414969444275\n",
            "Epoch: 1898, Train Loss: 0.7536, Test Loss: 3.2690179347991943\n",
            "Epoch: 1899, Train Loss: 0.8598, Test Loss: 3.2658085584640504\n",
            "Epoch: 1900, Train Loss: 0.7577, Test Loss: 3.278761553764343\n",
            "Epoch: 1901, Train Loss: 0.7313, Test Loss: 3.2820075511932374\n",
            "Epoch: 1902, Train Loss: 0.8179, Test Loss: 3.2993950128555296\n",
            "Epoch: 1903, Train Loss: 0.8339, Test Loss: 3.3092272758483885\n",
            "Epoch: 1904, Train Loss: 0.7939, Test Loss: 3.285611939430237\n",
            "Epoch: 1905, Train Loss: 0.8017, Test Loss: 3.283579158782959\n",
            "Epoch: 1906, Train Loss: 0.8003, Test Loss: 3.300259160995483\n",
            "Epoch: 1907, Train Loss: 0.8268, Test Loss: 3.292095422744751\n",
            "Epoch: 1908, Train Loss: 0.7984, Test Loss: 3.2717862367630004\n",
            "Epoch: 1909, Train Loss: 0.8162, Test Loss: 3.2769511222839354\n",
            "Epoch: 1910, Train Loss: 0.8625, Test Loss: 3.3127031087875367\n",
            "Epoch: 1911, Train Loss: 0.7438, Test Loss: 3.298557758331299\n",
            "Epoch: 1912, Train Loss: 0.8799, Test Loss: 3.28936927318573\n",
            "Epoch: 1913, Train Loss: 0.7693, Test Loss: 3.276899218559265\n",
            "Epoch: 1914, Train Loss: 0.7778, Test Loss: 3.2814722537994383\n",
            "Epoch: 1915, Train Loss: 0.8075, Test Loss: 3.293723440170288\n",
            "Epoch: 1916, Train Loss: 0.7662, Test Loss: 3.2819854974746705\n",
            "Epoch: 1917, Train Loss: 0.7632, Test Loss: 3.2718323469161987\n",
            "Epoch: 1918, Train Loss: 0.7791, Test Loss: 3.2678786754608153\n",
            "Epoch: 1919, Train Loss: 0.8383, Test Loss: 3.2692751407623293\n",
            "Epoch: 1920, Train Loss: 0.7556, Test Loss: 3.2898985385894775\n",
            "Epoch: 1921, Train Loss: 0.7671, Test Loss: 3.2907267093658445\n",
            "Epoch: 1922, Train Loss: 0.7463, Test Loss: 3.295995092391968\n",
            "Epoch: 1923, Train Loss: 0.8391, Test Loss: 3.3021135568618774\n",
            "Epoch: 1924, Train Loss: 0.7389, Test Loss: 3.2813039779663087\n",
            "Epoch: 1925, Train Loss: 0.8485, Test Loss: 3.2893677949905396\n",
            "Epoch: 1926, Train Loss: 0.8108, Test Loss: 3.273287868499756\n",
            "Epoch: 1927, Train Loss: 0.7910, Test Loss: 3.288490104675293\n",
            "Epoch: 1928, Train Loss: 0.8123, Test Loss: 3.272572088241577\n",
            "Epoch: 1929, Train Loss: 0.7598, Test Loss: 3.2699676275253298\n",
            "Epoch: 1930, Train Loss: 0.7751, Test Loss: 3.273302149772644\n",
            "Epoch: 1931, Train Loss: 0.7934, Test Loss: 3.261554551124573\n",
            "Epoch: 1932, Train Loss: 0.7448, Test Loss: 3.2890533447265624\n",
            "Epoch: 1933, Train Loss: 0.8058, Test Loss: 3.2753448247909547\n",
            "Epoch: 1934, Train Loss: 0.7765, Test Loss: 3.2734066963195803\n",
            "Epoch: 1935, Train Loss: 0.7553, Test Loss: 3.2918705463409426\n",
            "Epoch: 1936, Train Loss: 0.7852, Test Loss: 3.2797982454299928\n",
            "Epoch: 1937, Train Loss: 0.8900, Test Loss: 3.2968107223510743\n",
            "Epoch: 1938, Train Loss: 0.7661, Test Loss: 3.3060308933258056\n",
            "Epoch: 1939, Train Loss: 0.8303, Test Loss: 3.2892962455749513\n",
            "Epoch: 1940, Train Loss: 0.8639, Test Loss: 3.266200637817383\n",
            "Epoch: 1941, Train Loss: 0.8085, Test Loss: 3.2890501737594606\n",
            "Epoch: 1942, Train Loss: 0.7793, Test Loss: 3.3077247381210326\n",
            "Epoch: 1943, Train Loss: 0.8159, Test Loss: 3.28927264213562\n",
            "Epoch: 1944, Train Loss: 0.7387, Test Loss: 3.2869287014007567\n",
            "Epoch: 1945, Train Loss: 0.8824, Test Loss: 3.286942410469055\n",
            "Epoch: 1946, Train Loss: 0.7470, Test Loss: 3.2822805643081665\n",
            "Epoch: 1947, Train Loss: 0.7416, Test Loss: 3.296970319747925\n",
            "Epoch: 1948, Train Loss: 0.8002, Test Loss: 3.321744608879089\n",
            "Epoch: 1949, Train Loss: 0.7759, Test Loss: 3.26642107963562\n",
            "Epoch: 1950, Train Loss: 0.8218, Test Loss: 3.2720560312271116\n",
            "Epoch: 1951, Train Loss: 0.8405, Test Loss: 3.2921841382980346\n",
            "Epoch: 1952, Train Loss: 0.7652, Test Loss: 3.2898643255233764\n",
            "Epoch: 1953, Train Loss: 0.7978, Test Loss: 3.304401969909668\n",
            "Epoch: 1954, Train Loss: 0.7712, Test Loss: 3.335838866233826\n",
            "Epoch: 1955, Train Loss: 0.8517, Test Loss: 3.3048511505126954\n",
            "Epoch: 1956, Train Loss: 0.8625, Test Loss: 3.2607489585876466\n",
            "Epoch: 1957, Train Loss: 0.7604, Test Loss: 3.284239673614502\n",
            "Epoch: 1958, Train Loss: 0.7964, Test Loss: 3.3063235521316527\n",
            "Epoch: 1959, Train Loss: 0.7698, Test Loss: 3.269466280937195\n",
            "Epoch: 1960, Train Loss: 0.8183, Test Loss: 3.2761404037475588\n",
            "Epoch: 1961, Train Loss: 0.7740, Test Loss: 3.3017476558685304\n",
            "Epoch: 1962, Train Loss: 0.8272, Test Loss: 3.27161226272583\n",
            "Epoch: 1963, Train Loss: 0.7554, Test Loss: 3.2839550733566285\n",
            "Epoch: 1964, Train Loss: 0.8040, Test Loss: 3.2875322818756105\n",
            "Epoch: 1965, Train Loss: 0.7426, Test Loss: 3.2935935258865356\n",
            "Epoch: 1966, Train Loss: 0.7436, Test Loss: 3.2699901342391966\n",
            "Epoch: 1967, Train Loss: 0.8480, Test Loss: 3.254490303993225\n",
            "Epoch: 1968, Train Loss: 0.8369, Test Loss: 3.2664828300476074\n",
            "Epoch: 1969, Train Loss: 0.8488, Test Loss: 3.3040558099746704\n",
            "Epoch: 1970, Train Loss: 0.7453, Test Loss: 3.2789772748947144\n",
            "Epoch: 1971, Train Loss: 0.7486, Test Loss: 3.2653187036514284\n",
            "Epoch: 1972, Train Loss: 0.8412, Test Loss: 3.289345455169678\n",
            "Epoch: 1973, Train Loss: 0.7600, Test Loss: 3.290682339668274\n",
            "Epoch: 1974, Train Loss: 0.8247, Test Loss: 3.286702847480774\n",
            "Epoch: 1975, Train Loss: 0.8071, Test Loss: 3.2787128686904907\n",
            "Epoch: 1976, Train Loss: 0.8262, Test Loss: 3.309414792060852\n",
            "Epoch: 1977, Train Loss: 0.8042, Test Loss: 3.2719473361968996\n",
            "Epoch: 1978, Train Loss: 0.7716, Test Loss: 3.2730587482452393\n",
            "Epoch: 1979, Train Loss: 0.7916, Test Loss: 3.277610421180725\n",
            "Epoch: 1980, Train Loss: 0.7698, Test Loss: 3.2770560503005983\n",
            "Epoch: 1981, Train Loss: 0.7919, Test Loss: 3.259244275093079\n",
            "Epoch: 1982, Train Loss: 0.7627, Test Loss: 3.2871977567672728\n",
            "Epoch: 1983, Train Loss: 0.8297, Test Loss: 3.2941190719604494\n",
            "Epoch: 1984, Train Loss: 0.8037, Test Loss: 3.3028332471847532\n",
            "Epoch: 1985, Train Loss: 0.8209, Test Loss: 3.2751471042633056\n",
            "Epoch: 1986, Train Loss: 0.7589, Test Loss: 3.2879717111587525\n",
            "Epoch: 1987, Train Loss: 0.8215, Test Loss: 3.272592639923096\n",
            "Epoch: 1988, Train Loss: 0.8013, Test Loss: 3.2661521196365357\n",
            "Epoch: 1989, Train Loss: 0.7409, Test Loss: 3.2757755517959595\n",
            "Epoch: 1990, Train Loss: 0.8299, Test Loss: 3.2686997413635255\n",
            "Epoch: 1991, Train Loss: 0.8203, Test Loss: 3.307106637954712\n",
            "Epoch: 1992, Train Loss: 0.8601, Test Loss: 3.260072350502014\n",
            "Epoch: 1993, Train Loss: 0.7453, Test Loss: 3.255919313430786\n",
            "Epoch: 1994, Train Loss: 0.7655, Test Loss: 3.254521441459656\n",
            "Epoch: 1995, Train Loss: 0.7975, Test Loss: 3.2701783180236816\n",
            "Epoch: 1996, Train Loss: 0.7786, Test Loss: 3.268834924697876\n",
            "Epoch: 1997, Train Loss: 0.7842, Test Loss: 3.276258850097656\n",
            "Epoch: 1998, Train Loss: 0.7743, Test Loss: 3.271051859855652\n",
            "Epoch: 1999, Train Loss: 0.8223, Test Loss: 3.279982352256775\n",
            "Epoch: 2000, Train Loss: 0.7386, Test Loss: 3.2822144269943236\n",
            "Epoch: 1588 -> Best loss: 3.244602584838867\n",
            "time: 13min 56s (started: 2021-03-04 12:31:10 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWNd3T5MwIgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d52b57-5e22-4ca4-bff1-c1ce883c8526"
      },
      "source": [
        "model = torch.load(\"final_net.pth\").to(DEVICE)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 19.2 ms (started: 2021-03-04 12:45:33 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aARL8OY1xRvp",
        "outputId": "4dbdaa62-6c4c-4898-faa7-da4dc8963d28"
      },
      "source": [
        "train_dataset = TensorDataset(\r\n",
        "    torch.tensor(np.concatenate((X_train_final, X_test_final)), dtype=torch.float),\r\n",
        "    torch.tensor(pd.concat([y_train_final, y_test_final]).values)\r\n",
        ")\r\n",
        "\r\n",
        "train_loader = DataLoader(\r\n",
        "    train_dataset, batch_size=512, shuffle=True,\r\n",
        ")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 210 ms (started: 2021-03-04 12:45:34 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPHbfZ_rxSOt",
        "outputId": "cd4f2df4-e016-4e49-fefa-2bbbf757fe48"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0007660119153202037)\r\n",
        "criterion = torch.nn.L1Loss()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.85 ms (started: 2021-03-04 12:45:35 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60bgldLqx3dn",
        "outputId": "ec5209b7-afeb-40bd-dc74-8aa29b58e9f8"
      },
      "source": [
        "n_epochs = 1000\r\n",
        "for epoch in range(1, n_epochs+1):\r\n",
        "    total_loss = 0\r\n",
        "    model.train()\r\n",
        "    for (x, y) in train_loader:\r\n",
        "        x = x.to(DEVICE)\r\n",
        "        y = y.to(DEVICE)\r\n",
        "        optimizer.zero_grad()  # Clear gradients.\r\n",
        "        out = model(x)  # Perform a single forward pass.\r\n",
        "        loss = criterion(out, y)  # Compute the loss solely based on the training nodes\r\n",
        "        loss.backward()  # Derive gradients.\r\n",
        "        optimizer.step()\r\n",
        "        total_loss += loss.item()\r\n",
        "    print(f'Epoch: {epoch}, Train Loss: {total_loss/len(train_loader):.4f}')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train Loss: 1.3352\n",
            "Epoch: 2, Train Loss: 1.3256\n",
            "Epoch: 3, Train Loss: 1.2836\n",
            "Epoch: 4, Train Loss: 1.2715\n",
            "Epoch: 5, Train Loss: 1.2772\n",
            "Epoch: 6, Train Loss: 1.2404\n",
            "Epoch: 7, Train Loss: 1.2339\n",
            "Epoch: 8, Train Loss: 1.1941\n",
            "Epoch: 9, Train Loss: 1.2117\n",
            "Epoch: 10, Train Loss: 1.1904\n",
            "Epoch: 11, Train Loss: 1.1386\n",
            "Epoch: 12, Train Loss: 1.2035\n",
            "Epoch: 13, Train Loss: 1.1627\n",
            "Epoch: 14, Train Loss: 1.1078\n",
            "Epoch: 15, Train Loss: 1.0681\n",
            "Epoch: 16, Train Loss: 1.1412\n",
            "Epoch: 17, Train Loss: 1.1244\n",
            "Epoch: 18, Train Loss: 1.1263\n",
            "Epoch: 19, Train Loss: 1.1256\n",
            "Epoch: 20, Train Loss: 1.0872\n",
            "Epoch: 21, Train Loss: 1.0914\n",
            "Epoch: 22, Train Loss: 1.0524\n",
            "Epoch: 23, Train Loss: 1.0866\n",
            "Epoch: 24, Train Loss: 1.0629\n",
            "Epoch: 25, Train Loss: 1.0641\n",
            "Epoch: 26, Train Loss: 1.0085\n",
            "Epoch: 27, Train Loss: 1.0318\n",
            "Epoch: 28, Train Loss: 1.0138\n",
            "Epoch: 29, Train Loss: 1.0050\n",
            "Epoch: 30, Train Loss: 1.0612\n",
            "Epoch: 31, Train Loss: 1.0627\n",
            "Epoch: 32, Train Loss: 1.0460\n",
            "Epoch: 33, Train Loss: 1.0228\n",
            "Epoch: 34, Train Loss: 1.0117\n",
            "Epoch: 35, Train Loss: 0.9964\n",
            "Epoch: 36, Train Loss: 0.9815\n",
            "Epoch: 37, Train Loss: 0.9730\n",
            "Epoch: 38, Train Loss: 0.9896\n",
            "Epoch: 39, Train Loss: 0.9954\n",
            "Epoch: 40, Train Loss: 0.9810\n",
            "Epoch: 41, Train Loss: 0.9722\n",
            "Epoch: 42, Train Loss: 0.9543\n",
            "Epoch: 43, Train Loss: 1.0237\n",
            "Epoch: 44, Train Loss: 1.0020\n",
            "Epoch: 45, Train Loss: 0.9864\n",
            "Epoch: 46, Train Loss: 0.9951\n",
            "Epoch: 47, Train Loss: 1.0020\n",
            "Epoch: 48, Train Loss: 0.9790\n",
            "Epoch: 49, Train Loss: 0.9437\n",
            "Epoch: 50, Train Loss: 0.9634\n",
            "Epoch: 51, Train Loss: 0.9120\n",
            "Epoch: 52, Train Loss: 0.9456\n",
            "Epoch: 53, Train Loss: 0.9753\n",
            "Epoch: 54, Train Loss: 0.9227\n",
            "Epoch: 55, Train Loss: 0.8956\n",
            "Epoch: 56, Train Loss: 0.9609\n",
            "Epoch: 57, Train Loss: 0.9463\n",
            "Epoch: 58, Train Loss: 0.9738\n",
            "Epoch: 59, Train Loss: 0.9313\n",
            "Epoch: 60, Train Loss: 0.9386\n",
            "Epoch: 61, Train Loss: 0.8800\n",
            "Epoch: 62, Train Loss: 0.9139\n",
            "Epoch: 63, Train Loss: 0.9102\n",
            "Epoch: 64, Train Loss: 0.9407\n",
            "Epoch: 65, Train Loss: 0.9775\n",
            "Epoch: 66, Train Loss: 0.9316\n",
            "Epoch: 67, Train Loss: 0.9295\n",
            "Epoch: 68, Train Loss: 0.9417\n",
            "Epoch: 69, Train Loss: 0.9155\n",
            "Epoch: 70, Train Loss: 0.9346\n",
            "Epoch: 71, Train Loss: 0.9326\n",
            "Epoch: 72, Train Loss: 0.9584\n",
            "Epoch: 73, Train Loss: 0.9820\n",
            "Epoch: 74, Train Loss: 0.9199\n",
            "Epoch: 75, Train Loss: 0.9094\n",
            "Epoch: 76, Train Loss: 0.8817\n",
            "Epoch: 77, Train Loss: 0.8804\n",
            "Epoch: 78, Train Loss: 0.9104\n",
            "Epoch: 79, Train Loss: 0.9146\n",
            "Epoch: 80, Train Loss: 0.9323\n",
            "Epoch: 81, Train Loss: 0.9089\n",
            "Epoch: 82, Train Loss: 0.8872\n",
            "Epoch: 83, Train Loss: 0.9566\n",
            "Epoch: 84, Train Loss: 0.9238\n",
            "Epoch: 85, Train Loss: 0.9677\n",
            "Epoch: 86, Train Loss: 0.9189\n",
            "Epoch: 87, Train Loss: 0.9380\n",
            "Epoch: 88, Train Loss: 0.8881\n",
            "Epoch: 89, Train Loss: 0.9160\n",
            "Epoch: 90, Train Loss: 0.8820\n",
            "Epoch: 91, Train Loss: 0.8915\n",
            "Epoch: 92, Train Loss: 0.9024\n",
            "Epoch: 93, Train Loss: 0.9091\n",
            "Epoch: 94, Train Loss: 0.9793\n",
            "Epoch: 95, Train Loss: 0.8873\n",
            "Epoch: 96, Train Loss: 0.8813\n",
            "Epoch: 97, Train Loss: 0.8907\n",
            "Epoch: 98, Train Loss: 0.9336\n",
            "Epoch: 99, Train Loss: 0.9244\n",
            "Epoch: 100, Train Loss: 0.9147\n",
            "Epoch: 101, Train Loss: 0.8886\n",
            "Epoch: 102, Train Loss: 0.8971\n",
            "Epoch: 103, Train Loss: 0.9002\n",
            "Epoch: 104, Train Loss: 0.8603\n",
            "Epoch: 105, Train Loss: 0.8770\n",
            "Epoch: 106, Train Loss: 0.8775\n",
            "Epoch: 107, Train Loss: 0.9241\n",
            "Epoch: 108, Train Loss: 0.8593\n",
            "Epoch: 109, Train Loss: 0.9148\n",
            "Epoch: 110, Train Loss: 0.9200\n",
            "Epoch: 111, Train Loss: 0.8550\n",
            "Epoch: 112, Train Loss: 0.8802\n",
            "Epoch: 113, Train Loss: 0.9030\n",
            "Epoch: 114, Train Loss: 0.8327\n",
            "Epoch: 115, Train Loss: 0.9672\n",
            "Epoch: 116, Train Loss: 0.8258\n",
            "Epoch: 117, Train Loss: 0.9029\n",
            "Epoch: 118, Train Loss: 0.8764\n",
            "Epoch: 119, Train Loss: 0.8890\n",
            "Epoch: 120, Train Loss: 0.8626\n",
            "Epoch: 121, Train Loss: 0.8707\n",
            "Epoch: 122, Train Loss: 0.9052\n",
            "Epoch: 123, Train Loss: 0.8759\n",
            "Epoch: 124, Train Loss: 0.8993\n",
            "Epoch: 125, Train Loss: 0.9127\n",
            "Epoch: 126, Train Loss: 0.8890\n",
            "Epoch: 127, Train Loss: 0.8537\n",
            "Epoch: 128, Train Loss: 0.8914\n",
            "Epoch: 129, Train Loss: 0.9216\n",
            "Epoch: 130, Train Loss: 0.8789\n",
            "Epoch: 131, Train Loss: 0.8452\n",
            "Epoch: 132, Train Loss: 0.8133\n",
            "Epoch: 133, Train Loss: 0.8817\n",
            "Epoch: 134, Train Loss: 0.8785\n",
            "Epoch: 135, Train Loss: 0.8610\n",
            "Epoch: 136, Train Loss: 0.8990\n",
            "Epoch: 137, Train Loss: 0.8457\n",
            "Epoch: 138, Train Loss: 0.8775\n",
            "Epoch: 139, Train Loss: 0.8725\n",
            "Epoch: 140, Train Loss: 0.8906\n",
            "Epoch: 141, Train Loss: 0.8596\n",
            "Epoch: 142, Train Loss: 0.9036\n",
            "Epoch: 143, Train Loss: 0.8661\n",
            "Epoch: 144, Train Loss: 0.8772\n",
            "Epoch: 145, Train Loss: 0.8929\n",
            "Epoch: 146, Train Loss: 0.8835\n",
            "Epoch: 147, Train Loss: 0.8775\n",
            "Epoch: 148, Train Loss: 0.9350\n",
            "Epoch: 149, Train Loss: 0.9381\n",
            "Epoch: 150, Train Loss: 0.8914\n",
            "Epoch: 151, Train Loss: 0.8970\n",
            "Epoch: 152, Train Loss: 0.8398\n",
            "Epoch: 153, Train Loss: 0.9108\n",
            "Epoch: 154, Train Loss: 0.8477\n",
            "Epoch: 155, Train Loss: 0.8719\n",
            "Epoch: 156, Train Loss: 0.9175\n",
            "Epoch: 157, Train Loss: 0.8640\n",
            "Epoch: 158, Train Loss: 0.9245\n",
            "Epoch: 159, Train Loss: 0.8669\n",
            "Epoch: 160, Train Loss: 0.8514\n",
            "Epoch: 161, Train Loss: 0.8786\n",
            "Epoch: 162, Train Loss: 0.8371\n",
            "Epoch: 163, Train Loss: 0.8888\n",
            "Epoch: 164, Train Loss: 0.9101\n",
            "Epoch: 165, Train Loss: 0.8603\n",
            "Epoch: 166, Train Loss: 0.8290\n",
            "Epoch: 167, Train Loss: 0.8314\n",
            "Epoch: 168, Train Loss: 0.8510\n",
            "Epoch: 169, Train Loss: 0.8510\n",
            "Epoch: 170, Train Loss: 0.8863\n",
            "Epoch: 171, Train Loss: 0.8548\n",
            "Epoch: 172, Train Loss: 0.8860\n",
            "Epoch: 173, Train Loss: 0.8360\n",
            "Epoch: 174, Train Loss: 0.8394\n",
            "Epoch: 175, Train Loss: 0.8534\n",
            "Epoch: 176, Train Loss: 0.8744\n",
            "Epoch: 177, Train Loss: 0.8727\n",
            "Epoch: 178, Train Loss: 0.8312\n",
            "Epoch: 179, Train Loss: 0.8552\n",
            "Epoch: 180, Train Loss: 0.8811\n",
            "Epoch: 181, Train Loss: 0.8165\n",
            "Epoch: 182, Train Loss: 0.8821\n",
            "Epoch: 183, Train Loss: 0.9147\n",
            "Epoch: 184, Train Loss: 0.8400\n",
            "Epoch: 185, Train Loss: 0.8302\n",
            "Epoch: 186, Train Loss: 0.8633\n",
            "Epoch: 187, Train Loss: 0.8889\n",
            "Epoch: 188, Train Loss: 0.8752\n",
            "Epoch: 189, Train Loss: 0.8135\n",
            "Epoch: 190, Train Loss: 0.8698\n",
            "Epoch: 191, Train Loss: 0.8575\n",
            "Epoch: 192, Train Loss: 0.8602\n",
            "Epoch: 193, Train Loss: 0.9077\n",
            "Epoch: 194, Train Loss: 0.8564\n",
            "Epoch: 195, Train Loss: 0.8176\n",
            "Epoch: 196, Train Loss: 0.8630\n",
            "Epoch: 197, Train Loss: 0.8895\n",
            "Epoch: 198, Train Loss: 0.8910\n",
            "Epoch: 199, Train Loss: 0.8218\n",
            "Epoch: 200, Train Loss: 0.8452\n",
            "Epoch: 201, Train Loss: 0.8477\n",
            "Epoch: 202, Train Loss: 0.8550\n",
            "Epoch: 203, Train Loss: 0.8824\n",
            "Epoch: 204, Train Loss: 0.8276\n",
            "Epoch: 205, Train Loss: 0.8642\n",
            "Epoch: 206, Train Loss: 0.8747\n",
            "Epoch: 207, Train Loss: 0.8318\n",
            "Epoch: 208, Train Loss: 0.8472\n",
            "Epoch: 209, Train Loss: 0.8904\n",
            "Epoch: 210, Train Loss: 0.8247\n",
            "Epoch: 211, Train Loss: 0.8118\n",
            "Epoch: 212, Train Loss: 0.8330\n",
            "Epoch: 213, Train Loss: 0.9395\n",
            "Epoch: 214, Train Loss: 0.8244\n",
            "Epoch: 215, Train Loss: 0.8642\n",
            "Epoch: 216, Train Loss: 0.8497\n",
            "Epoch: 217, Train Loss: 0.8537\n",
            "Epoch: 218, Train Loss: 0.8058\n",
            "Epoch: 219, Train Loss: 0.8140\n",
            "Epoch: 220, Train Loss: 0.8594\n",
            "Epoch: 221, Train Loss: 0.8125\n",
            "Epoch: 222, Train Loss: 0.8357\n",
            "Epoch: 223, Train Loss: 0.8299\n",
            "Epoch: 224, Train Loss: 0.8311\n",
            "Epoch: 225, Train Loss: 0.8528\n",
            "Epoch: 226, Train Loss: 0.8686\n",
            "Epoch: 227, Train Loss: 0.8592\n",
            "Epoch: 228, Train Loss: 0.8410\n",
            "Epoch: 229, Train Loss: 0.8379\n",
            "Epoch: 230, Train Loss: 0.8493\n",
            "Epoch: 231, Train Loss: 0.8369\n",
            "Epoch: 232, Train Loss: 0.8435\n",
            "Epoch: 233, Train Loss: 0.7975\n",
            "Epoch: 234, Train Loss: 0.8984\n",
            "Epoch: 235, Train Loss: 0.8373\n",
            "Epoch: 236, Train Loss: 0.8420\n",
            "Epoch: 237, Train Loss: 0.8784\n",
            "Epoch: 238, Train Loss: 0.8659\n",
            "Epoch: 239, Train Loss: 0.8741\n",
            "Epoch: 240, Train Loss: 0.8647\n",
            "Epoch: 241, Train Loss: 0.8639\n",
            "Epoch: 242, Train Loss: 0.7992\n",
            "Epoch: 243, Train Loss: 0.8658\n",
            "Epoch: 244, Train Loss: 0.8265\n",
            "Epoch: 245, Train Loss: 0.8743\n",
            "Epoch: 246, Train Loss: 0.8473\n",
            "Epoch: 247, Train Loss: 0.8083\n",
            "Epoch: 248, Train Loss: 0.8311\n",
            "Epoch: 249, Train Loss: 0.8028\n",
            "Epoch: 250, Train Loss: 0.8695\n",
            "Epoch: 251, Train Loss: 0.7834\n",
            "Epoch: 252, Train Loss: 0.8053\n",
            "Epoch: 253, Train Loss: 0.8025\n",
            "Epoch: 254, Train Loss: 0.8801\n",
            "Epoch: 255, Train Loss: 0.8868\n",
            "Epoch: 256, Train Loss: 0.8314\n",
            "Epoch: 257, Train Loss: 0.8133\n",
            "Epoch: 258, Train Loss: 0.8346\n",
            "Epoch: 259, Train Loss: 0.8338\n",
            "Epoch: 260, Train Loss: 0.8676\n",
            "Epoch: 261, Train Loss: 0.8523\n",
            "Epoch: 262, Train Loss: 0.8105\n",
            "Epoch: 263, Train Loss: 0.8004\n",
            "Epoch: 264, Train Loss: 0.8136\n",
            "Epoch: 265, Train Loss: 0.8353\n",
            "Epoch: 266, Train Loss: 0.8092\n",
            "Epoch: 267, Train Loss: 0.8604\n",
            "Epoch: 268, Train Loss: 0.8666\n",
            "Epoch: 269, Train Loss: 0.8679\n",
            "Epoch: 270, Train Loss: 0.8802\n",
            "Epoch: 271, Train Loss: 0.8019\n",
            "Epoch: 272, Train Loss: 0.8447\n",
            "Epoch: 273, Train Loss: 0.7999\n",
            "Epoch: 274, Train Loss: 0.8356\n",
            "Epoch: 275, Train Loss: 0.8881\n",
            "Epoch: 276, Train Loss: 0.8318\n",
            "Epoch: 277, Train Loss: 0.9202\n",
            "Epoch: 278, Train Loss: 0.8215\n",
            "Epoch: 279, Train Loss: 0.8870\n",
            "Epoch: 280, Train Loss: 0.8194\n",
            "Epoch: 281, Train Loss: 0.8585\n",
            "Epoch: 282, Train Loss: 0.8525\n",
            "Epoch: 283, Train Loss: 0.8157\n",
            "Epoch: 284, Train Loss: 0.8442\n",
            "Epoch: 285, Train Loss: 0.8236\n",
            "Epoch: 286, Train Loss: 0.8577\n",
            "Epoch: 287, Train Loss: 0.8791\n",
            "Epoch: 288, Train Loss: 0.8977\n",
            "Epoch: 289, Train Loss: 0.7938\n",
            "Epoch: 290, Train Loss: 0.8183\n",
            "Epoch: 291, Train Loss: 0.8234\n",
            "Epoch: 292, Train Loss: 0.8165\n",
            "Epoch: 293, Train Loss: 0.8750\n",
            "Epoch: 294, Train Loss: 0.8374\n",
            "Epoch: 295, Train Loss: 0.8148\n",
            "Epoch: 296, Train Loss: 0.8554\n",
            "Epoch: 297, Train Loss: 0.9056\n",
            "Epoch: 298, Train Loss: 0.8552\n",
            "Epoch: 299, Train Loss: 0.8128\n",
            "Epoch: 300, Train Loss: 0.8658\n",
            "Epoch: 301, Train Loss: 0.8300\n",
            "Epoch: 302, Train Loss: 0.8365\n",
            "Epoch: 303, Train Loss: 0.8506\n",
            "Epoch: 304, Train Loss: 0.8167\n",
            "Epoch: 305, Train Loss: 0.8662\n",
            "Epoch: 306, Train Loss: 0.8081\n",
            "Epoch: 307, Train Loss: 0.8320\n",
            "Epoch: 308, Train Loss: 0.8068\n",
            "Epoch: 309, Train Loss: 0.8733\n",
            "Epoch: 310, Train Loss: 0.8143\n",
            "Epoch: 311, Train Loss: 0.8664\n",
            "Epoch: 312, Train Loss: 0.8899\n",
            "Epoch: 313, Train Loss: 0.8426\n",
            "Epoch: 314, Train Loss: 0.8533\n",
            "Epoch: 315, Train Loss: 0.8330\n",
            "Epoch: 316, Train Loss: 0.8219\n",
            "Epoch: 317, Train Loss: 0.8151\n",
            "Epoch: 318, Train Loss: 0.8608\n",
            "Epoch: 319, Train Loss: 0.8313\n",
            "Epoch: 320, Train Loss: 0.8726\n",
            "Epoch: 321, Train Loss: 0.8626\n",
            "Epoch: 322, Train Loss: 0.7963\n",
            "Epoch: 323, Train Loss: 0.7972\n",
            "Epoch: 324, Train Loss: 0.8022\n",
            "Epoch: 325, Train Loss: 0.8646\n",
            "Epoch: 326, Train Loss: 0.8303\n",
            "Epoch: 327, Train Loss: 0.8359\n",
            "Epoch: 328, Train Loss: 0.8670\n",
            "Epoch: 329, Train Loss: 0.8443\n",
            "Epoch: 330, Train Loss: 0.8539\n",
            "Epoch: 331, Train Loss: 0.8079\n",
            "Epoch: 332, Train Loss: 0.9242\n",
            "Epoch: 333, Train Loss: 0.8638\n",
            "Epoch: 334, Train Loss: 0.8361\n",
            "Epoch: 335, Train Loss: 0.8642\n",
            "Epoch: 336, Train Loss: 0.8115\n",
            "Epoch: 337, Train Loss: 0.8314\n",
            "Epoch: 338, Train Loss: 0.8273\n",
            "Epoch: 339, Train Loss: 0.8174\n",
            "Epoch: 340, Train Loss: 0.8625\n",
            "Epoch: 341, Train Loss: 0.8462\n",
            "Epoch: 342, Train Loss: 0.8108\n",
            "Epoch: 343, Train Loss: 0.8538\n",
            "Epoch: 344, Train Loss: 0.7990\n",
            "Epoch: 345, Train Loss: 0.8469\n",
            "Epoch: 346, Train Loss: 0.8380\n",
            "Epoch: 347, Train Loss: 0.8511\n",
            "Epoch: 348, Train Loss: 0.8275\n",
            "Epoch: 349, Train Loss: 0.8608\n",
            "Epoch: 350, Train Loss: 0.8175\n",
            "Epoch: 351, Train Loss: 0.8328\n",
            "Epoch: 352, Train Loss: 0.7948\n",
            "Epoch: 353, Train Loss: 0.8330\n",
            "Epoch: 354, Train Loss: 0.8210\n",
            "Epoch: 355, Train Loss: 0.8486\n",
            "Epoch: 356, Train Loss: 0.8281\n",
            "Epoch: 357, Train Loss: 0.8307\n",
            "Epoch: 358, Train Loss: 0.8609\n",
            "Epoch: 359, Train Loss: 0.8465\n",
            "Epoch: 360, Train Loss: 0.8110\n",
            "Epoch: 361, Train Loss: 0.8444\n",
            "Epoch: 362, Train Loss: 0.8037\n",
            "Epoch: 363, Train Loss: 0.8145\n",
            "Epoch: 364, Train Loss: 0.8227\n",
            "Epoch: 365, Train Loss: 0.8272\n",
            "Epoch: 366, Train Loss: 0.7803\n",
            "Epoch: 367, Train Loss: 0.8374\n",
            "Epoch: 368, Train Loss: 0.8157\n",
            "Epoch: 369, Train Loss: 0.7828\n",
            "Epoch: 370, Train Loss: 0.8210\n",
            "Epoch: 371, Train Loss: 0.7986\n",
            "Epoch: 372, Train Loss: 0.8192\n",
            "Epoch: 373, Train Loss: 0.8567\n",
            "Epoch: 374, Train Loss: 0.8762\n",
            "Epoch: 375, Train Loss: 0.8615\n",
            "Epoch: 376, Train Loss: 0.7992\n",
            "Epoch: 377, Train Loss: 0.8307\n",
            "Epoch: 378, Train Loss: 0.8401\n",
            "Epoch: 379, Train Loss: 0.7983\n",
            "Epoch: 380, Train Loss: 0.7985\n",
            "Epoch: 381, Train Loss: 0.8084\n",
            "Epoch: 382, Train Loss: 0.8178\n",
            "Epoch: 383, Train Loss: 0.8529\n",
            "Epoch: 384, Train Loss: 0.8489\n",
            "Epoch: 385, Train Loss: 0.8487\n",
            "Epoch: 386, Train Loss: 0.8360\n",
            "Epoch: 387, Train Loss: 0.8252\n",
            "Epoch: 388, Train Loss: 0.8032\n",
            "Epoch: 389, Train Loss: 0.8132\n",
            "Epoch: 390, Train Loss: 0.8778\n",
            "Epoch: 391, Train Loss: 0.8037\n",
            "Epoch: 392, Train Loss: 0.7933\n",
            "Epoch: 393, Train Loss: 0.8948\n",
            "Epoch: 394, Train Loss: 0.7931\n",
            "Epoch: 395, Train Loss: 0.7991\n",
            "Epoch: 396, Train Loss: 0.7686\n",
            "Epoch: 397, Train Loss: 0.8700\n",
            "Epoch: 398, Train Loss: 0.8220\n",
            "Epoch: 399, Train Loss: 0.8070\n",
            "Epoch: 400, Train Loss: 0.8378\n",
            "Epoch: 401, Train Loss: 0.7932\n",
            "Epoch: 402, Train Loss: 0.8318\n",
            "Epoch: 403, Train Loss: 0.8941\n",
            "Epoch: 404, Train Loss: 0.8629\n",
            "Epoch: 405, Train Loss: 0.8488\n",
            "Epoch: 406, Train Loss: 0.8602\n",
            "Epoch: 407, Train Loss: 0.8006\n",
            "Epoch: 408, Train Loss: 0.7865\n",
            "Epoch: 409, Train Loss: 0.8152\n",
            "Epoch: 410, Train Loss: 0.8222\n",
            "Epoch: 411, Train Loss: 0.7920\n",
            "Epoch: 412, Train Loss: 0.8439\n",
            "Epoch: 413, Train Loss: 0.8160\n",
            "Epoch: 414, Train Loss: 0.8175\n",
            "Epoch: 415, Train Loss: 0.7926\n",
            "Epoch: 416, Train Loss: 0.7993\n",
            "Epoch: 417, Train Loss: 0.8321\n",
            "Epoch: 418, Train Loss: 0.8447\n",
            "Epoch: 419, Train Loss: 0.8413\n",
            "Epoch: 420, Train Loss: 0.7887\n",
            "Epoch: 421, Train Loss: 0.8213\n",
            "Epoch: 422, Train Loss: 0.8029\n",
            "Epoch: 423, Train Loss: 0.8410\n",
            "Epoch: 424, Train Loss: 0.7726\n",
            "Epoch: 425, Train Loss: 0.8561\n",
            "Epoch: 426, Train Loss: 0.8197\n",
            "Epoch: 427, Train Loss: 0.8476\n",
            "Epoch: 428, Train Loss: 0.8072\n",
            "Epoch: 429, Train Loss: 0.8221\n",
            "Epoch: 430, Train Loss: 0.8161\n",
            "Epoch: 431, Train Loss: 0.8020\n",
            "Epoch: 432, Train Loss: 0.8902\n",
            "Epoch: 433, Train Loss: 0.8105\n",
            "Epoch: 434, Train Loss: 0.8118\n",
            "Epoch: 435, Train Loss: 0.8340\n",
            "Epoch: 436, Train Loss: 0.8563\n",
            "Epoch: 437, Train Loss: 0.8174\n",
            "Epoch: 438, Train Loss: 0.8479\n",
            "Epoch: 439, Train Loss: 0.7603\n",
            "Epoch: 440, Train Loss: 0.8814\n",
            "Epoch: 441, Train Loss: 0.8483\n",
            "Epoch: 442, Train Loss: 0.8265\n",
            "Epoch: 443, Train Loss: 0.8688\n",
            "Epoch: 444, Train Loss: 0.8019\n",
            "Epoch: 445, Train Loss: 0.8258\n",
            "Epoch: 446, Train Loss: 0.7635\n",
            "Epoch: 447, Train Loss: 0.8334\n",
            "Epoch: 448, Train Loss: 0.8332\n",
            "Epoch: 449, Train Loss: 0.8252\n",
            "Epoch: 450, Train Loss: 0.8189\n",
            "Epoch: 451, Train Loss: 0.8550\n",
            "Epoch: 452, Train Loss: 0.7605\n",
            "Epoch: 453, Train Loss: 0.8324\n",
            "Epoch: 454, Train Loss: 0.8394\n",
            "Epoch: 455, Train Loss: 0.8465\n",
            "Epoch: 456, Train Loss: 0.8321\n",
            "Epoch: 457, Train Loss: 0.8283\n",
            "Epoch: 458, Train Loss: 0.7891\n",
            "Epoch: 459, Train Loss: 0.8557\n",
            "Epoch: 460, Train Loss: 0.8309\n",
            "Epoch: 461, Train Loss: 0.8078\n",
            "Epoch: 462, Train Loss: 0.8440\n",
            "Epoch: 463, Train Loss: 0.9292\n",
            "Epoch: 464, Train Loss: 0.7865\n",
            "Epoch: 465, Train Loss: 0.8154\n",
            "Epoch: 466, Train Loss: 0.7637\n",
            "Epoch: 467, Train Loss: 0.8576\n",
            "Epoch: 468, Train Loss: 0.8456\n",
            "Epoch: 469, Train Loss: 0.8909\n",
            "Epoch: 470, Train Loss: 0.8114\n",
            "Epoch: 471, Train Loss: 0.8392\n",
            "Epoch: 472, Train Loss: 0.8323\n",
            "Epoch: 473, Train Loss: 0.9014\n",
            "Epoch: 474, Train Loss: 0.7621\n",
            "Epoch: 475, Train Loss: 0.8117\n",
            "Epoch: 476, Train Loss: 0.7842\n",
            "Epoch: 477, Train Loss: 0.8148\n",
            "Epoch: 478, Train Loss: 0.7867\n",
            "Epoch: 479, Train Loss: 0.7676\n",
            "Epoch: 480, Train Loss: 0.7997\n",
            "Epoch: 481, Train Loss: 0.8301\n",
            "Epoch: 482, Train Loss: 0.8303\n",
            "Epoch: 483, Train Loss: 0.8251\n",
            "Epoch: 484, Train Loss: 0.8396\n",
            "Epoch: 485, Train Loss: 0.8261\n",
            "Epoch: 486, Train Loss: 0.8083\n",
            "Epoch: 487, Train Loss: 0.8744\n",
            "Epoch: 488, Train Loss: 0.8429\n",
            "Epoch: 489, Train Loss: 0.8032\n",
            "Epoch: 490, Train Loss: 0.8183\n",
            "Epoch: 491, Train Loss: 0.7608\n",
            "Epoch: 492, Train Loss: 0.8240\n",
            "Epoch: 493, Train Loss: 0.8062\n",
            "Epoch: 494, Train Loss: 0.8757\n",
            "Epoch: 495, Train Loss: 0.8297\n",
            "Epoch: 496, Train Loss: 0.8526\n",
            "Epoch: 497, Train Loss: 0.7973\n",
            "Epoch: 498, Train Loss: 0.7822\n",
            "Epoch: 499, Train Loss: 0.8050\n",
            "Epoch: 500, Train Loss: 0.7943\n",
            "Epoch: 501, Train Loss: 0.8016\n",
            "Epoch: 502, Train Loss: 0.7832\n",
            "Epoch: 503, Train Loss: 0.8082\n",
            "Epoch: 504, Train Loss: 0.8134\n",
            "Epoch: 505, Train Loss: 0.8056\n",
            "Epoch: 506, Train Loss: 0.8059\n",
            "Epoch: 507, Train Loss: 0.8641\n",
            "Epoch: 508, Train Loss: 0.8536\n",
            "Epoch: 509, Train Loss: 0.8472\n",
            "Epoch: 510, Train Loss: 0.8643\n",
            "Epoch: 511, Train Loss: 0.7853\n",
            "Epoch: 512, Train Loss: 0.8704\n",
            "Epoch: 513, Train Loss: 0.8131\n",
            "Epoch: 514, Train Loss: 0.8882\n",
            "Epoch: 515, Train Loss: 0.8204\n",
            "Epoch: 516, Train Loss: 0.8499\n",
            "Epoch: 517, Train Loss: 0.8290\n",
            "Epoch: 518, Train Loss: 0.8545\n",
            "Epoch: 519, Train Loss: 0.8192\n",
            "Epoch: 520, Train Loss: 0.8059\n",
            "Epoch: 521, Train Loss: 0.7883\n",
            "Epoch: 522, Train Loss: 0.8033\n",
            "Epoch: 523, Train Loss: 0.8478\n",
            "Epoch: 524, Train Loss: 0.8832\n",
            "Epoch: 525, Train Loss: 0.8211\n",
            "Epoch: 526, Train Loss: 0.7904\n",
            "Epoch: 527, Train Loss: 0.8114\n",
            "Epoch: 528, Train Loss: 0.8482\n",
            "Epoch: 529, Train Loss: 0.8592\n",
            "Epoch: 530, Train Loss: 0.8487\n",
            "Epoch: 531, Train Loss: 0.8000\n",
            "Epoch: 532, Train Loss: 0.8193\n",
            "Epoch: 533, Train Loss: 0.7974\n",
            "Epoch: 534, Train Loss: 0.8167\n",
            "Epoch: 535, Train Loss: 0.7983\n",
            "Epoch: 536, Train Loss: 0.7883\n",
            "Epoch: 537, Train Loss: 0.7621\n",
            "Epoch: 538, Train Loss: 0.8684\n",
            "Epoch: 539, Train Loss: 0.8450\n",
            "Epoch: 540, Train Loss: 0.8499\n",
            "Epoch: 541, Train Loss: 0.7996\n",
            "Epoch: 542, Train Loss: 0.8312\n",
            "Epoch: 543, Train Loss: 0.8053\n",
            "Epoch: 544, Train Loss: 0.8668\n",
            "Epoch: 545, Train Loss: 0.8579\n",
            "Epoch: 546, Train Loss: 0.7787\n",
            "Epoch: 547, Train Loss: 0.8520\n",
            "Epoch: 548, Train Loss: 0.8314\n",
            "Epoch: 549, Train Loss: 0.7674\n",
            "Epoch: 550, Train Loss: 0.7975\n",
            "Epoch: 551, Train Loss: 0.8397\n",
            "Epoch: 552, Train Loss: 0.8318\n",
            "Epoch: 553, Train Loss: 0.7733\n",
            "Epoch: 554, Train Loss: 0.8601\n",
            "Epoch: 555, Train Loss: 0.8186\n",
            "Epoch: 556, Train Loss: 0.8134\n",
            "Epoch: 557, Train Loss: 0.8165\n",
            "Epoch: 558, Train Loss: 0.7820\n",
            "Epoch: 559, Train Loss: 0.7756\n",
            "Epoch: 560, Train Loss: 0.8102\n",
            "Epoch: 561, Train Loss: 0.8260\n",
            "Epoch: 562, Train Loss: 0.8400\n",
            "Epoch: 563, Train Loss: 0.8046\n",
            "Epoch: 564, Train Loss: 0.8007\n",
            "Epoch: 565, Train Loss: 0.8603\n",
            "Epoch: 566, Train Loss: 0.7970\n",
            "Epoch: 567, Train Loss: 0.8728\n",
            "Epoch: 568, Train Loss: 0.8273\n",
            "Epoch: 569, Train Loss: 0.8314\n",
            "Epoch: 570, Train Loss: 0.8450\n",
            "Epoch: 571, Train Loss: 0.8288\n",
            "Epoch: 572, Train Loss: 0.7634\n",
            "Epoch: 573, Train Loss: 0.8499\n",
            "Epoch: 574, Train Loss: 0.8016\n",
            "Epoch: 575, Train Loss: 0.8145\n",
            "Epoch: 576, Train Loss: 0.8199\n",
            "Epoch: 577, Train Loss: 0.8247\n",
            "Epoch: 578, Train Loss: 0.8430\n",
            "Epoch: 579, Train Loss: 0.8067\n",
            "Epoch: 580, Train Loss: 0.8461\n",
            "Epoch: 581, Train Loss: 0.8385\n",
            "Epoch: 582, Train Loss: 0.7765\n",
            "Epoch: 583, Train Loss: 0.8287\n",
            "Epoch: 584, Train Loss: 0.7812\n",
            "Epoch: 585, Train Loss: 0.8222\n",
            "Epoch: 586, Train Loss: 0.7666\n",
            "Epoch: 587, Train Loss: 0.7754\n",
            "Epoch: 588, Train Loss: 0.8073\n",
            "Epoch: 589, Train Loss: 0.8717\n",
            "Epoch: 590, Train Loss: 0.8629\n",
            "Epoch: 591, Train Loss: 0.7989\n",
            "Epoch: 592, Train Loss: 0.7391\n",
            "Epoch: 593, Train Loss: 0.7742\n",
            "Epoch: 594, Train Loss: 0.8021\n",
            "Epoch: 595, Train Loss: 0.8547\n",
            "Epoch: 596, Train Loss: 0.7725\n",
            "Epoch: 597, Train Loss: 0.7987\n",
            "Epoch: 598, Train Loss: 0.8211\n",
            "Epoch: 599, Train Loss: 0.8392\n",
            "Epoch: 600, Train Loss: 0.8262\n",
            "Epoch: 601, Train Loss: 0.8024\n",
            "Epoch: 602, Train Loss: 0.7948\n",
            "Epoch: 603, Train Loss: 0.8134\n",
            "Epoch: 604, Train Loss: 0.8077\n",
            "Epoch: 605, Train Loss: 0.8363\n",
            "Epoch: 606, Train Loss: 0.8361\n",
            "Epoch: 607, Train Loss: 0.8440\n",
            "Epoch: 608, Train Loss: 0.8276\n",
            "Epoch: 609, Train Loss: 0.7810\n",
            "Epoch: 610, Train Loss: 0.8421\n",
            "Epoch: 611, Train Loss: 0.7831\n",
            "Epoch: 612, Train Loss: 0.8217\n",
            "Epoch: 613, Train Loss: 0.8684\n",
            "Epoch: 614, Train Loss: 0.8554\n",
            "Epoch: 615, Train Loss: 0.8288\n",
            "Epoch: 616, Train Loss: 0.8078\n",
            "Epoch: 617, Train Loss: 0.8136\n",
            "Epoch: 618, Train Loss: 0.8143\n",
            "Epoch: 619, Train Loss: 0.8064\n",
            "Epoch: 620, Train Loss: 0.7822\n",
            "Epoch: 621, Train Loss: 0.8144\n",
            "Epoch: 622, Train Loss: 0.8157\n",
            "Epoch: 623, Train Loss: 0.7990\n",
            "Epoch: 624, Train Loss: 0.7973\n",
            "Epoch: 625, Train Loss: 0.9003\n",
            "Epoch: 626, Train Loss: 0.7978\n",
            "Epoch: 627, Train Loss: 0.8238\n",
            "Epoch: 628, Train Loss: 0.8183\n",
            "Epoch: 629, Train Loss: 0.8277\n",
            "Epoch: 630, Train Loss: 0.8229\n",
            "Epoch: 631, Train Loss: 0.7800\n",
            "Epoch: 632, Train Loss: 0.8343\n",
            "Epoch: 633, Train Loss: 0.8510\n",
            "Epoch: 634, Train Loss: 0.7796\n",
            "Epoch: 635, Train Loss: 0.8151\n",
            "Epoch: 636, Train Loss: 0.8557\n",
            "Epoch: 637, Train Loss: 0.8717\n",
            "Epoch: 638, Train Loss: 0.7980\n",
            "Epoch: 639, Train Loss: 0.7799\n",
            "Epoch: 640, Train Loss: 0.8159\n",
            "Epoch: 641, Train Loss: 0.8057\n",
            "Epoch: 642, Train Loss: 0.8396\n",
            "Epoch: 643, Train Loss: 0.8386\n",
            "Epoch: 644, Train Loss: 0.8252\n",
            "Epoch: 645, Train Loss: 0.7804\n",
            "Epoch: 646, Train Loss: 0.8016\n",
            "Epoch: 647, Train Loss: 0.7820\n",
            "Epoch: 648, Train Loss: 0.8147\n",
            "Epoch: 649, Train Loss: 0.8357\n",
            "Epoch: 650, Train Loss: 0.8468\n",
            "Epoch: 651, Train Loss: 0.8014\n",
            "Epoch: 652, Train Loss: 0.7778\n",
            "Epoch: 653, Train Loss: 0.7897\n",
            "Epoch: 654, Train Loss: 0.9395\n",
            "Epoch: 655, Train Loss: 0.8144\n",
            "Epoch: 656, Train Loss: 0.8073\n",
            "Epoch: 657, Train Loss: 0.8382\n",
            "Epoch: 658, Train Loss: 0.7901\n",
            "Epoch: 659, Train Loss: 0.8412\n",
            "Epoch: 660, Train Loss: 0.7987\n",
            "Epoch: 661, Train Loss: 0.8366\n",
            "Epoch: 662, Train Loss: 0.8105\n",
            "Epoch: 663, Train Loss: 0.8556\n",
            "Epoch: 664, Train Loss: 0.8213\n",
            "Epoch: 665, Train Loss: 0.8128\n",
            "Epoch: 666, Train Loss: 0.8001\n",
            "Epoch: 667, Train Loss: 0.7846\n",
            "Epoch: 668, Train Loss: 0.7761\n",
            "Epoch: 669, Train Loss: 0.8026\n",
            "Epoch: 670, Train Loss: 0.8002\n",
            "Epoch: 671, Train Loss: 0.7828\n",
            "Epoch: 672, Train Loss: 0.8553\n",
            "Epoch: 673, Train Loss: 0.7652\n",
            "Epoch: 674, Train Loss: 0.7971\n",
            "Epoch: 675, Train Loss: 0.8284\n",
            "Epoch: 676, Train Loss: 0.8211\n",
            "Epoch: 677, Train Loss: 0.8239\n",
            "Epoch: 678, Train Loss: 0.7835\n",
            "Epoch: 679, Train Loss: 0.8511\n",
            "Epoch: 680, Train Loss: 0.7849\n",
            "Epoch: 681, Train Loss: 0.7363\n",
            "Epoch: 682, Train Loss: 0.8056\n",
            "Epoch: 683, Train Loss: 0.8265\n",
            "Epoch: 684, Train Loss: 0.8045\n",
            "Epoch: 685, Train Loss: 0.7914\n",
            "Epoch: 686, Train Loss: 0.7839\n",
            "Epoch: 687, Train Loss: 0.8117\n",
            "Epoch: 688, Train Loss: 0.9025\n",
            "Epoch: 689, Train Loss: 0.8189\n",
            "Epoch: 690, Train Loss: 0.7880\n",
            "Epoch: 691, Train Loss: 0.8936\n",
            "Epoch: 692, Train Loss: 0.8005\n",
            "Epoch: 693, Train Loss: 0.8438\n",
            "Epoch: 694, Train Loss: 0.8324\n",
            "Epoch: 695, Train Loss: 0.8312\n",
            "Epoch: 696, Train Loss: 0.7911\n",
            "Epoch: 697, Train Loss: 0.7939\n",
            "Epoch: 698, Train Loss: 0.7978\n",
            "Epoch: 699, Train Loss: 0.7670\n",
            "Epoch: 700, Train Loss: 0.7587\n",
            "Epoch: 701, Train Loss: 0.7854\n",
            "Epoch: 702, Train Loss: 0.8296\n",
            "Epoch: 703, Train Loss: 0.8736\n",
            "Epoch: 704, Train Loss: 0.8372\n",
            "Epoch: 705, Train Loss: 0.8402\n",
            "Epoch: 706, Train Loss: 0.7816\n",
            "Epoch: 707, Train Loss: 0.7769\n",
            "Epoch: 708, Train Loss: 0.7766\n",
            "Epoch: 709, Train Loss: 0.7372\n",
            "Epoch: 710, Train Loss: 0.8176\n",
            "Epoch: 711, Train Loss: 0.8142\n",
            "Epoch: 712, Train Loss: 0.8258\n",
            "Epoch: 713, Train Loss: 0.8203\n",
            "Epoch: 714, Train Loss: 0.8000\n",
            "Epoch: 715, Train Loss: 0.8101\n",
            "Epoch: 716, Train Loss: 0.7935\n",
            "Epoch: 717, Train Loss: 0.8259\n",
            "Epoch: 718, Train Loss: 0.7856\n",
            "Epoch: 719, Train Loss: 0.8170\n",
            "Epoch: 720, Train Loss: 0.8049\n",
            "Epoch: 721, Train Loss: 0.9032\n",
            "Epoch: 722, Train Loss: 0.7997\n",
            "Epoch: 723, Train Loss: 0.8799\n",
            "Epoch: 724, Train Loss: 0.8089\n",
            "Epoch: 725, Train Loss: 0.8153\n",
            "Epoch: 726, Train Loss: 0.7997\n",
            "Epoch: 727, Train Loss: 0.7747\n",
            "Epoch: 728, Train Loss: 0.7929\n",
            "Epoch: 729, Train Loss: 0.8167\n",
            "Epoch: 730, Train Loss: 0.8247\n",
            "Epoch: 731, Train Loss: 0.7913\n",
            "Epoch: 732, Train Loss: 0.7721\n",
            "Epoch: 733, Train Loss: 0.7978\n",
            "Epoch: 734, Train Loss: 0.8170\n",
            "Epoch: 735, Train Loss: 0.7320\n",
            "Epoch: 736, Train Loss: 0.8081\n",
            "Epoch: 737, Train Loss: 0.7733\n",
            "Epoch: 738, Train Loss: 0.8308\n",
            "Epoch: 739, Train Loss: 0.8274\n",
            "Epoch: 740, Train Loss: 0.7884\n",
            "Epoch: 741, Train Loss: 0.7916\n",
            "Epoch: 742, Train Loss: 0.7812\n",
            "Epoch: 743, Train Loss: 0.8702\n",
            "Epoch: 744, Train Loss: 0.8061\n",
            "Epoch: 745, Train Loss: 0.7979\n",
            "Epoch: 746, Train Loss: 0.8155\n",
            "Epoch: 747, Train Loss: 0.7860\n",
            "Epoch: 748, Train Loss: 0.8031\n",
            "Epoch: 749, Train Loss: 0.7369\n",
            "Epoch: 750, Train Loss: 0.7859\n",
            "Epoch: 751, Train Loss: 0.8206\n",
            "Epoch: 752, Train Loss: 0.8480\n",
            "Epoch: 753, Train Loss: 0.7707\n",
            "Epoch: 754, Train Loss: 0.8025\n",
            "Epoch: 755, Train Loss: 0.8016\n",
            "Epoch: 756, Train Loss: 0.8104\n",
            "Epoch: 757, Train Loss: 0.7839\n",
            "Epoch: 758, Train Loss: 0.8543\n",
            "Epoch: 759, Train Loss: 0.8579\n",
            "Epoch: 760, Train Loss: 0.7355\n",
            "Epoch: 761, Train Loss: 0.8770\n",
            "Epoch: 762, Train Loss: 0.7881\n",
            "Epoch: 763, Train Loss: 0.7569\n",
            "Epoch: 764, Train Loss: 0.7842\n",
            "Epoch: 765, Train Loss: 0.8345\n",
            "Epoch: 766, Train Loss: 0.8627\n",
            "Epoch: 767, Train Loss: 0.7942\n",
            "Epoch: 768, Train Loss: 0.7530\n",
            "Epoch: 769, Train Loss: 0.7603\n",
            "Epoch: 770, Train Loss: 0.7920\n",
            "Epoch: 771, Train Loss: 0.7675\n",
            "Epoch: 772, Train Loss: 0.7915\n",
            "Epoch: 773, Train Loss: 0.7971\n",
            "Epoch: 774, Train Loss: 0.7794\n",
            "Epoch: 775, Train Loss: 0.8430\n",
            "Epoch: 776, Train Loss: 0.7889\n",
            "Epoch: 777, Train Loss: 0.7628\n",
            "Epoch: 778, Train Loss: 0.8005\n",
            "Epoch: 779, Train Loss: 0.8055\n",
            "Epoch: 780, Train Loss: 0.8125\n",
            "Epoch: 781, Train Loss: 0.8336\n",
            "Epoch: 782, Train Loss: 0.8123\n",
            "Epoch: 783, Train Loss: 0.8403\n",
            "Epoch: 784, Train Loss: 0.7493\n",
            "Epoch: 785, Train Loss: 0.8380\n",
            "Epoch: 786, Train Loss: 0.7732\n",
            "Epoch: 787, Train Loss: 0.8336\n",
            "Epoch: 788, Train Loss: 0.8245\n",
            "Epoch: 789, Train Loss: 0.8233\n",
            "Epoch: 790, Train Loss: 0.7894\n",
            "Epoch: 791, Train Loss: 0.7715\n",
            "Epoch: 792, Train Loss: 0.7837\n",
            "Epoch: 793, Train Loss: 0.7591\n",
            "Epoch: 794, Train Loss: 0.7931\n",
            "Epoch: 795, Train Loss: 0.8226\n",
            "Epoch: 796, Train Loss: 0.8040\n",
            "Epoch: 797, Train Loss: 0.7909\n",
            "Epoch: 798, Train Loss: 0.8042\n",
            "Epoch: 799, Train Loss: 0.8207\n",
            "Epoch: 800, Train Loss: 0.8247\n",
            "Epoch: 801, Train Loss: 0.8110\n",
            "Epoch: 802, Train Loss: 0.7726\n",
            "Epoch: 803, Train Loss: 0.8203\n",
            "Epoch: 804, Train Loss: 0.8294\n",
            "Epoch: 805, Train Loss: 0.8165\n",
            "Epoch: 806, Train Loss: 0.8054\n",
            "Epoch: 807, Train Loss: 0.7765\n",
            "Epoch: 808, Train Loss: 0.8360\n",
            "Epoch: 809, Train Loss: 0.7921\n",
            "Epoch: 810, Train Loss: 0.8003\n",
            "Epoch: 811, Train Loss: 0.7867\n",
            "Epoch: 812, Train Loss: 0.8528\n",
            "Epoch: 813, Train Loss: 0.7879\n",
            "Epoch: 814, Train Loss: 0.7697\n",
            "Epoch: 815, Train Loss: 0.8407\n",
            "Epoch: 816, Train Loss: 0.7520\n",
            "Epoch: 817, Train Loss: 0.8073\n",
            "Epoch: 818, Train Loss: 0.7547\n",
            "Epoch: 819, Train Loss: 0.7886\n",
            "Epoch: 820, Train Loss: 0.7793\n",
            "Epoch: 821, Train Loss: 0.7596\n",
            "Epoch: 822, Train Loss: 0.7943\n",
            "Epoch: 823, Train Loss: 0.7969\n",
            "Epoch: 824, Train Loss: 0.8342\n",
            "Epoch: 825, Train Loss: 0.7771\n",
            "Epoch: 826, Train Loss: 0.7773\n",
            "Epoch: 827, Train Loss: 0.7918\n",
            "Epoch: 828, Train Loss: 0.8132\n",
            "Epoch: 829, Train Loss: 0.8208\n",
            "Epoch: 830, Train Loss: 0.7832\n",
            "Epoch: 831, Train Loss: 0.9178\n",
            "Epoch: 832, Train Loss: 0.7531\n",
            "Epoch: 833, Train Loss: 0.7712\n",
            "Epoch: 834, Train Loss: 0.8021\n",
            "Epoch: 835, Train Loss: 0.7938\n",
            "Epoch: 836, Train Loss: 0.7974\n",
            "Epoch: 837, Train Loss: 0.7833\n",
            "Epoch: 838, Train Loss: 0.7515\n",
            "Epoch: 839, Train Loss: 0.8241\n",
            "Epoch: 840, Train Loss: 0.8385\n",
            "Epoch: 841, Train Loss: 0.7720\n",
            "Epoch: 842, Train Loss: 0.8733\n",
            "Epoch: 843, Train Loss: 0.7950\n",
            "Epoch: 844, Train Loss: 0.7886\n",
            "Epoch: 845, Train Loss: 0.7718\n",
            "Epoch: 846, Train Loss: 0.8261\n",
            "Epoch: 847, Train Loss: 0.7665\n",
            "Epoch: 848, Train Loss: 0.8038\n",
            "Epoch: 849, Train Loss: 0.8167\n",
            "Epoch: 850, Train Loss: 0.8192\n",
            "Epoch: 851, Train Loss: 0.7914\n",
            "Epoch: 852, Train Loss: 0.7989\n",
            "Epoch: 853, Train Loss: 0.7724\n",
            "Epoch: 854, Train Loss: 0.9146\n",
            "Epoch: 855, Train Loss: 0.7924\n",
            "Epoch: 856, Train Loss: 0.7777\n",
            "Epoch: 857, Train Loss: 0.7899\n",
            "Epoch: 858, Train Loss: 0.8340\n",
            "Epoch: 859, Train Loss: 0.7941\n",
            "Epoch: 860, Train Loss: 0.7893\n",
            "Epoch: 861, Train Loss: 0.7805\n",
            "Epoch: 862, Train Loss: 0.7435\n",
            "Epoch: 863, Train Loss: 0.7580\n",
            "Epoch: 864, Train Loss: 0.7560\n",
            "Epoch: 865, Train Loss: 0.7689\n",
            "Epoch: 866, Train Loss: 0.8105\n",
            "Epoch: 867, Train Loss: 0.8165\n",
            "Epoch: 868, Train Loss: 0.8018\n",
            "Epoch: 869, Train Loss: 0.7740\n",
            "Epoch: 870, Train Loss: 0.8292\n",
            "Epoch: 871, Train Loss: 0.8115\n",
            "Epoch: 872, Train Loss: 0.7827\n",
            "Epoch: 873, Train Loss: 0.7877\n",
            "Epoch: 874, Train Loss: 0.7660\n",
            "Epoch: 875, Train Loss: 0.8509\n",
            "Epoch: 876, Train Loss: 0.7731\n",
            "Epoch: 877, Train Loss: 0.8128\n",
            "Epoch: 878, Train Loss: 0.7559\n",
            "Epoch: 879, Train Loss: 0.7677\n",
            "Epoch: 880, Train Loss: 0.8032\n",
            "Epoch: 881, Train Loss: 0.7962\n",
            "Epoch: 882, Train Loss: 0.7933\n",
            "Epoch: 883, Train Loss: 0.8091\n",
            "Epoch: 884, Train Loss: 0.7864\n",
            "Epoch: 885, Train Loss: 0.7977\n",
            "Epoch: 886, Train Loss: 0.8125\n",
            "Epoch: 887, Train Loss: 0.8091\n",
            "Epoch: 888, Train Loss: 0.7959\n",
            "Epoch: 889, Train Loss: 0.7854\n",
            "Epoch: 890, Train Loss: 0.7749\n",
            "Epoch: 891, Train Loss: 0.8018\n",
            "Epoch: 892, Train Loss: 0.8103\n",
            "Epoch: 893, Train Loss: 0.8164\n",
            "Epoch: 894, Train Loss: 0.7512\n",
            "Epoch: 895, Train Loss: 0.8364\n",
            "Epoch: 896, Train Loss: 0.7382\n",
            "Epoch: 897, Train Loss: 0.7674\n",
            "Epoch: 898, Train Loss: 0.7867\n",
            "Epoch: 899, Train Loss: 0.8143\n",
            "Epoch: 900, Train Loss: 0.7798\n",
            "Epoch: 901, Train Loss: 0.7372\n",
            "Epoch: 902, Train Loss: 0.7847\n",
            "Epoch: 903, Train Loss: 0.8308\n",
            "Epoch: 904, Train Loss: 0.8612\n",
            "Epoch: 905, Train Loss: 0.8455\n",
            "Epoch: 906, Train Loss: 0.8486\n",
            "Epoch: 907, Train Loss: 0.7882\n",
            "Epoch: 908, Train Loss: 0.8490\n",
            "Epoch: 909, Train Loss: 0.7811\n",
            "Epoch: 910, Train Loss: 0.8237\n",
            "Epoch: 911, Train Loss: 0.8212\n",
            "Epoch: 912, Train Loss: 0.7783\n",
            "Epoch: 913, Train Loss: 0.7787\n",
            "Epoch: 914, Train Loss: 0.7604\n",
            "Epoch: 915, Train Loss: 0.7870\n",
            "Epoch: 916, Train Loss: 0.7946\n",
            "Epoch: 917, Train Loss: 0.7572\n",
            "Epoch: 918, Train Loss: 0.7523\n",
            "Epoch: 919, Train Loss: 0.7626\n",
            "Epoch: 920, Train Loss: 0.7914\n",
            "Epoch: 921, Train Loss: 0.8002\n",
            "Epoch: 922, Train Loss: 0.7852\n",
            "Epoch: 923, Train Loss: 0.8287\n",
            "Epoch: 924, Train Loss: 0.7971\n",
            "Epoch: 925, Train Loss: 0.8228\n",
            "Epoch: 926, Train Loss: 0.8393\n",
            "Epoch: 927, Train Loss: 0.8152\n",
            "Epoch: 928, Train Loss: 0.8249\n",
            "Epoch: 929, Train Loss: 0.7396\n",
            "Epoch: 930, Train Loss: 0.7573\n",
            "Epoch: 931, Train Loss: 0.8062\n",
            "Epoch: 932, Train Loss: 0.7880\n",
            "Epoch: 933, Train Loss: 0.7796\n",
            "Epoch: 934, Train Loss: 0.7982\n",
            "Epoch: 935, Train Loss: 0.7775\n",
            "Epoch: 936, Train Loss: 0.8006\n",
            "Epoch: 937, Train Loss: 0.7837\n",
            "Epoch: 938, Train Loss: 0.7769\n",
            "Epoch: 939, Train Loss: 0.7666\n",
            "Epoch: 940, Train Loss: 0.8342\n",
            "Epoch: 941, Train Loss: 0.7768\n",
            "Epoch: 942, Train Loss: 0.7618\n",
            "Epoch: 943, Train Loss: 0.7855\n",
            "Epoch: 944, Train Loss: 0.7910\n",
            "Epoch: 945, Train Loss: 0.8059\n",
            "Epoch: 946, Train Loss: 0.7606\n",
            "Epoch: 947, Train Loss: 0.7571\n",
            "Epoch: 948, Train Loss: 0.7994\n",
            "Epoch: 949, Train Loss: 0.7829\n",
            "Epoch: 950, Train Loss: 0.7765\n",
            "Epoch: 951, Train Loss: 0.8169\n",
            "Epoch: 952, Train Loss: 0.8374\n",
            "Epoch: 953, Train Loss: 0.7828\n",
            "Epoch: 954, Train Loss: 0.8733\n",
            "Epoch: 955, Train Loss: 0.7616\n",
            "Epoch: 956, Train Loss: 0.8014\n",
            "Epoch: 957, Train Loss: 0.8448\n",
            "Epoch: 958, Train Loss: 0.7851\n",
            "Epoch: 959, Train Loss: 0.8067\n",
            "Epoch: 960, Train Loss: 0.7638\n",
            "Epoch: 961, Train Loss: 0.9163\n",
            "Epoch: 962, Train Loss: 0.8165\n",
            "Epoch: 963, Train Loss: 0.7957\n",
            "Epoch: 964, Train Loss: 0.8317\n",
            "Epoch: 965, Train Loss: 0.7616\n",
            "Epoch: 966, Train Loss: 0.7833\n",
            "Epoch: 967, Train Loss: 0.7893\n",
            "Epoch: 968, Train Loss: 0.8203\n",
            "Epoch: 969, Train Loss: 0.8126\n",
            "Epoch: 970, Train Loss: 0.7944\n",
            "Epoch: 971, Train Loss: 0.7716\n",
            "Epoch: 972, Train Loss: 0.7784\n",
            "Epoch: 973, Train Loss: 0.8645\n",
            "Epoch: 974, Train Loss: 0.7927\n",
            "Epoch: 975, Train Loss: 0.7817\n",
            "Epoch: 976, Train Loss: 0.7829\n",
            "Epoch: 977, Train Loss: 0.7683\n",
            "Epoch: 978, Train Loss: 0.7922\n",
            "Epoch: 979, Train Loss: 0.7634\n",
            "Epoch: 980, Train Loss: 0.7964\n",
            "Epoch: 981, Train Loss: 0.7954\n",
            "Epoch: 982, Train Loss: 0.7816\n",
            "Epoch: 983, Train Loss: 0.7991\n",
            "Epoch: 984, Train Loss: 0.8132\n",
            "Epoch: 985, Train Loss: 0.7278\n",
            "Epoch: 986, Train Loss: 0.7617\n",
            "Epoch: 987, Train Loss: 0.8019\n",
            "Epoch: 988, Train Loss: 0.7392\n",
            "Epoch: 989, Train Loss: 0.8420\n",
            "Epoch: 990, Train Loss: 0.7924\n",
            "Epoch: 991, Train Loss: 0.8008\n",
            "Epoch: 992, Train Loss: 0.7728\n",
            "Epoch: 993, Train Loss: 0.8140\n",
            "Epoch: 994, Train Loss: 0.7674\n",
            "Epoch: 995, Train Loss: 0.8189\n",
            "Epoch: 996, Train Loss: 0.7712\n",
            "Epoch: 997, Train Loss: 0.7988\n",
            "Epoch: 998, Train Loss: 0.8152\n",
            "Epoch: 999, Train Loss: 0.7827\n",
            "Epoch: 1000, Train Loss: 0.8020\n",
            "time: 7min 32s (started: 2021-03-04 12:45:36 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc_hGCQyRcII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19942900-4909-4cb0-e558-4c7af8b94753"
      },
      "source": [
        "y_pred_save = model(torch.tensor(X_test_scaled, dtype=torch.float).to(DEVICE))\r\n",
        "y_pred_save = y_pred_save.cpu().detach().numpy()\r\n",
        "df_test['h_index_pred'].update(pd.Series(np.rint(y_pred_save)))\r\n",
        "df_test.loc[:, [\"authorID\", \"h_index_pred\"]].to_csv(\r\n",
        "    f'predictions_final_net.csv', index=False\r\n",
        ")"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.16 s (started: 2021-03-04 12:53:08 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REJUe-agyE6f",
        "outputId": "b0e6a527-ac4f-4573-cf15-d16dc4086999"
      },
      "source": [
        "torch.save(model, \"final_net_full.pth\")"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 21.2 ms (started: 2021-03-04 12:53:10 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRdAL-eStIL0"
      },
      "source": [
        "# Correction step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GefZeI2OXLTM",
        "outputId": "8fd2eddc-f5e6-4dee-fc6f-f9d56c529373"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1n87aGFdtmgxsA5f195irRkShzE24SEKh\""
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n87aGFdtmgxsA5f195irRkShzE24SEKh\n",
            "To: /content/features_max.npy\n",
            "\r  0% 0.00/925k [00:00<?, ?B/s]\r100% 925k/925k [00:00<00:00, 57.9MB/s]\n",
            "time: 4.65 s (started: 2021-03-04 12:53:10 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVFETFtBjHlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99189b8e-a0c0-452f-b7d2-3dc89d289f04"
      },
      "source": [
        "features_max = np.load(\"features_max.npy\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.3 ms (started: 2021-03-04 12:53:14 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB6IKhZAjamk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3fd3d0c-f1a7-4b79-aac3-c85bcff68dfd"
      },
      "source": [
        "X_features_train = features_max[:len(y_train)]\r\n",
        "X_features_test = features_max[len(y_train):]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 1.89 ms (started: 2021-03-04 12:53:14 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "tJh3Hz2-ZvrD",
        "outputId": "cb36a982-c22e-45cf-b49e-aaeb0fcd712b"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "b = Counter(np.rint(X_features_train))\r\n",
        "c = Counter(y_train)\r\n",
        "\r\n",
        "plt.figure(figsize=(25, 15))\r\n",
        "plt.plot([x for _, x in b.most_common()], label=\"maxime_net\")\r\n",
        "plt.plot([x for _, x in c.most_common()], label=\"h_index_train\")\r\n",
        "plt.legend()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f070a747590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "time: 443 ms (started: 2021-03-04 12:53:14 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-1tgaX6qQOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0bd7c75-0f8f-4f43-f357-df32b32f6065"
      },
      "source": [
        "count_submission = {h_index: x/len(y_train) for h_index, (_, x) in enumerate(b.most_common())}\r\n",
        "count_y_train =  {h_index: x/len(y_train) for h_index, (_, x) in enumerate(c.most_common())}"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.51 ms (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v__Jx91lR8za",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "5612fe64-b6b1-4ba2-d8ee-0b7b14552d42"
      },
      "source": [
        "best_predictions = pd.read_csv(\"predictions_final_net.csv\")\r\n",
        "best_predictions"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authorID</th>\n",
              "      <th>h_index_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1036332</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1101850</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1336878</td>\n",
              "      <td>39.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1515524</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1606427</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208110</th>\n",
              "      <td>2908387141</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208111</th>\n",
              "      <td>2908425732</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208112</th>\n",
              "      <td>2908436250</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208113</th>\n",
              "      <td>2908499439</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208114</th>\n",
              "      <td>2908506980</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>208115 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          authorID  h_index_pred\n",
              "0          1036332          19.0\n",
              "1          1101850           5.0\n",
              "2          1336878          39.0\n",
              "3          1515524          13.0\n",
              "4          1606427           1.0\n",
              "...            ...           ...\n",
              "208110  2908387141           1.0\n",
              "208111  2908425732           0.0\n",
              "208112  2908436250           1.0\n",
              "208113  2908499439           6.0\n",
              "208114  2908506980           1.0\n",
              "\n",
              "[208115 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "stream",
          "text": [
            "time: 63.1 ms (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyd-aWF2tHzz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd06901-cadd-46dc-bf32-f06f062d5c24"
      },
      "source": [
        "best_predictions[\"h_index_pred\"] = best_predictions[\"h_index_pred\"].apply(lambda x: 1. if x == 0 else x)\r\n",
        "limit = max(best_predictions[\"h_index_pred\"])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 81.3 ms (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "oHT8F4B2sedA",
        "outputId": "a846f5bc-8ec5-4cde-d5d8-8b40af27b236"
      },
      "source": [
        "count_groups = best_predictions.groupby(\"h_index_pred\").count().sort_values(['h_index_pred'], ascending=False)\r\n",
        "count_groups = count_groups.query(\"h_index_pred <= @limit\")\r\n",
        "count_groups.reset_index(inplace=True)\r\n",
        "count_groups.rename(columns={\"authorID\": \"count\"}, inplace=True)\r\n",
        "count_groups"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>h_index_pred</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>142.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>141.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>139.0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>136.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>134.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>3.0</td>\n",
              "      <td>16414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>2.0</td>\n",
              "      <td>27985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>1.0</td>\n",
              "      <td>28719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>-1.0</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>-2.0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>136 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     h_index_pred  count\n",
              "0           142.0      2\n",
              "1           141.0      1\n",
              "2           139.0      2\n",
              "3           136.0      1\n",
              "4           134.0      1\n",
              "..            ...    ...\n",
              "131           3.0  16414\n",
              "132           2.0  27985\n",
              "133           1.0  28719\n",
              "134          -1.0     73\n",
              "135          -2.0      4\n",
              "\n",
              "[136 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "stream",
          "text": [
            "time: 33.8 ms (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxWZFxz14CfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c051e1c3-e3de-4d6a-c337-2b3c7a7453f6"
      },
      "source": [
        "new_best_predictions = best_predictions.copy()\r\n",
        "new_best_predictions.set_index(\"authorID\", inplace=True)\r\n",
        "num_corrections = 3"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 4.04 ms (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KzViPkpy_q5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22a0c192-0898-47f0-8ea8-5cd2d194a907"
      },
      "source": [
        "def correct(row):\r\n",
        "    h_index_pred, count = row[\"h_index_pred\"], row[\"count\"]\r\n",
        "    if h_index_pred > 0 and h_index_pred in count_submission and h_index_pred in count_y_train:\r\n",
        "        diff_train_count = count_submission[h_index_pred]-count_y_train[h_index_pred]\r\n",
        "        if diff_train_count > 0:\r\n",
        "            num_authors_replace = int(diff_train_count*len(new_best_predictions) / (i+1)) \r\n",
        "            authors_id = new_best_predictions.query(\"h_index_pred == @h_index_pred\")\r\n",
        "            worse_authors = authors_id.merge(X_test_graph, on=\"authorID\").sort_values(by=['paper_per_author', 'degree'], ascending=True).head(num_authors_replace)\r\n",
        "            for authorID in worse_authors[\"authorID\"]:\r\n",
        "                new_best_predictions.loc[authorID, \"h_index_pred\"] -= 1"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.71 ms (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FGj0jfR50OAc",
        "outputId": "d5401939-3055-40eb-d411-67c609248123"
      },
      "source": [
        "for i in range(num_corrections):\r\n",
        "    count_groups.apply(correct, axis=1)\r\n",
        "    new_best_predictions[\"h_index_pred\"] = new_best_predictions[\"h_index_pred\"].apply(lambda x: 1. if x == 0 else x)\r\n",
        "    old = Counter(best_predictions[\"h_index_pred\"])\r\n",
        "    new = Counter(new_best_predictions[\"h_index_pred\"])\r\n",
        "\r\n",
        "\r\n",
        "    plt.figure(figsize=(25, 15))\r\n",
        "    plt.plot([x for _, x in old.most_common()], label=\"initial_h_indedx\")\r\n",
        "    plt.plot([x for _, x in new.most_common()], label=\"corrected_h_index\")\r\n",
        "    plt.legend()\r\n",
        "    save_csv = new_best_predictions.copy()\r\n",
        "    save_csv.reset_index(inplace=True)\r\n",
        "    save_csv.rename(columns={\"index\": \"authorID\"}, inplace=True)\r\n",
        "    save_csv.to_csv(\"best_predictions_correction.csv\", index=False)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1800x1080 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "time: 8.98 s (started: 2021-03-04 12:53:15 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3il-xRUa6SL8"
      },
      "source": [
        "# Here is the opitmization with Optuna (not mandatory for the best score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qklfUk8yXQwr"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZVyp4qNXSW-"
      },
      "source": [
        "import optuna\r\n",
        "from optuna.trial import TrialState\r\n",
        "in_features = X_train_final.shape[-1]\r\n",
        "\r\n",
        "def define_model(trial):\r\n",
        "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\r\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\r\n",
        "    hidden_channels = trial.suggest_int(\"n_units_l00\", 500, 2000)\r\n",
        "    dropout = trial.suggest_float(\"dropout_l00\", 0.2, 0.7)\r\n",
        "    layers = [nn.Linear(in_features, hidden_channels),\r\n",
        "              nn.ReLU(),\r\n",
        "              nn.BatchNorm1d(hidden_channels),\r\n",
        "              nn.Dropout(dropout)]\r\n",
        "    for i in range(n_layers):\r\n",
        "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 200, 2000)\r\n",
        "        layers.append(nn.Linear(hidden_channels, out_features))\r\n",
        "        layers.append(nn.ReLU())\r\n",
        "        layers.append(nn.BatchNorm1d(out_features))\r\n",
        "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.7)\r\n",
        "        layers.append(nn.Dropout(p))\r\n",
        "\r\n",
        "        hidden_channels = out_features\r\n",
        "    layers.append(nn.Linear(out_features, 1))\r\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW1bja-pYL-G"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "def objective(trial):\r\n",
        "\r\n",
        "    # Generate the model.\r\n",
        "    model = define_model(trial).to(DEVICE)\r\n",
        "    EPOCHS = 500\r\n",
        "    # Generate the optimizers.\r\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\r\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\r\n",
        "    weight_decay =  trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\r\n",
        "    optimizer = getattr(optim, optimizer_name)(\r\n",
        "        model.parameters(), lr=lr, weight_decay=weight_decay\r\n",
        "    )\r\n",
        "\r\n",
        "    # Training of the model.\r\n",
        "    for epoch in range(1, EPOCHS+1):\r\n",
        "        total_loss = 0\r\n",
        "        model.train()\r\n",
        "        for (x, y) in train_loader:\r\n",
        "            x = x.to(DEVICE)\r\n",
        "            y = y.to(DEVICE)\r\n",
        "            optimizer.zero_grad()  # Clear gradients.\r\n",
        "            out = model(x).squeeze(1)  # Perform a single forward pass.\r\n",
        "            loss = criterion(out, y)  # Compute the loss solely based on the training nodes\r\n",
        "            loss.backward()  # Derive gradients.\r\n",
        "            optimizer.step()\r\n",
        "            total_loss += loss.item()\r\n",
        "        test_loss = 0\r\n",
        "        model.eval()\r\n",
        "        for (x, y) in test_loader:\r\n",
        "            x = x.to(DEVICE)\r\n",
        "            y = y.to(DEVICE)\r\n",
        "            pred = model(x).squeeze(1)  # Perform a single forward pass.\r\n",
        "            loss = criterion(pred, y)  # Compute the loss solely based on the training nodes\r\n",
        "            test_loss += loss.item()\r\n",
        "        #print(f'Epoch: {epoch:03d}, Train Loss: {total_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}')\r\n",
        "        trial.report(test_loss/len(test_loader), epoch)\r\n",
        "        if trial.should_prune():\r\n",
        "                    raise optuna.exceptions.TrialPruned()\r\n",
        "    return test_loss/len(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqnMg9wJY52F"
      },
      "source": [
        "study = optuna.create_study(direction=\"minimize\")\r\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}